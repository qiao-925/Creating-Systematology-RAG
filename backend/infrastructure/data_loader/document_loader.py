"""
æ•°æ®å¯¼å…¥æœåŠ¡ï¼šæ ¸å¿ƒæ–‡æ¡£åŠ è½½æµç¨‹

ä¸»è¦åŠŸèƒ½ï¼š
- ä»æ•°æ®æºåŠ è½½æ–‡æ¡£çš„æ ¸å¿ƒæµç¨‹
- æ”¯æŒè¿›åº¦è¿½è¸ªå’Œå–æ¶ˆæœºåˆ¶
"""

import time
from typing import List, Optional, TYPE_CHECKING

from llama_index.core.schema import Document as LlamaDocument

from backend.infrastructure.logger import get_logger
from backend.infrastructure.data_loader.processor import DocumentProcessor
from backend.infrastructure.data_loader.models import ProgressReporter

if TYPE_CHECKING:
    from backend.infrastructure.data_loader.source import DataSource
    from backend.infrastructure.data_loader.progress import ImportProgressManager

logger = get_logger('data_loader_service')

# æ£€æŸ¥æ–°æ¶æ„æ˜¯å¦å¯ç”¨
try:
    from backend.infrastructure.data_loader.parser import DocumentParser
    NEW_ARCHITECTURE_AVAILABLE = True
except ImportError:
    NEW_ARCHITECTURE_AVAILABLE = False
    DocumentParser = None


def load_documents_from_source(
    source: "DataSource",
    clean: bool = True,
    show_progress: bool = True,
    progress_reporter: ProgressReporter = None,
    progress_manager: Optional["ImportProgressManager"] = None
) -> List[LlamaDocument]:
    """ä»æ•°æ®æºåŠ è½½æ–‡æ¡£ï¼ˆæ ¸å¿ƒåŠ è½½æµç¨‹ï¼‰
    
    Args:
        source: æ•°æ®æºå¯¹è±¡ï¼ˆGitHubSource, LocalFileSourceç­‰ï¼‰
        clean: æ˜¯å¦æ¸…ç†æ–‡æœ¬
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        progress_reporter: è¿›åº¦åé¦ˆå™¨ï¼ˆå¯é€‰ï¼‰
        progress_manager: è¿›åº¦ç®¡ç†å™¨ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        æ–‡æ¡£åˆ—è¡¨
    """
    if not NEW_ARCHITECTURE_AVAILABLE:
        logger.error("[é˜¶æ®µ1.2] æ–°æ¶æ„æœªå¯ç”¨")
        return []
    
    if progress_reporter is None:
        progress_reporter = ProgressReporter(show_progress=show_progress)
    
    try:
        total_start_time = time.time()
        
        progress_reporter.print_if_enabled("ğŸ” æ­£åœ¨ä»æ•°æ®æºè·å–æ–‡ä»¶è·¯å¾„...")
        
        source_start_time = time.time()
        source_files = source.get_file_paths()
        source_elapsed = time.time() - source_start_time
        
        if not source_files:
            logger.warning(f"[é˜¶æ®µ1.2] æ•°æ®æºæœªè¿”å›ä»»ä½•æ–‡ä»¶")
            progress_reporter.print_if_enabled("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
            return []
        
        logger.info(f"[é˜¶æ®µ1.2] æ•°æ®æºè¿”å› {len(source_files)} ä¸ªæ–‡ä»¶ (è€—æ—¶: {source_elapsed:.2f}s)")
        progress_reporter.print_if_enabled(f"âœ… æ‰¾åˆ° {len(source_files)} ä¸ªæ–‡ä»¶")
        
        # å–æ¶ˆæ£€æŸ¥ç‚¹
        if progress_manager and progress_manager.check_cancelled():
            return []
        
        file_paths = [sf.path for sf in source_files]
        metadata_map = {
            sf.path: {**sf.metadata, 'source_type': sf.source_type}
            for sf in source_files
        }
        
        progress_reporter.print_if_enabled("ğŸ“„ æ­£åœ¨è§£ææ–‡ä»¶...")
        
        # å¼€å§‹è§£æé˜¶æ®µ
        if progress_manager:
            from backend.infrastructure.data_loader.progress import ImportStage
            progress_manager.start_stage(ImportStage.DOC_PARSE, total=len(file_paths))
        
        parser_start_time = time.time()
        documents = DocumentParser().parse_files(
            file_paths, metadata_map, clean=clean,
            progress_callback=_create_progress_callback(progress_manager) if progress_manager else None
        )
        parser_elapsed = time.time() - parser_start_time
        
        # å®Œæˆè§£æé˜¶æ®µ
        if progress_manager:
            progress_manager.complete_stage(
                ImportStage.DOC_PARSE, 
                f"è§£æå®Œæˆ ({len(documents)} ä¸ªæ–‡æ¡£)"
            )
        
        # å–æ¶ˆæ£€æŸ¥ç‚¹
        if progress_manager and progress_manager.check_cancelled():
            return []
        
        if not documents:
            logger.warning(f"[é˜¶æ®µ1.3] è§£æå™¨æœªè¿”å›ä»»ä½•æ–‡æ¡£ (è¾“å…¥æ–‡ä»¶æ•°: {len(file_paths)})")
            progress_reporter.print_if_enabled("âš ï¸  æœªèƒ½è§£æä»»ä½•æ–‡æ¡£")
            return []
        
        logger.info(f"[é˜¶æ®µ1.3] è§£æå™¨è¿”å› {len(documents)} ä¸ªæ–‡æ¡£ (è€—æ—¶: {parser_elapsed:.2f}s)")
        
        clean_start_time = time.time()
        if clean:
            processor = DocumentProcessor()
            documents = [
                LlamaDocument(
                    text=processor.clean_text(doc.text),
                    metadata=doc.metadata,
                    id_=doc.id_
                )
                for doc in documents
            ]
        clean_elapsed = time.time() - clean_start_time if clean else 0.0
        
        total_elapsed = time.time() - total_start_time
        progress_reporter.print_if_enabled(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        
        success_rate = (len(documents) / len(source_files) * 100) if source_files else 0
        logger.info(
            f"[é˜¶æ®µ1.3] æ–‡æ¡£åŠ è½½å®Œæˆ: æºæ–‡ä»¶æ•°={len(source_files)}, "
            f"è§£ææ–‡æ¡£æ•°={len(documents)}, æˆåŠŸç‡={success_rate:.1f}%, "
            f"æ€»è€—æ—¶={total_elapsed:.2f}s (è·å–è·¯å¾„={source_elapsed:.2f}s, "
            f"è§£æ={parser_elapsed:.2f}s, æ¸…ç†={clean_elapsed:.2f}s)"
        )
        
        return documents
        
    except Exception as e:
        logger.error(f"[é˜¶æ®µ1.3] æ–‡æ¡£åŠ è½½å¤±è´¥: {e}", exc_info=True)
        progress_reporter.report_error(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
        return []


def _create_progress_callback(progress_manager: "ImportProgressManager"):
    """åˆ›å»ºè¿›åº¦å›è°ƒå‡½æ•°"""
    def callback(current: int, total: int, filename: str = ""):
        progress_manager.update_progress(current, f"è§£æ: {filename}" if filename else None)
    return callback
