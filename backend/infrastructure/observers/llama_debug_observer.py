"""
LlamaDebug è§‚å¯Ÿå™¨
æä¾› LlamaIndex å†…ç½®çš„è°ƒè¯•æ—¥å¿—åŠŸèƒ½
"""

from typing import Any, Dict, List, Optional
from llama_index.core.callbacks import LlamaDebugHandler

from backend.infrastructure.observers.base import BaseObserver, ObserverType
from backend.infrastructure.logger import get_logger

logger = get_logger('llama_debug_observer')

# å°è¯•å¯¼å…¥ streamlitï¼ˆå¯é€‰ï¼‰
try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False
    st = None


class LlamaDebugObserver(BaseObserver):
    """LlamaDebug è§‚å¯Ÿå™¨
    
    æä¾› LlamaIndex å†…ç½®çš„è°ƒè¯•æ—¥å¿—åŠŸèƒ½
    """
    
    def __init__(
        self,
        name: str = "llama_debug",
        enabled: bool = True,
        print_trace_on_end: bool = True,
    ):
        """åˆå§‹åŒ– LlamaDebug è§‚å¯Ÿå™¨
        
        Args:
            name: è§‚å¯Ÿå™¨åç§°
            enabled: æ˜¯å¦å¯ç”¨
            print_trace_on_end: æ˜¯å¦åœ¨ç»“æŸæ—¶æ‰“å°è¿½è¸ªä¿¡æ¯
        """
        super().__init__(name, enabled)
        self.print_trace_on_end = print_trace_on_end
        self.handler = None
        
        if self.enabled:
            self.setup()
    
    def get_observer_type(self) -> ObserverType:
        return ObserverType.DEBUG
    
    def setup(self) -> None:
        """è®¾ç½® LlamaDebug"""
        logger.info("ğŸ› åˆå§‹åŒ– LlamaDebug è§‚å¯Ÿå™¨")
        
        try:
            self.handler = LlamaDebugHandler(
                print_trace_on_end=self.print_trace_on_end
            )
            
            logger.info("âœ… LlamaDebug è§‚å¯Ÿå™¨åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ LlamaDebug åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enabled = False
    
    def on_query_start(self, query: str, **kwargs) -> Optional[str]:
        """æŸ¥è¯¢å¼€å§‹æ—¶å›è°ƒ"""
        # LlamaDebugHandler è‡ªåŠ¨å¤„ç†
        return None
    
    def on_query_end(
        self,
        query: str,
        answer: str,
        sources: List[Dict],
        trace_id: Optional[str] = None,
        **kwargs
    ) -> None:
        """æŸ¥è¯¢ç»“æŸæ—¶å›è°ƒ"""
        # LlamaDebugHandler è‡ªåŠ¨å¤„ç†
        # åŒæ—¶å°†è°ƒè¯•ä¿¡æ¯å­˜å‚¨åˆ° session_state ä¾›å‰ç«¯æ˜¾ç¤º
        if self.handler:
            try:
                event_pairs = self.get_event_pairs()
                # æå–æ›´è¯¦ç»†çš„äº‹ä»¶ä¿¡æ¯
                event_details = []
                llm_calls = 0
                retrieval_calls = 0
                total_tokens = 0
                prompt_tokens = 0
                completion_tokens = 0
                llm_prompts = []
                llm_responses = []
                retrieval_queries = []
                retrieved_nodes = []
                event_type_counts = {}
                stage_times = {}
                
                for pair in event_pairs[:20]:  # ä¿å­˜å‰20ä¸ªäº‹ä»¶å¯¹
                    start_event = pair[0] if pair[0] else None
                    end_event = pair[1] if pair[1] else None
                    
                    event_type = None
                    event_type_str = None
                    if start_event and hasattr(start_event, 'event_type'):
                        event_type = start_event.event_type
                        event_type_str = str(event_type)
                        event_type_counts[event_type_str] = event_type_counts.get(event_type_str, 0) + 1
                    
                    # ç»Ÿè®¡äº‹ä»¶ç±»å‹
                    if event_type_str:
                        if 'llm' in event_type_str.lower():
                            llm_calls += 1
                        if 'retrieval' in event_type_str.lower() or 'retrieve' in event_type_str.lower():
                            retrieval_calls += 1
                    
                    # æå–äº‹ä»¶Payloadä¿¡æ¯
                    event_info = {
                        "event_type": event_type_str,
                        "start_event": str(start_event)[:500] if start_event else None,
                        "end_event": str(end_event)[:500] if end_event else None,
                        "payload": {},
                    }
                    
                    # æå–Payloadè¯¦æƒ…
                    if start_event and hasattr(start_event, 'payload'):
                        payload = start_event.payload
                        if isinstance(payload, dict):
                            event_info["payload"] = {}
                            
                            # æå–æ‰€æœ‰Payloadé”®å€¼å¯¹
                            for key, value in payload.items():
                                key_str = str(key)
                                
                                # LLMç›¸å…³
                                if 'prompt' in key_str.lower() or 'formatted_prompt' in key_str.lower():
                                    prompt_text = str(value)[:1000] if value else None
                                    if prompt_text:
                                        llm_prompts.append(prompt_text)
                                        event_info["payload"][key_str] = prompt_text
                                
                                if 'response' in key_str.lower() or 'message' in key_str.lower():
                                    response_text = str(value)[:1000] if value else None
                                    if response_text:
                                        llm_responses.append(response_text)
                                        event_info["payload"][key_str] = response_text
                                
                                # Tokenä¿¡æ¯
                                if 'token' in key_str.lower():
                                    if isinstance(value, (int, float)):
                                        total_tokens += value
                                        if 'prompt' in key_str.lower() or 'input' in key_str.lower():
                                            prompt_tokens += value
                                        elif 'completion' in key_str.lower() or 'output' in key_str.lower():
                                            completion_tokens += value
                                        event_info["payload"][key_str] = value
                                
                                # æ£€ç´¢ç›¸å…³
                                if 'query' in key_str.lower() and 'retrieval' in event_type_str.lower() if event_type_str else False:
                                    query_text = str(value)[:500] if value else None
                                    if query_text:
                                        retrieval_queries.append(query_text)
                                        event_info["payload"][key_str] = query_text
                                
                                # èŠ‚ç‚¹ä¿¡æ¯
                                if 'node' in key_str.lower() or 'chunk' in key_str.lower():
                                    if isinstance(value, (list, dict)):
                                        retrieved_nodes.append(str(value)[:500])
                                        event_info["payload"][key_str] = str(value)[:500]
                                
                                # å…¶ä»–é‡è¦ä¿¡æ¯
                                if key_str not in event_info["payload"]:
                                    # ä¿å­˜å…¶ä»–é‡è¦å­—æ®µï¼ˆé™åˆ¶é•¿åº¦ï¼‰
                                    if isinstance(value, (str, int, float, bool)):
                                        event_info["payload"][key_str] = str(value)[:200]
                    
                    # æå–æ—¶é—´ä¿¡æ¯
                    if start_event and hasattr(start_event, 'time'):
                        event_info["start_time"] = str(start_event.time)
                    if end_event and hasattr(end_event, 'time'):
                        event_info["end_time"] = str(end_event.time)
                        if start_event and hasattr(start_event, 'time'):
                            try:
                                duration = float(end_event.time) - float(start_event.time)
                                event_info["duration"] = duration
                                if event_type_str:
                                    stage_times[event_type_str] = stage_times.get(event_type_str, 0) + duration
                            except:
                                pass
                    
                    event_details.append(event_info)
                
                debug_info = {
                    # åŸºç¡€ä¿¡æ¯
                    "query": query,
                    "answer": answer[:500] + "..." if len(answer) > 500 else answer,
                    "answer_length": len(answer),
                    "sources_count": len(sources),
                    "sources": [
                        {
                            "text": src.get('text', '')[:200] if isinstance(src, dict) else str(src)[:200],
                            "score": src.get('score', 0) if isinstance(src, dict) else None,
                            "metadata": src.get('metadata', {}) if isinstance(src, dict) else {},
                            "id": src.get('id', None) if isinstance(src, dict) else None,
                        }
                        for src in sources[:10]  # ä¿å­˜å‰10ä¸ªæ¥æº
                    ],
                    
                    # äº‹ä»¶ç»Ÿè®¡
                    "events_count": len(event_pairs),
                    "llm_calls": llm_calls,
                    "retrieval_calls": retrieval_calls,
                    "event_types": list(set([
                        str(pair[0].event_type) if pair[0] and hasattr(pair[0], 'event_type') else 'unknown'
                        for pair in event_pairs[:20]
                    ])),
                    "event_type_counts": event_type_counts,
                    
                    # Tokenä¿¡æ¯
                    "total_tokens": total_tokens,
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    
                    # LLMè¯¦ç»†ä¿¡æ¯
                    "llm_prompts": llm_prompts[:5],  # ä¿å­˜å‰5ä¸ªprompt
                    "llm_responses": llm_responses[:5],  # ä¿å­˜å‰5ä¸ªresponse
                    
                    # æ£€ç´¢è¯¦ç»†ä¿¡æ¯
                    "retrieval_queries": retrieval_queries[:5],  # ä¿å­˜å‰5ä¸ªæ£€ç´¢æŸ¥è¯¢
                    "retrieved_nodes": retrieved_nodes[:5],  # ä¿å­˜å‰5ä¸ªèŠ‚ç‚¹
                    
                    # æ€§èƒ½æŒ‡æ ‡
                    "stage_times": stage_times,
                    "total_time": sum(stage_times.values()) if stage_times else None,
                    
                    # äº‹ä»¶å¯¹è¯¦æƒ…
                    "event_pairs": event_details,
                }
                
                # å­˜å‚¨åˆ° session_stateï¼ˆå¦‚æœ streamlit å¯ç”¨ï¼‰
                if STREAMLIT_AVAILABLE and hasattr(st, 'session_state'):
                    if 'llama_debug_logs' not in st.session_state:
                        st.session_state.llama_debug_logs = []
                    st.session_state.llama_debug_logs.append(debug_info)
                    
                    # åªä¿ç•™æœ€è¿‘50æ¡è®°å½•
                    if len(st.session_state.llama_debug_logs) > 50:
                        st.session_state.llama_debug_logs = st.session_state.llama_debug_logs[-50:]
                
                # æ‰“å°åˆ°æ§åˆ¶å°
                logger.info(f"ğŸ› LlamaDebug: æŸ¥è¯¢å®Œæˆï¼Œ{len(event_pairs)} ä¸ªäº‹ä»¶")
                print(f"\nğŸ› LlamaDebug è¿½è¸ªä¿¡æ¯:")
                print(f"   æŸ¥è¯¢: {query[:100]}...")
                print(f"   äº‹ä»¶æ•°: {len(event_pairs)}")
                if event_pairs:
                    print(f"   ç¬¬ä¸€ä¸ªäº‹ä»¶: {str(event_pairs[0][0])[:200]}...")
                
            except Exception as e:
                logger.warning(f"âš ï¸  ä¿å­˜ LlamaDebug ä¿¡æ¯å¤±è´¥: {e}")
    
    def get_callback_handler(self):
        """è·å– LlamaIndex å…¼å®¹çš„å›è°ƒå¤„ç†å™¨"""
        return self.handler
    
    def get_event_pairs(self):
        """è·å–äº‹ä»¶å¯¹"""
        if self.handler:
            return self.handler.get_event_pairs()
        return []
    
    def get_report(self) -> Dict[str, Any]:
        """è·å–è°ƒè¯•æŠ¥å‘Š"""
        report = {
            "observer": self.name,
            "type": self.get_observer_type().value,
            "enabled": self.enabled,
            "print_trace_on_end": self.print_trace_on_end,
        }
        
        if self.handler:
            event_pairs = self.get_event_pairs()
            report["events_count"] = len(event_pairs)
        
        return report
    
    def teardown(self) -> None:
        """æ¸…ç†èµ„æº"""
        logger.info("ğŸ§¹ æ¸…ç† LlamaDebug èµ„æº")

