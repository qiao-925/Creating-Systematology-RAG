"""
RAGASè¯„ä¼°å™¨è§‚å¯Ÿå™¨

ä½¿ç”¨RAGASæ¡†æ¶è¿›è¡ŒRAGç³»ç»Ÿè¯„ä¼°
"""

from typing import Any, Dict, List, Optional
from datetime import datetime

from backend.infrastructure.observers.base import BaseObserver, ObserverType
from backend.infrastructure.config import config
from backend.infrastructure.logger import get_logger

logger = get_logger('ragas_evaluator')

# å°è¯•å¯¼å…¥ streamlitï¼ˆå¯é€‰ï¼‰
try:
    import streamlit as st
    STREAMLIT_AVAILABLE = True
except ImportError:
    STREAMLIT_AVAILABLE = False
    st = None


class RAGASEvaluator(BaseObserver):
    """RAGASè¯„ä¼°å™¨è§‚å¯Ÿå™¨
    
    ä½¿ç”¨RAGASæ¡†æ¶è¿›è¡ŒRAGç³»ç»Ÿè¯„ä¼°
    æ”¯æŒå¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼š
    - faithfulnessï¼ˆå¿ å®åº¦ï¼‰
    - context_precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼‰
    - context_recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰
    - answer_relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰
    - context_relevancyï¼ˆä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼‰
    """
    
    def __init__(
        self,
        name: str = "ragas_evaluator",
        enabled: bool = True,
        metrics: Optional[List[str]] = None,
        batch_size: int = 10,
    ):
        """åˆå§‹åŒ–RAGASè¯„ä¼°å™¨
        
        Args:
            name: è§‚å¯Ÿå™¨åç§°
            enabled: æ˜¯å¦å¯ç”¨
            metrics: è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨ï¼ˆé»˜è®¤ä½¿ç”¨æ‰€æœ‰æŒ‡æ ‡ï¼‰
            batch_size: æ‰¹é‡è¯„ä¼°å¤§å°
        """
        super().__init__(name, enabled)
        # é»˜è®¤ä½¿ç”¨ RAGAS 0.4.3 æ”¯æŒçš„æ ¸å¿ƒæŒ‡æ ‡
        self.metrics = metrics or [
            "faithfulness",
            "context_precision",
            "context_recall",
            "answer_relevancy",
        ]
        self.batch_size = batch_size
        
        # è¯„ä¼°æ•°æ®å­˜å‚¨
        self.evaluation_data: List[Dict[str, Any]] = []
        self.evaluation_results: List[Dict[str, Any]] = []
        
        # RAGASç›¸å…³å¯¹è±¡
        self.dataset = None
        
        if self.enabled:
            self.setup()
    
    def get_observer_type(self) -> ObserverType:
        return ObserverType.EVALUATION
    
    def setup(self) -> None:
        """è®¾ç½®RAGASè¯„ä¼°å™¨"""
        logger.info("ğŸ“Š åˆå§‹åŒ– RAGAS è¯„ä¼°å™¨")
        
        try:
            # å»¶è¿Ÿå¯¼å…¥RAGASï¼ˆå› ä¸ºå®ƒæ˜¯å¯é€‰ä¾èµ–ï¼‰
            import ragas
            from ragas import evaluate
            from ragas.dataset_schema import EvaluationDataset, SingleTurnSample
            
            # å¯¼å…¥æŒ‡æ ‡ç±»ï¼ˆRAGAS 0.4.3+ APIï¼‰
            from ragas.metrics._faithfulness import Faithfulness
            from ragas.metrics._context_precision import ContextPrecision
            from ragas.metrics._context_recall import ContextRecall
            from ragas.metrics._answer_relevance import AnswerRelevancy
            
            self.ragas = ragas
            self.evaluate_func = evaluate
            self.EvaluationDataset = EvaluationDataset
            self.SingleTurnSample = SingleTurnSample
            
            # åˆ›å»ºæŒ‡æ ‡å®ä¾‹ï¼ˆRAGAS 0.4.3 å¯ç”¨æŒ‡æ ‡ï¼‰
            self.metric_instances = {
                "faithfulness": Faithfulness(),
                "context_precision": ContextPrecision(),
                "context_recall": ContextRecall(),
                "answer_relevancy": AnswerRelevancy(),
            }
            
            logger.info(f"âœ… RAGAS è¯„ä¼°å™¨å·²åˆå§‹åŒ– (ç‰ˆæœ¬: {ragas.__version__})")
            logger.info(f"   è¯„ä¼°æŒ‡æ ‡: {', '.join(self.metrics)}")
            
        except ImportError as e:
            logger.warning(f"âš ï¸  RAGAS æœªå®‰è£…: {e}")
            logger.info("   è¯·è¿è¡Œ: uv sync --extra evaluation")
            logger.info("   è§‚å¯Ÿå™¨å°†è¢«ç¦ç”¨")
            self.enabled = False
        except Exception as e:
            logger.error(f"âŒ RAGAS åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enabled = False
    
    def on_query_start(self, query: str, **kwargs) -> Optional[str]:
        """æŸ¥è¯¢å¼€å§‹æ—¶å›è°ƒ"""
        if not self.enabled:
            return None
        
        # è®°å½•æŸ¥è¯¢å¼€å§‹æ—¶é—´
        self.current_query_start = datetime.now()
        logger.debug(f"ğŸ” RAGAS è®°å½•æŸ¥è¯¢: {query[:50]}...")
        return None
    
    def on_query_end(
        self,
        query: str,
        answer: str,
        sources: List[Dict],
        trace_id: Optional[str] = None,
        **kwargs
    ) -> None:
        """æŸ¥è¯¢ç»“æŸæ—¶å›è°ƒ"""
        if not self.enabled:
            return
        
        try:
            # æå–ä¸Šä¸‹æ–‡
            contexts = []
            for source in sources:
                if isinstance(source, dict):
                    # ä»sourceä¸­æå–æ–‡æœ¬
                    context_text = source.get('text', '') or source.get('content', '')
                    if context_text:
                        contexts.append(context_text)
                elif hasattr(source, 'text'):
                    contexts.append(source.text)
            
            # è®°å½•è¯„ä¼°æ•°æ®
            evaluation_entry = {
                "question": query,
                "answer": answer,
                "contexts": contexts,
                "ground_truth": kwargs.get('ground_truth', None),  # å¯é€‰çš„çœŸå€¼
                "timestamp": datetime.now().isoformat(),
                "trace_id": trace_id,
            }
            
            self.evaluation_data.append(evaluation_entry)
            logger.debug(f"âœ… RAGAS è®°å½•æŸ¥è¯¢å®Œæˆ: {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")
            
            # å­˜å‚¨åˆ° session_state ä¾›å‰ç«¯æ˜¾ç¤ºï¼ˆå¦‚æœ streamlit å¯ç”¨ï¼‰
            if STREAMLIT_AVAILABLE and hasattr(st, 'session_state'):
                if 'ragas_logs' not in st.session_state:
                    st.session_state.ragas_logs = []
                
                log_entry = {
                    "query": query,
                    "answer": answer[:500] + "..." if len(answer) > 500 else answer,
                    "answer_length": len(answer),
                    "answer_preview": answer[:200] + "..." if len(answer) > 200 else answer,
                    "contexts_count": len(contexts),
                    "contexts": [
                        ctx[:500] + "..." if len(ctx) > 500 else ctx
                        for ctx in contexts[:10]  # ä¿å­˜å‰10ä¸ªä¸Šä¸‹æ–‡
                    ],
                    "contexts_full": contexts,  # ä¿å­˜å®Œæ•´ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ˆç”¨äºè¯„ä¼°ï¼‰
                    "timestamp": evaluation_entry["timestamp"],
                    "pending_evaluation": True,
                    "sources_count": len(sources),
                    "sources": [
                        {
                            "text": src.get('text', '')[:200] if isinstance(src, dict) else str(src)[:200],
                            "score": src.get('score', 0) if isinstance(src, dict) else None,
                            "metadata": src.get('metadata', {}) if isinstance(src, dict) else {},
                        }
                        for src in sources[:10]  # ä¿å­˜å‰10ä¸ªæ¥æº
                    ],
                    "trace_id": trace_id,
                    "ground_truth": kwargs.get('ground_truth', None),
                }
                st.session_state.ragas_logs.append(log_entry)
                
                # åªä¿ç•™æœ€è¿‘50æ¡è®°å½•
                if len(st.session_state.ragas_logs) > 50:
                    st.session_state.ragas_logs = st.session_state.ragas_logs[-50:]
            
            # æ‰“å°åˆ°æ§åˆ¶å°
            print(f"\nğŸ“Š RAGAS è¯„ä¼°æ•°æ®æ”¶é›†:")
            print(f"   æŸ¥è¯¢: {query[:100]}...")
            print(f"   ç­”æ¡ˆé•¿åº¦: {len(answer)} å­—ç¬¦")
            print(f"   ä¸Šä¸‹æ–‡æ•°é‡: {len(contexts)}")
            print(f"   å¾…è¯„ä¼°æ•°æ®: {len(self.evaluation_data)}/{self.batch_size}")
            
            # å¦‚æœè¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œæ‰§è¡Œæ‰¹é‡è¯„ä¼°
            if len(self.evaluation_data) >= self.batch_size:
                self._run_batch_evaluation()
                
        except Exception as e:
            logger.warning(f"âš ï¸  RAGAS è®°å½•æŸ¥è¯¢å¤±è´¥: {e}")
    
    def on_retrieval(self, query: str, nodes: List[Any], **kwargs) -> None:
        """æ£€ç´¢å®Œæˆæ—¶å›è°ƒï¼ˆå¯é€‰ï¼‰"""
        if not self.enabled:
            return
        
        # å¯ä»¥åœ¨è¿™é‡Œè®°å½•æ£€ç´¢ç›¸å…³çš„ä¿¡æ¯
        logger.debug(f"ğŸ“š RAGAS è®°å½•æ£€ç´¢: {len(nodes)} ä¸ªèŠ‚ç‚¹")
    
    def _run_batch_evaluation(self) -> None:
        """æ‰§è¡Œæ‰¹é‡è¯„ä¼°"""
        if not self.enabled or not self.evaluation_data:
            return
        
        try:
            logger.info(f"ğŸ“Š å¼€å§‹æ‰¹é‡è¯„ä¼°: {len(self.evaluation_data)} æ¡æ•°æ®")
            
            # å‡†å¤‡æ•°æ®é›†ï¼ˆRAGAS 0.4.3+ APIï¼‰
            samples = []
            for entry in self.evaluation_data:
                sample = self.SingleTurnSample(
                    user_input=entry["question"],
                    response=entry["answer"],
                    retrieved_contexts=entry["contexts"],
                    reference=entry.get("ground_truth"),  # å¯é€‰
                )
                samples.append(sample)
            
            # åˆ›å»ºè¯„ä¼°æ•°æ®é›†
            dataset = self.EvaluationDataset(samples=samples)
            
            # è·å–è¦ä½¿ç”¨çš„æŒ‡æ ‡å®ä¾‹
            metrics_to_use = [
                self.metric_instances[m] 
                for m in self.metrics 
                if m in self.metric_instances
            ]
            
            # æ‰§è¡Œè¯„ä¼°
            result = self.evaluate_func(
                dataset=dataset,
                metrics=metrics_to_use,
                show_progress=True,
            )
            
            # ä¿å­˜ç»“æœ
            evaluation_result = {
                "timestamp": datetime.now().isoformat(),
                "data_count": len(self.evaluation_data),
                "metrics": result.to_dict() if hasattr(result, 'to_dict') else str(result),
                "raw_result": result,
            }
            
            self.evaluation_results.append(evaluation_result)
            logger.info(f"âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ: {len(self.evaluation_data)} æ¡æ•°æ®")
            
            # æ›´æ–° session_state ä¸­çš„è¯„ä¼°ç»“æœï¼ˆå¦‚æœ streamlit å¯ç”¨ï¼‰
            if STREAMLIT_AVAILABLE and hasattr(st, 'session_state') and 'ragas_logs' in st.session_state:
                # æ ‡è®°æœ€è¿‘çš„å¾…è¯„ä¼°è®°å½•ä¸ºå·²è¯„ä¼°
                for log_entry in st.session_state.ragas_logs[-self.batch_size:]:
                    if log_entry.get('pending_evaluation'):
                        log_entry['pending_evaluation'] = False
                        
                        # æå–è¯„ä¼°ç»“æœ
                        eval_metrics = {}
                        if hasattr(result, 'to_dict'):
                            result_dict = result.to_dict()
                            # æå–æŒ‡æ ‡å€¼
                            for metric_name in self.metrics:
                                if metric_name in result_dict:
                                    eval_metrics[metric_name] = result_dict[metric_name]
                        elif isinstance(result, dict):
                            eval_metrics = result
                        elif hasattr(result, '__dict__'):
                            # å°è¯•ä»å¯¹è±¡å±æ€§ä¸­æå–
                            for metric_name in self.metrics:
                                if hasattr(result, metric_name):
                                    eval_metrics[metric_name] = getattr(result, metric_name)
                        
                        log_entry['evaluation_result'] = eval_metrics
                        log_entry['evaluation_timestamp'] = datetime.now().isoformat()
                        log_entry['evaluation_batch_size'] = len(self.evaluation_data)
            
            # æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦åˆ°æ§åˆ¶å°
            print(f"\nğŸ“Š RAGAS æ‰¹é‡è¯„ä¼°å®Œæˆ:")
            print(f"   è¯„ä¼°æ•°æ®é‡: {len(self.evaluation_data)}")
            if hasattr(result, '__dict__'):
                print(f"   è¯„ä¼°ç»“æœ:")
                for metric_name in self.metrics:
                    if hasattr(result, metric_name):
                        metric_value = getattr(result, metric_name)
                        print(f"     {metric_name}: {metric_value}")
                        logger.info(f"   {metric_name}: {metric_value}")
            
            # æ¸…ç©ºå·²è¯„ä¼°çš„æ•°æ®
            self.evaluation_data.clear()
            
        except Exception as e:
            logger.error(f"âŒ RAGAS æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
            logger.exception(e)
    
    def evaluate_all(self) -> Optional[Dict[str, Any]]:
        """è¯„ä¼°æ‰€æœ‰å·²æ”¶é›†çš„æ•°æ®
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not self.enabled:
            return None
        
        if not self.evaluation_data:
            logger.warning("âš ï¸  æ²¡æœ‰å¾…è¯„ä¼°çš„æ•°æ®")
            return None
        
        # æ‰§è¡Œæœ€åä¸€æ¬¡æ‰¹é‡è¯„ä¼°
        self._run_batch_evaluation()
        
        if not self.evaluation_results:
            return None
        
        # è¿”å›æœ€æ–°çš„è¯„ä¼°ç»“æœ
        return self.evaluation_results[-1] if self.evaluation_results else None
    
    def get_report(self) -> Dict[str, Any]:
        """è·å–è¯„ä¼°æŠ¥å‘Š"""
        report = {
            "observer_type": "ragas_evaluator",
            "enabled": self.enabled,
            "metrics": self.metrics,
            "pending_evaluations": len(self.evaluation_data),
            "completed_evaluations": len(self.evaluation_results),
            "latest_result": self.evaluation_results[-1] if self.evaluation_results else None,
        }
        
        return report
    
    def teardown(self) -> None:
        """æ¸…ç†èµ„æº"""
        if not self.enabled:
            return
        
        # å¦‚æœæœ‰æœªè¯„ä¼°çš„æ•°æ®ï¼Œæ‰§è¡Œæœ€åä¸€æ¬¡è¯„ä¼°
        if self.evaluation_data:
            logger.info(f"ğŸ“Š æ¸…ç†å‰æ‰§è¡Œæœ€åä¸€æ¬¡è¯„ä¼°: {len(self.evaluation_data)} æ¡æ•°æ®")
            self._run_batch_evaluation()
        
        logger.info("âœ… RAGAS è¯„ä¼°å™¨å·²æ¸…ç†")

