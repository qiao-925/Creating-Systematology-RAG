"""
æ­£å¸¸æ¨¡å¼æ„å»ºæ¨¡å—ï¼šæ‰¹é‡ä¼˜åŒ–çš„ç´¢å¼•æ„å»º

ä¼˜åŒ–ç‚¹ï¼š
1. æ‰¹é‡åˆ†å—ï¼šä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ–‡æ¡£
2. æ‰¹é‡æ’å…¥ï¼šä½¿ç”¨ insert_nodes() æ‰¹é‡æ’å…¥
3. æ‰¹é‡æŸ¥è¯¢ï¼šåˆå¹¶å‘é‡IDæŸ¥è¯¢å‡å°‘ç½‘ç»œè¯·æ±‚
"""

import time
from typing import List, Tuple, Dict, Optional, Callable, TYPE_CHECKING

from tqdm import tqdm
from llama_index.core import VectorStoreIndex
from llama_index.core.schema import Document as LlamaDocument

from backend.infrastructure.config import config
from backend.infrastructure.logger import get_logger
from backend.infrastructure.indexer.utils.ids import get_vector_ids_with_retry

if TYPE_CHECKING:
    from backend.infrastructure.data_loader.github_sync.manager import GitHubSyncManager

logger = get_logger('indexer')


def _collect_metadata(documents: List[LlamaDocument]) -> Dict[str, Dict]:
    """æ‰¹é‡æ”¶é›†æ–‡æ¡£å…ƒæ•°æ®
    
    Args:
        documents: æ–‡æ¡£åˆ—è¡¨
        
    Returns:
        æ–‡ä»¶è·¯å¾„åˆ°å…ƒæ•°æ®çš„æ˜ å°„
    """
    metadata_map = {}
    for doc in documents:
        file_path = doc.metadata.get("file_path", "")
        if file_path:
            metadata_map[file_path] = {
                "repository": doc.metadata.get("repository", ""),
                "branch": doc.metadata.get("branch", "main"),
                "owner": doc.metadata.get("owner", ""),
                "repo": doc.metadata.get("repo", "")
            }
    return metadata_map


def _batch_query_vector_ids(
    index_manager,
    documents: List[LlamaDocument],
    show_progress: bool = True
) -> Dict[str, List[str]]:
    """æ‰¹é‡æŸ¥è¯¢å‘é‡ID
    
    Args:
        index_manager: IndexManagerå®ä¾‹
        documents: æ–‡æ¡£åˆ—è¡¨
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        
    Returns:
        æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDåˆ—è¡¨çš„æ˜ å°„
    """
    vector_ids_map = {}
    file_paths = [doc.metadata.get("file_path", "") for doc in documents if doc.metadata.get("file_path")]
    
    if not file_paths:
        return vector_ids_map
    
    total = len(file_paths)
    logger.info(f"[é˜¶æ®µ2.3] ğŸ” æ‰¹é‡æŸ¥è¯¢å‘é‡ID: {total} ä¸ªæ–‡ä»¶")
    
    # æ‰¹é‡æŸ¥è¯¢ï¼Œæ¯ 20 ä¸ªä¸€ç»„
    batch_size = 20
    for i in range(0, total, batch_size):
        batch_paths = file_paths[i:i + batch_size]
        
        for file_path in batch_paths:
            try:
                vector_ids = get_vector_ids_with_retry(index_manager, file_path)
                vector_ids_map[file_path] = vector_ids
            except Exception as e:
                logger.warning(f"æŸ¥è¯¢å‘é‡IDå¤±è´¥ [{file_path}]: {e}")
                vector_ids_map[file_path] = []
        
        if show_progress and (i + batch_size) < total:
            logger.debug(f"   å‘é‡IDæŸ¥è¯¢è¿›åº¦: {min(i + batch_size, total)}/{total}")
    
    return vector_ids_map


def build_index_normal_mode(
    index_manager,
    documents: List[LlamaDocument],
    show_progress: bool = True,
    github_sync_manager: Optional["GitHubSyncManager"] = None,
    progress_callback: Optional[Callable[[int, int], None]] = None
) -> Tuple[VectorStoreIndex, Dict[str, List[str]], Dict[str, Dict]]:
    """æ‰¹é‡å¤„ç†æ¨¡å¼æ„å»ºç´¢å¼•
    
    ä¼˜åŒ–åçš„æµç¨‹ï¼š
    1. æ‰¹é‡æ”¶é›†å…ƒæ•°æ®
    2. æ‰¹é‡åˆ†å—æ‰€æœ‰æ–‡æ¡£
    3. æ‰¹é‡æ’å…¥èŠ‚ç‚¹
    4. æ‰¹é‡æŸ¥è¯¢å‘é‡ID
    
    Args:
        index_manager: IndexManagerå®ä¾‹
        documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆå·²è¿‡æ»¤ï¼ŒåªåŒ…å«æœªå‘é‡åŒ–çš„æ–‡æ¡£ï¼‰
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        github_sync_manager: GitHubåŒæ­¥ç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œç­¾å (current, total) -> None
        
    Returns:
        (ç´¢å¼•, å‘é‡IDæ˜ å°„, æ–‡æ¡£å…ƒæ•°æ®æ˜ å°„)
    """
    if not documents:
        index = index_manager.get_index()
        return index, {}, {}
    
    total_docs = len(documents)
    logger.info(f"[é˜¶æ®µ2.1] ğŸ”¨ å¼€å§‹æ‰¹é‡æ„å»ºç´¢å¼•ï¼Œæ–‡æ¡£æ•°: {total_docs}")
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°ç´¢å¼•
    need_create_new = (
        index_manager._index is None or 
        getattr(index_manager, '_collection_is_empty', False)
    )
    
    if not need_create_new and hasattr(index_manager, 'chroma_collection') and index_manager.chroma_collection:
        try:
            count = index_manager.chroma_collection.count()
            if count == 0:
                need_create_new = True
                logger.info("[é˜¶æ®µ2.1] â„¹ï¸  æ£€æµ‹åˆ° Collection ä¸ºç©ºï¼Œå°†åˆ›å»ºæ–°ç´¢å¼•")
        except Exception:
            pass
    
    # é˜¶æ®µ1: æ‰¹é‡æ”¶é›†å…ƒæ•°æ®
    metadata_start = time.time()
    metadata_map = _collect_metadata(documents)
    logger.debug(f"[é˜¶æ®µ2.1] å…ƒæ•°æ®æ”¶é›†å®Œæˆ: {len(metadata_map)} ä¸ªæ–‡ä»¶ ({time.time() - metadata_start:.2f}s)")
    
    # åˆå§‹åŒ–åˆ†å—å™¨
    from llama_index.core.node_parser import SentenceSplitter
    node_parser = SentenceSplitter(
        chunk_size=index_manager.chunk_size,
        chunk_overlap=index_manager.chunk_overlap
    )
    
    logger.info(f"[é˜¶æ®µ2.1]    åˆ†å—å‚æ•°: size={index_manager.chunk_size}, overlap={index_manager.chunk_overlap}")
    
    if need_create_new:
        # åˆ›å»ºæ–°ç´¢å¼•
        logger.info("[é˜¶æ®µ2.1]    æ¨¡å¼: åˆ›å»ºæ–°ç´¢å¼•ï¼ˆæ‰¹é‡å¤„ç†ï¼‰")
        index_start_time = time.time()
        
        try:
            llama_embed_model = index_manager._get_llama_index_compatible_embedding()
            
            if progress_callback:
                # æœ‰è¿›åº¦å›è°ƒæ—¶ï¼šå…ˆåˆ†å—å†é€æ‰¹æ’å…¥ï¼ˆæ”¯æŒè¿›åº¦åé¦ˆï¼‰
                from llama_index.core.node_parser import SentenceSplitter
                node_parser_new = SentenceSplitter(
                    chunk_size=index_manager.chunk_size,
                    chunk_overlap=index_manager.chunk_overlap
                )
                all_nodes = node_parser_new.get_nodes_from_documents(documents, show_progress=show_progress)
                total_nodes = len(all_nodes)
                
                logger.info(f"[é˜¶æ®µ2.1] âœ… åˆ†å—å®Œæˆ: {total_nodes} ä¸ªèŠ‚ç‚¹")
                
                # ä½¿ç”¨ç¬¬ä¸€æ‰¹èŠ‚ç‚¹åˆ›å»ºç´¢å¼•
                batch_size = config.EMBED_BATCH_SIZE * 5
                first_batch = all_nodes[:batch_size]
                remaining_nodes = all_nodes[batch_size:]
                
                # ç”¨ç¬¬ä¸€æ‰¹èŠ‚ç‚¹åˆ›å»ºç´¢å¼•ï¼ˆé¿å…ç©ºç´¢å¼•é—®é¢˜ï¼‰
                index_manager._index = VectorStoreIndex(
                    nodes=first_batch,
                    storage_context=index_manager.storage_context,
                    embed_model=llama_embed_model,
                    show_progress=show_progress,
                )
                
                processed_nodes = len(first_batch)
                progress_callback(processed_nodes, total_nodes)
                
                if show_progress:
                    pbar = tqdm(total=total_nodes, initial=processed_nodes, desc="æ’å…¥èŠ‚ç‚¹", unit="node")
                
                # æ’å…¥å‰©ä½™èŠ‚ç‚¹
                for i in range(0, len(remaining_nodes), batch_size):
                    batch_nodes = remaining_nodes[i:i + batch_size]
                    if hasattr(index_manager._index, 'insert_nodes'):
                        index_manager._index.insert_nodes(batch_nodes)
                    else:
                        for node in batch_nodes:
                            index_manager._index.insert(node)
                    
                    processed_nodes += len(batch_nodes)
                    
                    if show_progress:
                        pbar.update(len(batch_nodes))
                    
                    progress_callback(processed_nodes, total_nodes)
                
                if show_progress:
                    pbar.close()
                
                # æœ€ç»ˆå›è°ƒç¡®ä¿ 100%
                progress_callback(total_nodes, total_nodes)
            else:
                # æ— è¿›åº¦å›è°ƒæ—¶ï¼šä½¿ç”¨åŸå§‹æ–¹å¼ï¼ˆLlamaIndex å†…éƒ¨æ‰¹é‡å¤„ç†ï¼‰
                index_manager._index = VectorStoreIndex.from_documents(
                    documents,
                    storage_context=index_manager.storage_context,
                    embed_model=llama_embed_model,
                    show_progress=show_progress,
                )
            
            if hasattr(index_manager, '_collection_is_empty'):
                delattr(index_manager, '_collection_is_empty')
            
            index_elapsed = time.time() - index_start_time
            logger.info(f"[é˜¶æ®µ2.2] âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ (è€—æ—¶: {index_elapsed:.2f}s)")
            
        except Exception as e:
            logger.error(f"[é˜¶æ®µ2.1/2.2] âŒ ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}", exc_info=True)
            raise
    else:
        # å¢é‡æ·»åŠ  - æ‰¹é‡åˆ†å—å’Œæ’å…¥
        logger.info("[é˜¶æ®µ2.1]    æ¨¡å¼: å¢é‡æ·»åŠ ï¼ˆæ‰¹é‡ä¼˜åŒ–ï¼‰")
        insert_start_time = time.time()
        
        try:
            if index_manager._index is None:
                index_manager.get_index()
            
            # é˜¶æ®µ2: æ‰¹é‡åˆ†å—æ‰€æœ‰æ–‡æ¡£
            chunk_start = time.time()
            logger.info(f"[é˜¶æ®µ2.1] ğŸ“„ æ‰¹é‡åˆ†å— {total_docs} ä¸ªæ–‡æ¡£...")
            
            all_nodes = node_parser.get_nodes_from_documents(documents, show_progress=show_progress)
            
            chunk_elapsed = time.time() - chunk_start
            logger.info(f"[é˜¶æ®µ2.1] âœ… åˆ†å—å®Œæˆ: {len(all_nodes)} ä¸ªèŠ‚ç‚¹ (è€—æ—¶: {chunk_elapsed:.2f}s)")
            
            # é˜¶æ®µ3: æ‰¹é‡æ’å…¥èŠ‚ç‚¹
            insert_start = time.time()
            batch_size = config.EMBED_BATCH_SIZE * 5  # å¢å¤§æ‰¹æ¬¡å¤§å°
            total_nodes = len(all_nodes)
            
            logger.info(f"[é˜¶æ®µ2.2] ğŸ“¤ æ‰¹é‡æ’å…¥ {total_nodes} ä¸ªèŠ‚ç‚¹...")
            
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            if show_progress:
                pbar = tqdm(total=total_nodes, desc="æ’å…¥èŠ‚ç‚¹", unit="node")
            
            # è¿›åº¦å›è°ƒæ›´æ–°é—´éš”ï¼ˆæ¯ 10 ä¸ªèŠ‚ç‚¹ï¼‰
            callback_interval = 10
            processed_nodes = 0
            
            for i in range(0, total_nodes, batch_size):
                batch_nodes = all_nodes[i:i + batch_size]
                try:
                    if hasattr(index_manager._index, 'insert_nodes'):
                        index_manager._index.insert_nodes(batch_nodes)
                    else:
                        for node in batch_nodes:
                            index_manager._index.insert(node)
                    
                    processed_nodes += len(batch_nodes)
                    
                    if show_progress:
                        pbar.update(len(batch_nodes))
                    
                    # è°ƒç”¨è¿›åº¦å›è°ƒ
                    if progress_callback:
                        progress_callback(processed_nodes, total_nodes)
                        
                except Exception as insert_error:
                    logger.warning(f"æ‰¹æ¬¡æ’å…¥å¤±è´¥ (æ‰¹æ¬¡ {i//batch_size + 1}): {insert_error}")
                    # å•ä¸ªèŠ‚ç‚¹é‡è¯•
                    for node in batch_nodes:
                        try:
                            index_manager._index.insert(node)
                            processed_nodes += 1
                            if show_progress:
                                pbar.update(1)
                            # å•èŠ‚ç‚¹æ¨¡å¼ä¸‹ï¼Œæ¯ callback_interval ä¸ªèŠ‚ç‚¹å›è°ƒä¸€æ¬¡
                            if progress_callback and processed_nodes % callback_interval == 0:
                                progress_callback(processed_nodes, total_nodes)
                        except Exception:
                            pass
            
            if show_progress:
                pbar.close()
            
            # æœ€ç»ˆå›è°ƒç¡®ä¿ 100%
            if progress_callback:
                progress_callback(total_nodes, total_nodes)
            
            insert_elapsed = time.time() - insert_start
            logger.info(f"[é˜¶æ®µ2.2] âœ… æ’å…¥å®Œæˆ (è€—æ—¶: {insert_elapsed:.2f}s)")
            
            total_elapsed = time.time() - insert_start_time
            logger.info(f"[é˜¶æ®µ2.2] âœ… å¢é‡æ·»åŠ å®Œæˆï¼Œå…± {total_nodes} ä¸ªèŠ‚ç‚¹ (æ€»è€—æ—¶: {total_elapsed:.2f}s)")
            
        except Exception as e:
            logger.error(f"[é˜¶æ®µ2.1/2.2] âŒ å¢é‡æ·»åŠ å¤±è´¥: {e}", exc_info=True)
            raise
    
    # é˜¶æ®µ4: æ‰¹é‡æŸ¥è¯¢å‘é‡ID
    vector_ids_map = _batch_query_vector_ids(index_manager, documents, show_progress)
    
    return index_manager._index, vector_ids_map, metadata_map
