"""
æ­£å¸¸æ¨¡å¼æ„å»ºæ¨¡å—ï¼šæ­£å¸¸æ¨¡å¼æ„å»ºç´¢å¼•
"""

import time
from typing import List, Tuple, Dict

from tqdm import tqdm
from llama_index.core import VectorStoreIndex
from llama_index.core.schema import Document as LlamaDocument

from backend.infrastructure.config import config
from backend.infrastructure.logger import get_logger

logger = get_logger('indexer')


def build_index_normal_mode(
    index_manager,
    documents: List[LlamaDocument],
    show_progress: bool = True
) -> Tuple[VectorStoreIndex, Dict[str, List[str]]]:
    """æ­£å¸¸æ¨¡å¼æ„å»ºç´¢å¼•"""
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°ç´¢å¼•ï¼ˆ_index ä¸º None æˆ– collection ä¸ºç©ºï¼‰
    need_create_new = (
        index_manager._index is None or 
        getattr(index_manager, '_collection_is_empty', False)
    )
    
    # å¦‚æœ collection ä¸ºç©ºï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„ä¸ºç©º
    if not need_create_new and hasattr(index_manager, 'chroma_collection') and index_manager.chroma_collection:
        try:
            count = index_manager.chroma_collection.count()
            if count == 0:
                need_create_new = True
                logger.info("[é˜¶æ®µ2.1] â„¹ï¸  æ£€æµ‹åˆ° Collection ä¸ºç©ºï¼Œå°†åˆ›å»ºæ–°ç´¢å¼•")
        except Exception:
            pass
    
    if need_create_new:
        logger.info(f"[é˜¶æ®µ2.1] ğŸ”¨ å¼€å§‹åˆ›å»ºç´¢å¼•ï¼Œæ–‡æ¡£æ•°: {len(documents)}")
        logger.info(f"[é˜¶æ®µ2.1]    åˆ†å—å‚æ•°: size={index_manager.chunk_size}, overlap={index_manager.chunk_overlap}")
        
        index_start_time = time.time()
        try:
            logger.info("[é˜¶æ®µ2.1/2.2/2.3] ğŸ“ æ­¥éª¤1: æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–ä¸­...")
            # è·å–LlamaIndexå…¼å®¹çš„embeddingå®ä¾‹
            llama_embed_model = index_manager._get_llama_index_compatible_embedding()
            index_manager._index = VectorStoreIndex.from_documents(
                documents,
                storage_context=index_manager.storage_context,
                embed_model=llama_embed_model,
            )
            # æ¸…é™¤ç©ºæ ‡è®°
            if hasattr(index_manager, '_collection_is_empty'):
                delattr(index_manager, '_collection_is_empty')
            index_elapsed = time.time() - index_start_time
            logger.info(f"[é˜¶æ®µ2.3] âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ (è€—æ—¶: {index_elapsed:.2f}s)")
        except Exception as e:
            logger.error(f"[é˜¶æ®µ2.1/2.2/2.3] âŒ ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}", exc_info=True)
            raise
    else:
        # å¢é‡æ·»åŠ æ–‡æ¡£
        logger.info(f"[é˜¶æ®µ2.1] ğŸ“ å¼€å§‹å¢é‡æ·»åŠ æ–‡æ¡£ï¼Œæ–‡æ¡£æ•°: {len(documents)}")
        insert_start_time = time.time()
        try:
            logger.info("[é˜¶æ®µ2.1/2.2/2.3] ğŸ“ æ­¥éª¤1: æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–ä¸­...")
            # å°è¯•ä½¿ç”¨ insert_ref_docsï¼Œå¦‚æœå¤±è´¥åˆ™å›é€€åˆ°èŠ‚ç‚¹æ‰¹é‡æ’å…¥
            try:
                index_manager._index.insert_ref_docs(documents, show_progress=show_progress)
            except TypeError:
                # å¦‚æœ insert_ref_docs ä¸æ”¯æŒ show_progress å‚æ•°ï¼Œåˆ™ä¸å¸¦å‚æ•°è°ƒç”¨
                index_manager._index.insert_ref_docs(documents)
            insert_elapsed = time.time() - insert_start_time
            logger.info(f"[é˜¶æ®µ2.3] âœ… æ–‡æ¡£å·²æ‰¹é‡æ·»åŠ åˆ°ç°æœ‰ç´¢å¼• (è€—æ—¶: {insert_elapsed:.2f}s)")
        except AttributeError:
            # å›é€€åˆ°èŠ‚ç‚¹æ‰¹é‡æ’å…¥
            from llama_index.core.node_parser import SentenceSplitter
            node_parser = SentenceSplitter(
                chunk_size=index_manager.chunk_size,
                chunk_overlap=index_manager.chunk_overlap
            )
            
            all_nodes = []
            if show_progress:
                logger.debug("[é˜¶æ®µ2.1]    æ­£åœ¨åˆ†å—æ–‡æ¡£...")
            for doc in tqdm(documents, desc="åˆ†å—", disable=not show_progress, unit="doc"):
                nodes = node_parser.get_nodes_from_documents([doc])
                all_nodes.extend(nodes)
            
            total_nodes = len(all_nodes)
            batch_size = config.EMBED_BATCH_SIZE
            inserted_count = 0
            
            if show_progress:
                pbar = tqdm(total=total_nodes, desc="å‘é‡åŒ–å¹¶æ’å…¥", unit="node")
            
            batch_start_time = time.time()
            try:
                if hasattr(index_manager._index, 'insert_nodes'):
                    for i in range(0, len(all_nodes), batch_size):
                        batch_nodes = all_nodes[i:i+batch_size]
                        index_manager._index.insert_nodes(batch_nodes)
                        inserted_count += len(batch_nodes)
                        if show_progress:
                            pbar.update(len(batch_nodes))
                else:
                    raise AttributeError("insert_nodes not available")
            except (AttributeError, TypeError):
                for i in range(0, len(all_nodes), batch_size):
                    batch_nodes = all_nodes[i:i+batch_size]
                    for node in batch_nodes:
                        index_manager._index.insert(node)
                    inserted_count += len(batch_nodes)
                    if show_progress:
                        pbar.update(len(batch_nodes))
            
            if show_progress:
                pbar.close()
            
            insert_elapsed = time.time() - insert_start_time
            avg_rate = total_nodes / insert_elapsed if insert_elapsed > 0 else 0
            logger.info(f"[é˜¶æ®µ2.3] âœ… æ–‡æ¡£å·²æ‰¹é‡æ·»åŠ åˆ°ç°æœ‰ç´¢å¼• (è€—æ—¶: {insert_elapsed:.2f}s, å¹³å‡é€Ÿç‡: {avg_rate:.1f} nodes/s)")
    
    return index_manager._index, {}
