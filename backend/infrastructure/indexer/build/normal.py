"""
æ­£å¸¸æ¨¡å¼æ„å»ºæ¨¡å—ï¼šæ­£å¸¸æ¨¡å¼æ„å»ºç´¢å¼•
"""

import time
from typing import List, Tuple, Dict, Optional, TYPE_CHECKING

from tqdm import tqdm
from llama_index.core import VectorStoreIndex
from llama_index.core.schema import Document as LlamaDocument

from backend.infrastructure.config import config
from backend.infrastructure.logger import get_logger
from backend.infrastructure.indexer.utils.ids import get_vector_ids_with_retry

if TYPE_CHECKING:
    from backend.infrastructure.data_loader.github_sync.manager import GitHubSyncManager

logger = get_logger('indexer')


def build_index_normal_mode(
    index_manager,
    documents: List[LlamaDocument],
    show_progress: bool = True,
    github_sync_manager: Optional["GitHubSyncManager"] = None
) -> Tuple[VectorStoreIndex, Dict[str, List[str]], Dict[str, Dict]]:
    """æŒ‰æ–‡æ¡£é€ä¸ªå¤„ç†ï¼Œè¿”å›å‘é‡IDæ˜ å°„å’Œæ–‡æ¡£å…ƒæ•°æ®æ˜ å°„
    
    Args:
        index_manager: IndexManagerå®ä¾‹
        documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆå·²è¿‡æ»¤ï¼ŒåªåŒ…å«æœªå‘é‡åŒ–çš„æ–‡æ¡£ï¼‰
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        github_sync_manager: GitHubåŒæ­¥ç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        (ç´¢å¼•, å‘é‡IDæ˜ å°„, æ–‡æ¡£å…ƒæ•°æ®æ˜ å°„)
    """
    if not documents:
        index = index_manager.get_index()
        return index, {}, {}
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºæ–°ç´¢å¼•ï¼ˆ_index ä¸º None æˆ– collection ä¸ºç©ºï¼‰
    need_create_new = (
        index_manager._index is None or 
        getattr(index_manager, '_collection_is_empty', False)
    )
    
    # å¦‚æœ collection ä¸ºç©ºï¼Œæ£€æŸ¥æ˜¯å¦çœŸçš„ä¸ºç©º
    if not need_create_new and hasattr(index_manager, 'chroma_collection') and index_manager.chroma_collection:
        try:
            count = index_manager.chroma_collection.count()
            if count == 0:
                need_create_new = True
                logger.info("[é˜¶æ®µ2.1] â„¹ï¸  æ£€æµ‹åˆ° Collection ä¸ºç©ºï¼Œå°†åˆ›å»ºæ–°ç´¢å¼•")
        except Exception:
            pass
    
    vector_ids_map = {}
    metadata_map = {}  # æ–‡æ¡£å…ƒæ•°æ®æ˜ å°„
    
    # åˆå§‹åŒ–åˆ†å—å™¨
    from llama_index.core.node_parser import SentenceSplitter
    node_parser = SentenceSplitter(
        chunk_size=index_manager.chunk_size,
        chunk_overlap=index_manager.chunk_overlap
    )
    
    batch_size = config.EMBED_BATCH_SIZE
    
    if need_create_new:
        logger.info(f"[é˜¶æ®µ2.1] ğŸ”¨ å¼€å§‹åˆ›å»ºç´¢å¼•ï¼Œæ–‡æ¡£æ•°: {len(documents)}")
        logger.info(f"[é˜¶æ®µ2.1]    åˆ†å—å‚æ•°: size={index_manager.chunk_size}, overlap={index_manager.chunk_overlap}")
        logger.info("[é˜¶æ®µ2.1]    æŒ‰æ–‡æ¡£é€ä¸ªå¤„ç†æ¨¡å¼")
        
        index_start_time = time.time()
        try:
            # è·å–LlamaIndexå…¼å®¹çš„embeddingå®ä¾‹
            llama_embed_model = index_manager._get_llama_index_compatible_embedding()
            
            # æŒ‰æ–‡æ¡£é€ä¸ªå¤„ç†ï¼Œæ”¶é›†å…ƒæ•°æ®
            for doc_idx, doc in enumerate(documents, 1):
                file_path = doc.metadata.get("file_path", "")
                
                # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®ï¼ˆç”¨äºä¸­é—´å±‚æå–owner/repo/branchï¼‰
                if file_path:
                    metadata_map[file_path] = {
                        "repository": doc.metadata.get("repository", ""),
                        "branch": doc.metadata.get("branch", "main"),
                        "owner": doc.metadata.get("owner", ""),
                        "repo": doc.metadata.get("repo", "")
                    }
                
                if show_progress and (doc_idx % 10 == 0 or doc_idx == len(documents)):
                    logger.info(f"[é˜¶æ®µ2.1]    å¤„ç†è¿›åº¦: {doc_idx}/{len(documents)}")
            
            # åˆ›å»ºç´¢å¼•ï¼ˆä½¿ç”¨æ‰€æœ‰æ–‡æ¡£ï¼‰
            index_manager._index = VectorStoreIndex.from_documents(
                documents,
                storage_context=index_manager.storage_context,
                embed_model=llama_embed_model,
            )
            
            # æ¸…é™¤ç©ºæ ‡è®°
            if hasattr(index_manager, '_collection_is_empty'):
                delattr(index_manager, '_collection_is_empty')
            
            # æŸ¥è¯¢å‘é‡IDï¼ˆå¸¦é‡è¯•ï¼‰
            for doc in documents:
                file_path = doc.metadata.get("file_path", "")
                if file_path:
                    vector_ids = get_vector_ids_with_retry(index_manager, file_path)
                    vector_ids_map[file_path] = vector_ids
            
            index_elapsed = time.time() - index_start_time
            logger.info(f"[é˜¶æ®µ2.3] âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ (è€—æ—¶: {index_elapsed:.2f}s)")
        except Exception as e:
            logger.error(f"[é˜¶æ®µ2.1/2.2/2.3] âŒ ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}", exc_info=True)
            raise
    else:
        # å¢é‡æ·»åŠ æ–‡æ¡£ - æŒ‰æ–‡æ¡£é€ä¸ªå¤„ç†
        logger.info(f"[é˜¶æ®µ2.1] ğŸ“ å¼€å§‹å¢é‡æ·»åŠ æ–‡æ¡£ï¼Œæ–‡æ¡£æ•°: {len(documents)}")
        logger.info("[é˜¶æ®µ2.1]    æŒ‰æ–‡æ¡£é€ä¸ªå¤„ç†æ¨¡å¼")
        insert_start_time = time.time()
        
        try:
            # ç¡®ä¿ç´¢å¼•å­˜åœ¨
            if index_manager._index is None:
                index_manager.get_index()
            
            # æŒ‰æ–‡æ¡£é€ä¸ªå¤„ç†
            doc_progress = tqdm(documents, desc="å¤„ç†æ–‡æ¡£", disable=not show_progress, unit="doc") if show_progress else documents
            
            for doc in doc_progress:
                file_path = doc.metadata.get("file_path", "")
                
                # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®ï¼ˆç”¨äºä¸­é—´å±‚æå–owner/repo/branchï¼‰
                if file_path:
                    metadata_map[file_path] = {
                        "repository": doc.metadata.get("repository", ""),
                        "branch": doc.metadata.get("branch", "main"),
                        "owner": doc.metadata.get("owner", ""),
                        "repo": doc.metadata.get("repo", "")
                    }
                
                # åˆ†å—ï¼ˆä¸å†æ£€æŸ¥æ˜¯å¦å·²å‘é‡åŒ–ï¼Œå› ä¸ºå·²ç”±filterè¿‡æ»¤ï¼‰
                nodes = node_parser.get_nodes_from_documents([doc])
                
                # æ‰¹é‡ä¸Šä¼ ï¼ˆæ¯10ä¸ªchunksä¸€æ‰¹ï¼‰
                for i in range(0, len(nodes), batch_size):
                    batch_nodes = nodes[i:i+batch_size]
                    try:
                        if hasattr(index_manager._index, 'insert_nodes'):
                            index_manager._index.insert_nodes(batch_nodes)
                        else:
                            for node in batch_nodes:
                                index_manager._index.insert(node)
                    except Exception as insert_error:
                        logger.warning(f"æ’å…¥èŠ‚ç‚¹å¤±è´¥ [{file_path}] (æ‰¹æ¬¡ {i//batch_size + 1}): {insert_error}")
                        # ç»§ç»­å¤„ç†å…¶ä»–èŠ‚ç‚¹
                        continue
                
                # æŸ¥è¯¢å‘é‡IDï¼ˆå¸¦é‡è¯•ï¼‰
                if file_path:
                    vector_ids = get_vector_ids_with_retry(index_manager, file_path)
                    vector_ids_map[file_path] = vector_ids
            
            if show_progress and hasattr(doc_progress, 'close'):
                doc_progress.close()
            
            insert_elapsed = time.time() - insert_start_time
            logger.info(f"[é˜¶æ®µ2.3] âœ… æ–‡æ¡£å·²æŒ‰æ–‡æ¡£é€ä¸ªæ·»åŠ åˆ°ç°æœ‰ç´¢å¼• (è€—æ—¶: {insert_elapsed:.2f}s)")
        except Exception as e:
            logger.error(f"[é˜¶æ®µ2.1/2.2/2.3] âŒ å¢é‡æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
            raise
    
    return index_manager._index, vector_ids_map, metadata_map
