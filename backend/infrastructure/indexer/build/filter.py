"""
æ–‡æ¡£è¿‡æ»¤æ¨¡å—ï¼šè¿‡æ»¤å·²å‘é‡åŒ–çš„æ–‡æ¡£ï¼Œå®ç°æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ 
"""

from typing import List, Tuple

from llama_index.core.schema import Document as LlamaDocument

from backend.infrastructure.indexer.utils.ids import get_vector_ids_by_metadata
from backend.infrastructure.logger import get_logger

logger = get_logger('indexer')


def filter_vectorized_documents(index_manager, documents: List[LlamaDocument]) -> Tuple[List[LlamaDocument], int]:
    """è¿‡æ»¤å·²å‘é‡åŒ–çš„æ–‡æ¡£ï¼Œå®ç°æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ 
    
    Args:
        index_manager: IndexManagerå®ä¾‹
        documents: æ–‡æ¡£åˆ—è¡¨
        
    Returns:
        (å¾…å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨, å·²å‘é‡åŒ–çš„æ–‡æ¡£æ•°é‡)
    """
    if not documents:
        return [], 0
    
    logger.info(f"ğŸ” å¼€å§‹è¿‡æ»¤å·²å‘é‡åŒ–æ–‡æ¡£ï¼Œæ€»æ–‡æ¡£æ•°: {len(documents)}")
    
    if index_manager._index is None:
        logger.debug("   _index ä¸º Noneï¼Œè°ƒç”¨ get_index()...")
        index_manager.get_index()
        logger.debug("   get_index() è°ƒç”¨å®Œæˆ")
    
    if not hasattr(index_manager, 'chroma_collection'):
        logger.info("â„¹ï¸  IndexManager æ²¡æœ‰ chroma_collectionï¼Œæ‰€æœ‰æ–‡æ¡£éƒ½éœ€è¦å¤„ç†")
        return documents, 0
    
    logger.debug(f"   æ£€æŸ¥ chroma_collection å±æ€§: {hasattr(index_manager, 'chroma_collection')}")
    logger.debug(f"   chroma_collection å€¼: {index_manager.chroma_collection}")
    
    try:
        logger.info("ğŸ“Š å‡†å¤‡æŸ¥è¯¢ Collection å‘é‡æ•°é‡...")
        logger.debug(f"    Collectionåç§°: {index_manager.collection_name}")
        logger.debug(f"    chroma_collectionå¯¹è±¡: {type(index_manager.chroma_collection).__name__}")
        
        logger.info("ğŸ“Š æ­£åœ¨è°ƒç”¨ chroma_collection.count()...")
        collection_count = index_manager.chroma_collection.count()
        logger.info(f"ğŸ“Š Collection å‘é‡æ•°é‡æŸ¥è¯¢å®Œæˆ: {collection_count}")
        
        if collection_count == 0:
            logger.info("â„¹ï¸  Collectionä¸ºç©ºï¼Œæ‰€æœ‰æ–‡æ¡£éƒ½éœ€è¦å¤„ç†")
            return documents, 0
        
        documents_to_process = []
        already_vectorized_count = 0
        
        logger.info(f"ğŸ” å¼€å§‹æ£€æŸ¥ {len(documents)} ä¸ªæ–‡æ¡£çš„å‘é‡åŒ–çŠ¶æ€...")
        for idx, doc in enumerate(documents, 1):
            file_path = doc.metadata.get("file_path", "")
            if not file_path:
                documents_to_process.append(doc)
                continue
            
            # æ·»åŠ è¿›åº¦æ—¥å¿—ï¼ˆæ¯10ä¸ªæ–‡æ¡£æˆ–æœ€åä¸€ä¸ªæ–‡æ¡£ï¼‰
            if idx % 10 == 0 or idx == len(documents):
                logger.info(f"   æ£€æŸ¥è¿›åº¦: {idx}/{len(documents)}")
            
            try:
                vector_ids = get_vector_ids_by_metadata(index_manager, file_path)
                if vector_ids:
                    already_vectorized_count += 1
                    logger.debug(f"æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡: {file_path}")
                else:
                    documents_to_process.append(doc)
            except Exception as check_error:
                logger.warning(f"æ£€æŸ¥æ–‡æ¡£å‘é‡åŒ–çŠ¶æ€å¤±è´¥ [{file_path}]: {check_error}ï¼Œå°†å¤„ç†è¯¥æ–‡æ¡£")
                documents_to_process.append(doc)
        
        logger.info(
            f"âœ… æ–‡æ¡£è¿‡æ»¤å®Œæˆ: "
            f"æ€»æ–‡æ¡£æ•°={len(documents)}, "
            f"å·²å‘é‡åŒ–={already_vectorized_count}, "
            f"å¾…å¤„ç†={len(documents_to_process)}"
        )
        
        return documents_to_process, already_vectorized_count
        
    except Exception as e:
        logger.error(f"âŒ è¿‡æ»¤å·²å‘é‡åŒ–æ–‡æ¡£å¤±è´¥: {e}", exc_info=True)
        logger.error(f"   å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        logger.error(f"   å¼‚å¸¸è¯¦æƒ…: {str(e)}")
        logger.warning("âš ï¸  å°†å¤„ç†æ‰€æœ‰æ–‡æ¡£ï¼ˆè·³è¿‡è¿‡æ»¤ï¼‰")
        return documents, 0
