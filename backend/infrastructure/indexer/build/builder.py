"""
æ„å»ºå…¥å£æ¨¡å—ï¼šæ„å»ºæˆ–æ›´æ–°ç´¢å¼•çš„å…¥å£å‡½æ•°
"""

import time
from typing import List, Optional, Tuple, Dict

from llama_index.core.schema import Document as LlamaDocument

from backend.infrastructure.config import config, get_gpu_device
from backend.infrastructure.logger import get_logger
from backend.infrastructure.indexer.build.normal import build_index_normal_mode
from backend.infrastructure.indexer.build.filter import filter_vectorized_documents
from backend.infrastructure.indexer.utils.ids import get_vector_ids_batch

logger = get_logger('indexer')


def build_index_method(
    index_manager,
    documents: List[LlamaDocument],
    show_progress: bool = True
) -> Tuple:
    """æ„å»ºæˆ–æ›´æ–°ç´¢å¼•ï¼ˆIndexManagerçš„build_indexæ–¹æ³•å®ç°ï¼‰"""
    start_time = time.time()
    
    logger.info(f"[é˜¶æ®µ2.1] ğŸ“¥ build_index è¢«è°ƒç”¨ï¼Œæ–‡æ¡£æ•°: {len(documents) if documents else 0}")
    
    if not documents:
        logger.warning("[é˜¶æ®µ2.1] âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ç´¢å¼•")
        return index_manager.get_index(), {}
    
    logger.info(f"[é˜¶æ®µ2.1] ğŸ” å¼€å§‹è¿‡æ»¤å·²å‘é‡åŒ–æ–‡æ¡£...")
    logger.debug(f"[é˜¶æ®µ2.1]    è°ƒç”¨ filter_vectorized_documentsï¼Œè¾“å…¥æ–‡æ¡£æ•°: {len(documents)}")
    # æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ 
    try:
        documents_to_process, already_vectorized = filter_vectorized_documents(index_manager, documents)
        logger.info(f"[é˜¶æ®µ2.1] âœ… filter_vectorized_documents è°ƒç”¨å®Œæˆ")
        logger.debug(f"[é˜¶æ®µ2.1]    è¿”å›ç»“æœ: å¾…å¤„ç†={len(documents_to_process)}, å·²å‘é‡åŒ–={already_vectorized}")
    except Exception as e:
        logger.error(f"[é˜¶æ®µ2.1] âŒ filter_vectorized_documents è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
        raise
    
    if already_vectorized > 0:
        logger.info(f"[é˜¶æ®µ2.1] âœ… æ£€æµ‹åˆ° {already_vectorized} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å¤„ç†")
        logger.info(f"[é˜¶æ®µ2.1] ğŸ“Š æ–­ç‚¹ç»­ä¼ : {already_vectorized}/{len(documents)} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œå‰©ä½™ {len(documents_to_process)} ä¸ªå¾…å¤„ç†")
    
    if not documents_to_process:
        logger.info(f"[é˜¶æ®µ2.1] âœ… æ‰€æœ‰æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å‘é‡åŒ–æ­¥éª¤")
        index = index_manager.get_index()
        vector_ids_map = get_vector_ids_batch(
            index_manager,
            [doc.metadata.get("file_path", "") for doc in documents 
             if doc.metadata.get("file_path")]
            )
        
        return index, vector_ids_map
    
    documents = documents_to_process
    device = get_gpu_device()
    
    logger.info(f"[é˜¶æ®µ2.1] ğŸ”¨ å¼€å§‹æ„å»ºç´¢å¼•ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
    logger.info(f"[é˜¶æ®µ2.1]    åˆ†å—å‚æ•°: size={index_manager.chunk_size}, overlap={index_manager.chunk_overlap}")
    
    if device.startswith("cuda"):
        import torch
        device_name = torch.cuda.get_device_name()
        logger.info(f"[é˜¶æ®µ2.2] ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} âš¡ GPUåŠ é€Ÿæ¨¡å¼")
        logger.info(f"[é˜¶æ®µ2.2]    GPU: {device_name}")
    else:
        logger.warning(f"[é˜¶æ®µ2.2] ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} ğŸŒ CPUæ¨¡å¼")
    
    try:
        # åªä½¿ç”¨æ­£å¸¸æ¨¡å¼ï¼ˆæ‰¹å¤„ç†æ¨¡å¼å·²ç§»é™¤ï¼‰
        index, _ = build_index_normal_mode(index_manager, documents, show_progress)
        
        # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        stats = index_manager.get_stats()
        total_elapsed = time.time() - start_time
        
        logger.info(f"[é˜¶æ®µ2.3] ğŸ“Š ç´¢å¼•ç»Ÿè®¡: {stats}")
        logger.info(
            f"[é˜¶æ®µ2.3] ç´¢å¼•æ„å»ºå®Œæˆ: "
            f"æ–‡æ¡£æ•°={len(documents)}, "
            f"å‘é‡æ•°={stats.get('document_count', 0)}, "
            f"æ€»è€—æ—¶={total_elapsed:.2f}s"
        )
        
        # æ„å»ºå‘é‡IDæ˜ å°„
        vector_ids_map = get_vector_ids_batch(
            index_manager,
            [doc.metadata.get("file_path", "") for doc in documents 
             if doc.metadata.get("file_path")]
        )
        
        return index_manager._index, vector_ids_map
        
    except Exception as e:
        logger.error(f"[é˜¶æ®µ2.1/2.2/2.3] âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        raise
