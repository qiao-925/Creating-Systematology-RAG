"""
æ„å»ºå…¥å£æ¨¡å—ï¼šæ„å»ºæˆ–æ›´æ–°ç´¢å¼•çš„å…¥å£å‡½æ•°
"""

import time
from typing import List, Optional, Tuple, Dict, TYPE_CHECKING

from llama_index.core.schema import Document as LlamaDocument

from backend.infrastructure.config import config, get_gpu_device
from backend.infrastructure.logger import get_logger
from backend.infrastructure.indexer.build.normal import build_index_normal_mode
from backend.infrastructure.indexer.build.filter import filter_vectorized_documents
from backend.infrastructure.indexer.utils.ids import get_vector_ids_batch

if TYPE_CHECKING:
    from backend.infrastructure.data_loader.github_sync.manager import GitHubSyncManager

logger = get_logger('indexer')


def build_index_method(
    index_manager,
    documents: List[LlamaDocument],
    show_progress: bool = True,
    github_sync_manager: Optional["GitHubSyncManager"] = None
) -> Tuple:
    """æ„å»ºæˆ–æ›´æ–°ç´¢å¼•ï¼ˆIndexManagerçš„build_indexæ–¹æ³•å®ç°ï¼‰
    
    Args:
        index_manager: IndexManagerå®ä¾‹
        documents: æ–‡æ¡£åˆ—è¡¨
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        github_sync_manager: GitHubåŒæ­¥ç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        (ç´¢å¼•, å‘é‡IDæ˜ å°„)
    """
    start_time = time.time()
    
    logger.info(f"[é˜¶æ®µ2.1] ğŸ“¥ build_index è¢«è°ƒç”¨ï¼Œæ–‡æ¡£æ•°: {len(documents) if documents else 0}")
    
    if not documents:
        logger.warning("[é˜¶æ®µ2.1] âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ç´¢å¼•")
        return index_manager.get_index(), {}
    
    # ä¿å­˜åŸå§‹æ–‡æ¡£åˆ—è¡¨ï¼ˆç”¨äºä¸­é—´å±‚ä¿å­˜çŠ¶æ€æ—¶æŸ¥æ‰¾å…ƒæ•°æ®ï¼‰
    all_documents = documents
    
    logger.info(f"[é˜¶æ®µ2.1] ğŸ” å¼€å§‹è¿‡æ»¤å·²å‘é‡åŒ–æ–‡æ¡£...")
    logger.debug(f"[é˜¶æ®µ2.1]    è°ƒç”¨ filter_vectorized_documentsï¼Œè¾“å…¥æ–‡æ¡£æ•°: {len(documents)}")
    # æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ 
    try:
        documents_to_process, already_vectorized, already_vectorized_map = filter_vectorized_documents(
            index_manager, documents, github_sync_manager
        )
        logger.info(f"[é˜¶æ®µ2.1] âœ… filter_vectorized_documents è°ƒç”¨å®Œæˆ")
        logger.debug(f"[é˜¶æ®µ2.1]    è¿”å›ç»“æœ: å¾…å¤„ç†={len(documents_to_process)}, å·²å‘é‡åŒ–={already_vectorized}, å·²å‘é‡åŒ–æ–‡æ¡£å‘é‡IDæ˜ å°„={len(already_vectorized_map)}ä¸ªæ–‡ä»¶")
    except Exception as e:
        logger.error(f"[é˜¶æ®µ2.1] âŒ filter_vectorized_documents è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
        raise
    
    if already_vectorized > 0:
        logger.info(f"[é˜¶æ®µ2.1] âœ… æ£€æµ‹åˆ° {already_vectorized} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å¤„ç†")
        logger.info(f"[é˜¶æ®µ2.1] ğŸ“Š æ–­ç‚¹ç»­ä¼ : {already_vectorized}/{len(documents)} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œå‰©ä½™ {len(documents_to_process)} ä¸ªå¾…å¤„ç†")
    
    if not documents_to_process:
        logger.info(f"[é˜¶æ®µ2.1] âœ… æ‰€æœ‰æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å‘é‡åŒ–æ­¥éª¤")
        index = index_manager.get_index()
        # ä½¿ç”¨å·²å‘é‡åŒ–æ–‡æ¡£çš„å‘é‡IDæ˜ å°„
        vector_ids_map = already_vectorized_map
        
        # ä¸­é—´å±‚ï¼šä¿å­˜å·²å‘é‡åŒ–æ–‡æ¡£çš„çŠ¶æ€ï¼ˆä½¿ç”¨åŸå§‹æ–‡æ¡£åˆ—è¡¨æŸ¥æ‰¾å…ƒæ•°æ®ï¼‰
        if github_sync_manager and vector_ids_map:
            _save_vector_ids_middle_layer(
                github_sync_manager, vector_ids_map, all_documents
            )
        
        return index, vector_ids_map
    
    documents = documents_to_process
    device = get_gpu_device()
    
    logger.info(f"[é˜¶æ®µ2.1] ğŸ”¨ å¼€å§‹æ„å»ºç´¢å¼•ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
    logger.info(f"[é˜¶æ®µ2.1]    åˆ†å—å‚æ•°: size={index_manager.chunk_size}, overlap={index_manager.chunk_overlap}")
    
    if device.startswith("cuda"):
        import torch
        device_name = torch.cuda.get_device_name()
        logger.info(f"[é˜¶æ®µ2.2] ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} âš¡ GPUåŠ é€Ÿæ¨¡å¼")
        logger.info(f"[é˜¶æ®µ2.2]    GPU: {device_name}")
    else:
        logger.warning(f"[é˜¶æ®µ2.2] ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} ğŸŒ CPUæ¨¡å¼")
    
    try:
        # åªä½¿ç”¨æ­£å¸¸æ¨¡å¼ï¼ˆæ‰¹å¤„ç†æ¨¡å¼å·²ç§»é™¤ï¼‰
        index, new_vector_ids_map, metadata_map = build_index_normal_mode(
            index_manager, documents, show_progress, github_sync_manager
        )
        
        # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        stats = index_manager.get_stats()
        total_elapsed = time.time() - start_time
        
        logger.info(f"[é˜¶æ®µ2.3] ğŸ“Š ç´¢å¼•ç»Ÿè®¡: {stats}")
        logger.info(
            f"[é˜¶æ®µ2.3] ç´¢å¼•æ„å»ºå®Œæˆ: "
            f"æ–‡æ¡£æ•°={len(documents)}, "
            f"å‘é‡æ•°={stats.get('document_count', 0)}, "
            f"æ€»è€—æ—¶={total_elapsed:.2f}s"
        )
        
        # åˆå¹¶å‘é‡IDæ˜ å°„ï¼ˆå·²å‘é‡åŒ– + æ–°å¤„ç†ï¼‰
        all_vector_ids_map = {**already_vectorized_map, **new_vector_ids_map}
        
        # ä¸­é—´å±‚ï¼šæŒ‰æ–‡æ¡£é€ä¸ªä¿å­˜çŠ¶æ€ï¼ˆä½¿ç”¨åŸå§‹æ–‡æ¡£åˆ—è¡¨æŸ¥æ‰¾å…ƒæ•°æ®ï¼‰
        if github_sync_manager and all_vector_ids_map:
            _save_vector_ids_middle_layer(
                github_sync_manager, all_vector_ids_map, all_documents, metadata_map
            )
        
        return index_manager._index, all_vector_ids_map
        
    except Exception as e:
        logger.error(f"[é˜¶æ®µ2.1/2.2/2.3] âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        raise


def _save_vector_ids_middle_layer(
    github_sync_manager: "GitHubSyncManager",
    vector_ids_map: Dict[str, List[str]],
    documents: List[LlamaDocument],
    metadata_map: Optional[Dict[str, Dict]] = None
) -> None:
    """ä¸­é—´å±‚ï¼šæŒ‰æ–‡æ¡£é€ä¸ªä¿å­˜å‘é‡IDçŠ¶æ€ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        github_sync_manager: GitHubåŒæ­¥ç®¡ç†å™¨å®ä¾‹
        vector_ids_map: å‘é‡IDæ˜ å°„
        documents: æ–‡æ¡£åˆ—è¡¨ï¼ˆç”¨äºæŸ¥æ‰¾å…ƒæ•°æ®ï¼‰
        metadata_map: æ–‡æ¡£å…ƒæ•°æ®æ˜ å°„ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›åˆ™ä¼˜å…ˆä½¿ç”¨ï¼‰
    """
    if not vector_ids_map:
        return
    
    logger.info(f"[ä¸­é—´å±‚] å¼€å§‹ä¿å­˜å‘é‡IDçŠ¶æ€ï¼Œå…± {len(vector_ids_map)} ä¸ªæ–‡ä»¶")
    
    # åˆ›å»ºæ–‡ä»¶è·¯å¾„åˆ°æ–‡æ¡£çš„æ˜ å°„ï¼ˆç”¨äºæŸ¥æ‰¾å…ƒæ•°æ®ï¼‰
    doc_map = {}
    for doc in documents:
        file_path = doc.metadata.get("file_path", "")
        if file_path:
            doc_map[file_path] = doc
    
    saved_count = 0
    skipped_count = 0
    failed_count = 0
    
    for file_path, vector_ids in vector_ids_map.items():
        # æå–å…ƒæ•°æ®
        if metadata_map and file_path in metadata_map:
            metadata = metadata_map[file_path]
            repository = metadata.get("repository", "")
            branch = metadata.get("branch", "main")
        elif file_path in doc_map:
            doc = doc_map[file_path]
            repository = doc.metadata.get("repository", "")
            branch = doc.metadata.get("branch", "main")
        else:
            logger.warning(f"[ä¸­é—´å±‚] æ— æ³•æ‰¾åˆ°æ–‡æ¡£å…ƒæ•°æ® [{file_path}]ï¼Œè·³è¿‡çŠ¶æ€ä¿å­˜")
            skipped_count += 1
            continue
        
        # å…ƒæ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        if not repository or "/" not in repository:
            logger.warning(f"[ä¸­é—´å±‚] æ–‡æ¡£å…ƒæ•°æ®ä¸å®Œæ•´ [{file_path}]ï¼Œè·³è¿‡çŠ¶æ€ä¿å­˜")
            skipped_count += 1
            continue
        
        owner, repo = repository.split("/", 1)
        if not owner or not repo:
            logger.warning(f"[ä¸­é—´å±‚] æ–‡æ¡£å…ƒæ•°æ®æ ¼å¼é”™è¯¯ [{file_path}]ï¼Œè·³è¿‡çŠ¶æ€ä¿å­˜")
            skipped_count += 1
            continue
        
        # æ›´æ–°å¹¶ä¿å­˜ï¼ˆå¸¦é‡è¯•ï¼‰
        success = False
        for retry in range(3):
            try:
                github_sync_manager.update_file_vector_ids(
                    owner, repo, branch, file_path, vector_ids
                )
                github_sync_manager.save_sync_state()
                success = True
                saved_count += 1
                break
            except Exception as e:
                if retry < 2:
                    delay = 0.1 * (retry + 1)  # é€’å¢å»¶è¿Ÿ
                    logger.warning(f"[ä¸­é—´å±‚] ä¿å­˜çŠ¶æ€å¤±è´¥ [{file_path}] (é‡è¯• {retry + 1}/3): {e}")
                    time.sleep(delay)
                else:
                    logger.error(f"[ä¸­é—´å±‚] ä¿å­˜çŠ¶æ€æœ€ç»ˆå¤±è´¥ [{file_path}]: {e}")
        
        if not success:
            failed_count += 1
            logger.warning(f"[ä¸­é—´å±‚] çŠ¶æ€ä¿å­˜å¤±è´¥ [{file_path}]ï¼Œç»§ç»­å¤„ç†å…¶ä»–æ–‡æ¡£")
    
    logger.info(
        f"[ä¸­é—´å±‚] çŠ¶æ€ä¿å­˜å®Œæˆ: "
        f"æˆåŠŸ={saved_count}, è·³è¿‡={skipped_count}, å¤±è´¥={failed_count}, "
        f"æ€»è®¡={len(vector_ids_map)}"
    )
