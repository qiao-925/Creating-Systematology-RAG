"""
é…ç½®ç®¡ç† - GPUè®¾å¤‡æ£€æµ‹æ¨¡å—
GPUè®¾å¤‡æ£€æµ‹ç›¸å…³å‡½æ•°
"""

from typing import Optional, Tuple

# å…¨å±€GPUè®¾å¤‡ä¿¡æ¯ï¼ˆé¡¹ç›®å¯åŠ¨æ—¶æ£€æµ‹ï¼‰
_GPU_DEVICE: Optional[str] = None
_GPU_AVAILABLE: bool = False
_GPU_DEVICE_NAME: Optional[str] = None


def detect_gpu_device() -> Tuple[bool, str, Optional[str]]:
    """æ£€æµ‹GPUè®¾å¤‡é…ç½®ï¼ˆå…¨å±€å‡½æ•°ï¼Œé¡¹ç›®å¯åŠ¨æ—¶è°ƒç”¨ï¼‰
    
    Returns:
        (has_gpu, device, device_name)
    """
    global _GPU_AVAILABLE, _GPU_DEVICE, _GPU_DEVICE_NAME
    
    if _GPU_DEVICE is not None:
        return _GPU_AVAILABLE, _GPU_DEVICE, _GPU_DEVICE_NAME
    
    print("ğŸ” å¼€å§‹æ£€æµ‹GPUè®¾å¤‡ï¼ˆGPUä¼˜å…ˆï¼ŒCPUå…œåº•ï¼‰...")
    
    try:
        import torch
        print(f"ğŸ“¦ PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        _GPU_AVAILABLE = torch.cuda.is_available()
        print(f"ğŸ” torch.cuda.is_available() = {_GPU_AVAILABLE}")
        
        if _GPU_AVAILABLE:
            try:
                device_count = torch.cuda.device_count()
                current_device = torch.cuda.current_device()
                _GPU_DEVICE = f"cuda:{current_device}"
                _GPU_DEVICE_NAME = torch.cuda.get_device_name(current_device)
                
                print(f"âœ… æ£€æµ‹åˆ° GPUï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰:")
                print(f"   è®¾å¤‡æ•°é‡: {device_count}")
                print(f"   å½“å‰è®¾å¤‡: {current_device}")
                print(f"   è®¾å¤‡åç§°: {_GPU_DEVICE_NAME}")
                print(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
                print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {_GPU_DEVICE} âš¡ GPUåŠ é€Ÿæ¨¡å¼")
            except Exception as e:
                print(f"âš ï¸  è·å–GPUè¯¦ç»†ä¿¡æ¯å¤±è´¥: {e}")
                _GPU_AVAILABLE = False
                _GPU_DEVICE = "cpu"
                _GPU_DEVICE_NAME = None
                print("âš ï¸  é™çº§åˆ° CPU æ¨¡å¼")
        else:
            _GPU_DEVICE = "cpu"
            _GPU_DEVICE_NAME = None
            print("âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œä½¿ç”¨ CPU å…œåº•æ¨¡å¼")
            
            if hasattr(torch.version, 'cuda') and torch.version.cuda:
                print(f"   PyTorchå·²ç¼–è¯‘CUDAæ”¯æŒï¼Œä½†è¿è¡Œæ—¶ä¸å¯ç”¨")
                print(f"   å¯èƒ½åŸå› ï¼šCUDAé©±åŠ¨ç‰ˆæœ¬ä¸åŒ¹é…æˆ–GPUè¢«å ç”¨")
            else:
                print(f"   PyTorchæœªç¼–è¯‘CUDAæ”¯æŒï¼ˆCPUç‰ˆæœ¬ï¼‰")
            
            print(f"ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                
    except ImportError as e:
        _GPU_AVAILABLE = False
        _GPU_DEVICE = "cpu"
        _GPU_DEVICE_NAME = None
        print(f"âš ï¸  PyTorch æœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {e}")
        print("âš ï¸  ä½¿ç”¨ CPU å…œåº•æ¨¡å¼")
        print(f"ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œå»ºè®®å®‰è£…CUDAç‰ˆæœ¬çš„PyTorch")
    except Exception as e:
        _GPU_AVAILABLE = False
        _GPU_DEVICE = "cpu"
        _GPU_DEVICE_NAME = None
        print(f"âš ï¸  GPUæ£€æµ‹å¤±è´¥: {e}")
        import traceback
        print(f"   é”™è¯¯è¯¦æƒ…:")
        traceback.print_exc()
        print("âš ï¸  ä½¿ç”¨ CPU å…œåº•æ¨¡å¼")
    
    return _GPU_AVAILABLE, _GPU_DEVICE, _GPU_DEVICE_NAME


def get_gpu_device() -> str:
    """è·å–GPUè®¾å¤‡å­—ç¬¦ä¸²ï¼ˆGPUä¼˜å…ˆï¼ŒCPUå…œåº•ï¼‰
    
    Returns:
        è®¾å¤‡å­—ç¬¦ä¸² ("cuda:0" æˆ– "cpu")
    """
    if _GPU_DEVICE is None:
        detect_gpu_device()
    return _GPU_DEVICE or "cpu"


def is_gpu_available() -> bool:
    """æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
    
    Returns:
        æ˜¯å¦æœ‰GPUå¯ç”¨
    """
    if _GPU_DEVICE is None:
        detect_gpu_device()
    return _GPU_AVAILABLE


def get_device_status() -> dict:
    """è·å–å½“å‰è®¾å¤‡çŠ¶æ€æ‘˜è¦
    
    Returns:
        åŒ…å«è®¾å¤‡çŠ¶æ€çš„å­—å…¸
    """
    device = get_gpu_device()
    has_gpu, _, device_name = detect_gpu_device()
    
    return {
        "device": device,
        "has_gpu": has_gpu,
        "device_name": device_name,
        "is_gpu": device.startswith("cuda"),
    }
