"""
Hugging Face Inference API å®¢æˆ·ç«¯

ä¸»è¦åŠŸèƒ½ï¼š
- å¤„ç†å•ä¸ªå’Œæ‰¹é‡ API è¯·æ±‚
- é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
"""

import time
import json
from typing import List, Set

import requests
from requests.exceptions import RequestException

from backend.infrastructure.logger import get_logger
from backend.infrastructure.embeddings.hf_utils import TimeMonitor
from backend.infrastructure.embeddings.hf_thread_pool import _get_or_create_executor

logger = get_logger('hf_api_client')


class HFAPIClient:
    """Hugging Face Inference API å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        api_url: str,
        headers: dict,
        model_name: str,
        closed: bool,
        active_requests: Set[int]
    ):
        """åˆå§‹åŒ– API å®¢æˆ·ç«¯
        
        Args:
            api_url: API URL
            headers: è¯·æ±‚å¤´
            model_name: æ¨¡å‹åç§°
            closed: æ˜¯å¦å·²å…³é—­
            active_requests: æ´»è·ƒè¯·æ±‚é›†åˆ
        """
        self.api_url = api_url
        self.headers = headers
        self.model_name = model_name
        self._closed = closed
        self._active_requests = active_requests
    
    def make_single_request(self, text: str, retry_count: int = 0) -> List[float]:
        """å‘èµ·å•ä¸ªæ–‡æœ¬çš„ API è¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            text: å•ä¸ªæ–‡æœ¬
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            
        Returns:
            å•ä¸ªå‘é‡
            
        Raises:
            RuntimeError: API è°ƒç”¨å¤±è´¥æˆ–å®ä¾‹å·²å…³é—­
        """
        if self._closed:
            raise RuntimeError("HFInferenceEmbedding å®ä¾‹å·²å…³é—­ï¼Œè¯·æ±‚è¢«å–æ¶ˆ")
        
        max_retries = 3
        payload = {"inputs": text}
        
        request_start = time.time()
        try:
            response = requests.post(
                self.api_url,
                headers=self.headers,
                json=payload,
                timeout=30,
            )
            response.raise_for_status()
            request_elapsed = time.time() - request_start
            
            result = response.json()
            
            # å¤„ç†å“åº”æ ¼å¼
            if isinstance(result, list):
                embedding = [float(x) for x in result]
            elif isinstance(result, dict):
                if "embeddings" in result:
                    embedding = [float(x) for x in result["embeddings"]]
                elif "output" in result:
                    embedding = [float(x) for x in result["output"]]
                else:
                    first_key = next(iter(result.values()))
                    embedding = [float(x) for x in first_key] if isinstance(first_key, list) else [float(first_key)]
            else:
                embedding = [float(result)] if not isinstance(result, list) else [float(x) for x in result]
            
            logger.debug(f"ğŸ“¡ HF API: è€—æ—¶={request_elapsed:.2f}s, ç»´åº¦={len(embedding)}")
            return embedding
            
        except (RequestException, json.JSONDecodeError) as e:
            if self._closed:
                raise RuntimeError("HFInferenceEmbedding å®ä¾‹å·²å…³é—­ï¼Œè¯·æ±‚è¢«å–æ¶ˆ") from e
            
            if retry_count < max_retries:
                wait_time = (retry_count + 1) * 1.0
                logger.warning(f"âš ï¸  è¯·æ±‚å¤±è´¥ï¼Œ{wait_time:.1f}ç§’åé‡è¯• ({retry_count + 1}/{max_retries})")
                time.sleep(wait_time)
                return self.make_single_request(text, retry_count + 1)
            else:
                error_details = str(e)
                if isinstance(e, RequestException) and hasattr(e, 'response') and e.response is not None:
                    error_details = f"HTTP {e.response.status_code}: {e.response.text[:200]}"
                raise RuntimeError(f"HF API è°ƒç”¨å¤±è´¥ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰: {error_details}") from e
    
    def make_request(self, texts: List[str]) -> List[List[float]]:
        """å‘èµ· API è¯·æ±‚ï¼ˆå¹¶è¡Œå¤„ç†ä¼˜åŒ–ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨
            
        Raises:
            RuntimeError: API è°ƒç”¨å¤±è´¥æˆ–å®ä¾‹å·²å…³é—­
        """
        if self._closed:
            raise RuntimeError(f"HFInferenceEmbedding å®ä¾‹å·²å…³é—­ï¼Œæ— æ³•ç»§ç»­è¯·æ±‚")
        
        if not texts:
            return []
        
        total = len(texts)
        logger.debug(f"ğŸ“¤ HF Inference API è¯·æ±‚: æ¨¡å‹={self.model_name}, æ–‡æœ¬æ•°é‡={total}")
        
        request_id = id(texts)
        self._active_requests.add(request_id)
        
        try:
            # å•ä¸ªæ–‡æœ¬ç›´æ¥å¤„ç†ï¼Œæ— éœ€å¹¶è¡Œ
            if total == 1:
                result = self.make_single_request(texts[0])
                return [result]
            
            # å¤šä¸ªæ–‡æœ¬ä½¿ç”¨å¹¶è¡Œå¤„ç†
            batch_start = time.time()
            time_monitor = TimeMonitor(
                logger,
                f"â±ï¸  HF API å¹¶è¡Œè°ƒç”¨: å·²èŠ±è´¹ {{elapsed}} ç§’ (æ¨¡å‹={self.model_name}, æ–‡æœ¬æ•°é‡={total})"
            )
            time_monitor.__enter__()
            
            try:
                executor = _get_or_create_executor()
                max_workers = min(5, total)  # æœ€å¤š 5 ä¸ªå¹¶å‘
                
                results = []
                errors = []
                
                # åˆ†æ‰¹å¹¶è¡Œå¤„ç†
                for batch_start_idx in range(0, total, max_workers):
                    batch_end_idx = min(batch_start_idx + max_workers, total)
                    batch_texts = texts[batch_start_idx:batch_end_idx]
                    
                    futures = [executor.submit(self.make_single_request, text) for text in batch_texts]
                    
                    for i, future in enumerate(futures):
                        try:
                            result = future.result(timeout=60)
                            results.append(result)
                        except Exception as e:
                            logger.error(f"âŒ å¹¶è¡Œè¯·æ±‚å¤±è´¥ (ç´¢å¼• {batch_start_idx + i}): {e}")
                            errors.append((batch_start_idx + i, e))
                            results.append(None)
                    
                    if batch_end_idx < total:
                        logger.debug(f"   è¿›åº¦: {batch_end_idx}/{total}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥
                if errors:
                    failed_count = len(errors)
                    logger.warning(f"âš ï¸  {failed_count}/{total} ä¸ªè¯·æ±‚å¤±è´¥")
                    for idx, error in errors:
                        try:
                            logger.debug(f"   é‡è¯•ç´¢å¼• {idx}...")
                            results[idx] = self.make_single_request(texts[idx])
                        except Exception as retry_error:
                            logger.error(f"âŒ é‡è¯•å¤±è´¥ (ç´¢å¼• {idx}): {retry_error}")
                            raise RuntimeError(f"æ‰¹é‡è¯·æ±‚å¤±è´¥ï¼Œç´¢å¼• {idx}: {retry_error}") from retry_error
                
                batch_elapsed = time.time() - batch_start
                avg_time = batch_elapsed / total
                logger.info(
                    f"ğŸ“¥ å¹¶è¡Œæ‰¹é‡å®Œæˆ: {total} ä¸ªæ–‡æœ¬, "
                    f"æ€»è€—æ—¶={batch_elapsed:.2f}s, å¹³å‡={avg_time:.2f}s/ä¸ª"
                )
                
                return results
                
            finally:
                time_monitor.__exit__(None, None, None)
        finally:
            self._active_requests.discard(request_id)
