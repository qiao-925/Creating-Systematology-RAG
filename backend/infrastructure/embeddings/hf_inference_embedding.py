"""
Hugging Face Inference API Embeddingé€‚é…å™¨ï¼šæ”¯æŒé€šè¿‡HF Inference Providersè°ƒç”¨embeddingæ¨¡å‹

ä¸»è¦åŠŸèƒ½ï¼š
- HFInferenceEmbeddingç±»ï¼šHugging Face Inference APIé€‚é…å™¨ï¼Œå®ç°BaseEmbeddingæ¥å£
- get_query_embedding()ï¼šé€šè¿‡HF Inference APIç”ŸæˆæŸ¥è¯¢å‘é‡
- get_text_embeddings()ï¼šé€šè¿‡HF Inference APIæ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡

ç‰¹æ€§ï¼š
- ä½¿ç”¨ç›´æ¥HTTPè¯·æ±‚ï¼ˆrequestsï¼‰è°ƒç”¨HF Inference APIï¼Œæé«˜é€æ˜åº¦å’Œå¯è°ƒè¯•æ€§
- æ”¯æŒæŒ‰é‡ä»˜è´¹ï¼ˆPROç”¨æˆ·æ¯æœˆæœ‰$2.00å…è´¹é¢åº¦ï¼‰
- ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
"""

import os
from typing import List, Optional
import time
import asyncio
import json

import requests
from requests.exceptions import RequestException

from backend.infrastructure.embeddings.base import BaseEmbedding
from backend.infrastructure.config import config
from backend.infrastructure.logger import get_logger
from backend.infrastructure.embeddings.hf_thread_pool import (
    register_embedding_instance,
    cleanup_hf_embedding_resources,
)
from backend.infrastructure.embeddings.hf_llama_adapter import create_llama_index_adapter
from backend.infrastructure.embeddings.hf_api_client import HFAPIClient

logger = get_logger('hf_inference_embedding')


class HFInferenceEmbedding(BaseEmbedding):
    """Hugging Face Inference API Embedding é€‚é…å™¨
    
    ä½¿ç”¨ Hugging Face Inference Providers æœåŠ¡è°ƒç”¨ embedding æ¨¡å‹
    æ”¯æŒæŒ‰é‡ä»˜è´¹ï¼ŒPRO ç”¨æˆ·æ¯æœˆæœ‰ $2.00 å…è´¹é¢åº¦
    """
    
    def __init__(
        self,
        model_name: str = "BAAI/bge-base-zh-v1.5",
        api_key: Optional[str] = None,
    ):
        """åˆå§‹åŒ– HF Inference API Embedding
        
        Args:
            model_name: Hugging Face æ¨¡å‹åç§°ï¼ˆé»˜è®¤ BAAI/bge-base-zh-v1.5ï¼‰
            api_key: Hugging Face API Tokenï¼ˆä»ç¯å¢ƒå˜é‡ HF_TOKEN æˆ–é…ç½®è¯»å–ï¼‰
        """
        self.model_name = model_name
        self._dimension: Optional[int] = None
        self._closed = False
        self._active_requests: set = set()  # è·Ÿè¸ªæ­£åœ¨è¿›è¡Œçš„è¯·æ±‚
        
        # è·å– API keyï¼ˆä¼˜å…ˆçº§ï¼šå‚æ•° > ç¯å¢ƒå˜é‡ > é…ç½®ï¼‰
        self.api_key = api_key or os.getenv("HF_TOKEN") or getattr(config, 'HF_TOKEN', None)
        
        if not self.api_key:
            raise ValueError(
                "HF_TOKEN æœªè®¾ç½®ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ HF_TOKEN æˆ–é…ç½®ä¸­çš„ HF_TOKENã€‚"
                "è·å– Token: https://huggingface.co/settings/tokens"
            )
        
        # æ„å»º API URL å’Œ headersï¼ˆä½¿ç”¨ç›´æ¥ HTTP è¯·æ±‚ï¼‰
        self.api_url = f"https://router.huggingface.co/hf-inference/models/{self.model_name}/pipeline/feature-extraction"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }
        
        # æ³¨å†Œåˆ°å…¨å±€å®ä¾‹é›†åˆï¼Œä»¥ä¾¿åœ¨é€€å‡ºæ—¶æ¸…ç†
        register_embedding_instance(self)
        
        # åˆ›å»º API å®¢æˆ·ç«¯
        self._api_client = HFAPIClient(
            api_url=self.api_url,
            headers=self.headers,
            model_name=self.model_name,
            closed=self._closed,
            active_requests=self._active_requests
        )
        
        logger.info(f"ğŸ“¡ åˆå§‹åŒ–HF Inference API Embedding: {self.model_name}")
    
    def _get_default_dimension(self, model_name: str) -> int:
        """æ ¹æ®æ¨¡å‹åç§°è·å–é»˜è®¤ç»´åº¦"""
        model_lower = model_name.lower()
        if "qwen" in model_lower and ("0.6b" in model_lower or "8b" in model_lower):
            return 1024
        elif "bge" in model_lower:
            return 768 if "base" in model_lower else 384
        return 384  # é€šç”¨é»˜è®¤å€¼
    
    def _make_request(self, texts: List[str]) -> List[List[float]]:
        """å‘èµ· API è¯·æ±‚ï¼ˆä½¿ç”¨ API å®¢æˆ·ç«¯ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        # æ›´æ–° API å®¢æˆ·ç«¯çŠ¶æ€
        self._api_client._closed = self._closed
        return self._api_client.make_request(texts)
    
    def get_query_embedding(self, query: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢å‘é‡"""
        embeddings = self.get_text_embeddings([query])
        return embeddings[0]
    
    def get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡
        
        æ”¯æŒæ‰¹é‡å¤„ç†ï¼Œè‡ªåŠ¨åˆ†æ‰¹ä»¥é¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§ã€‚
        ç”±äº feature_extraction ä¸€æ¬¡åªèƒ½å¤„ç†ä¸€ä¸ªæ–‡æœ¬ï¼Œå†…éƒ¨ä¼šé€ä¸ªå¤„ç†ã€‚
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬å¯¹åº”ä¸€ä¸ªå‘é‡
        """
        if not texts:
            return []
        
        # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š 100 ä¸ªæ–‡æœ¬
        batch_size = 100
        total_batches = (len(texts) + batch_size - 1) // batch_size
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            if total_batches > 1:
                logger.debug(f"å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches} ({len(batch)} ä¸ªæ–‡æœ¬)")
            
            batch_embeddings = self._make_request(batch)
            all_embeddings.extend(batch_embeddings)
        
        return all_embeddings
    
    def get_embedding_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦ï¼ˆç¡®ä¿æ€»æ˜¯è¿”å›æœ‰æ•ˆå€¼ï¼‰"""
        if self._dimension is None:
            self._dimension = self._get_default_dimension(self.model_name)
            logger.debug(f"ä½¿ç”¨é»˜è®¤ç»´åº¦: {self._dimension}")
            try:
                test_embedding = self.get_query_embedding("test")
                detected_dim = len(test_embedding)
                if detected_dim != self._dimension:
                    logger.info(f"ğŸ”„ æ£€æµ‹åˆ°å®é™…ç»´åº¦ {detected_dim}ï¼Œæ›´æ–°é»˜è®¤å€¼ {self._dimension}")
                    self._dimension = detected_dim
            except Exception as e:
                logger.warning(f"âš ï¸  æ— æ³•é€šè¿‡APIè·å–ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
        return self._dimension
    
    def get_model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        return self.model_name
    
    def close(self) -> None:
        """å…³é—­å®ä¾‹ï¼Œæ¸…ç†èµ„æº
        
        åœæ­¢æ‰€æœ‰æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚ï¼Œæ¸…ç†çº¿ç¨‹å’Œè¿æ¥ã€‚
        åº”è¯¥åœ¨åº”ç”¨é€€å‡ºæ—¶è°ƒç”¨æ­¤æ–¹æ³•ã€‚
        """
        if self._closed:
            return
        
        logger.info(f"ğŸ”§ å¼€å§‹å…³é—­ HFInferenceEmbedding å®ä¾‹: {self.model_name}")
        self._closed = True
        
        # ç­‰å¾…æ­£åœ¨è¿›è¡Œçš„è¯·æ±‚å®Œæˆï¼ˆæœ€å¤šç­‰å¾…5ç§’ï¼‰
        if self._active_requests:
            logger.debug(f"ç­‰å¾… {len(self._active_requests)} ä¸ªæ­£åœ¨è¿›è¡Œçš„è¯·æ±‚å®Œæˆ...")
            start_wait = time.time()
            while self._active_requests and (time.time() - start_wait) < 5.0:
                time.sleep(0.1)
            
            if self._active_requests:
                logger.warning(f"âš ï¸  ä»æœ‰ {len(self._active_requests)} ä¸ªè¯·æ±‚æœªå®Œæˆï¼Œå¼ºåˆ¶å…³é—­")
        
        # æ¸…ç†å¼•ç”¨
        self._active_requests.clear()
        logger.info(f"âœ… HFInferenceEmbedding å®ä¾‹å·²å…³é—­: {self.model_name}")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«æ¸…ç†"""
        if not self._closed:
            try:
                self.close()
            except Exception:
                pass  # ææ„å‡½æ•°ä¸­ä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸
    
    def get_llama_index_embedding(self):
        """è·å–LlamaIndexå…¼å®¹çš„Embeddingé€‚é…å™¨
        
        Returns:
            LlamaIndexå…¼å®¹çš„é€‚é…å™¨åŒ…è£…å™¨ï¼ˆç»§æ‰¿è‡ªLlamaIndex BaseEmbeddingï¼‰
            
        Raises:
            ImportError: å¦‚æœæ— æ³•å¯¼å…¥LlamaIndex BaseEmbedding
        """
        return create_llama_index_adapter(self)
