"""
RAGå¼•æ“æµå¼æŸ¥è¯¢æ¨¡å—ï¼šå¤„ç†æµå¼æŸ¥è¯¢é€»è¾‘

ä¸»è¦åŠŸèƒ½ï¼š
- æµå¼æŸ¥è¯¢æ‰§è¡Œ
- å®æ—¶ token è¾“å‡º
- æ¨ç†é“¾æå–
"""

import time
from typing import Dict, Any, Optional

from backend.infrastructure.logger import get_logger
from backend.business.rag_engine.formatting import ResponseFormatter
from backend.infrastructure.llms.reasoning import extract_reasoning_from_stream_chunk
from backend.infrastructure.llms import extract_reasoning_content
from backend.infrastructure.llms.message_builder import build_chat_messages

logger = get_logger('rag_engine')


async def execute_stream_query(
    llm,
    formatter: ResponseFormatter,
    query_processor,
    retriever,
    postprocessors,
    query_router,
    enable_auto_routing: bool,
    retrieval_strategy: str,
    similarity_top_k: int,
    final_query: str,
    understanding: Optional[Dict[str, Any]] = None
):
    """æ‰§è¡Œæµå¼æŸ¥è¯¢
    
    Args:
        llm: LLMå®ä¾‹
        formatter: å“åº”æ ¼å¼åŒ–å™¨
        query_processor: æŸ¥è¯¢å¤„ç†å™¨
        retriever: æ£€ç´¢å™¨
        postprocessors: åå¤„ç†å™¨åˆ—è¡¨
        query_router: æŸ¥è¯¢è·¯ç”±å™¨
        enable_auto_routing: æ˜¯å¦å¯ç”¨è‡ªåŠ¨è·¯ç”±
        retrieval_strategy: æ£€ç´¢ç­–ç•¥
        similarity_top_k: ç›¸ä¼¼åº¦top_k
        final_query: å¤„ç†åçš„æŸ¥è¯¢
        understanding: æŸ¥è¯¢ç†è§£ç»“æœï¼ˆå¯é€‰ï¼‰
        
    Yields:
        dict: æµå¼å“åº”å­—å…¸
    """
    # Step 2: è·å–æ£€ç´¢å™¨å’Œæ£€ç´¢èŠ‚ç‚¹
    actual_retriever = None
    strategy_info = ""
    
    if enable_auto_routing and query_router:
        # è‡ªåŠ¨è·¯ç”±æ¨¡å¼
        if understanding:
            actual_retriever, routing_decision = query_router.route_with_understanding(
                final_query,
                understanding=understanding,
                top_k=similarity_top_k
            )
        else:
            actual_retriever, routing_decision = query_router.route(
                final_query,
                top_k=similarity_top_k
            )
        strategy_info = f"ç­–ç•¥={routing_decision}, åŸå› =è‡ªåŠ¨è·¯ç”±æ¨¡å¼"
    else:
        # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨åˆå§‹åŒ–æ—¶åˆ›å»ºçš„æ£€ç´¢å™¨
        actual_retriever = retriever
        strategy_info = f"ç­–ç•¥={retrieval_strategy}, åŸå› =å›ºå®šæ£€ç´¢æ¨¡å¼"
    
    logger.info("ä½¿ç”¨æ£€ç´¢ç­–ç•¥ï¼ˆç›´æ¥æµå¼ï¼‰", strategy_info=strategy_info)
    
    # Step 3: æ£€ç´¢èŠ‚ç‚¹
    nodes_with_scores = []
    sources = []
    full_answer = ""
    reasoning_content = ""
    
    try:
        if actual_retriever:
            # æ‰§è¡Œæ£€ç´¢
            nodes_with_scores = actual_retriever.retrieve(final_query)
            
            # åº”ç”¨åå¤„ç†
            if postprocessors:
                for postprocessor in postprocessors:
                    nodes_with_scores = postprocessor.postprocess_nodes(
                        nodes_with_scores,
                        query_str=final_query
                    )
            
            # è½¬æ¢ä¸ºå¼•ç”¨æ¥æºæ ¼å¼
            for i, node_with_score in enumerate(nodes_with_scores, 1):
                node = node_with_score.node if hasattr(node_with_score, 'node') else node_with_score
                score = node_with_score.score if hasattr(node_with_score, 'score') else None
                
                source = {
                    'index': i,
                    'text': node.text if hasattr(node, 'text') else str(node),
                    'score': score,
                    'metadata': node.metadata if hasattr(node, 'metadata') else {},
                }
                sources.append(source)
            
            logger.info(f"æ£€ç´¢åˆ° {len(nodes_with_scores)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        
        # Step 4: æ„å»º prompt
        from backend.business.rag_engine.formatting.templates import get_template
        
        # æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        context_str = ""
        if nodes_with_scores:
            context_parts = []
            for i, node_with_score in enumerate(nodes_with_scores, 1):
                node = node_with_score.node if hasattr(node_with_score, 'node') else node_with_score
                text = node.text if hasattr(node, 'text') else str(node)
                context_parts.append(f"[{i}] {text}")
            context_str = "\n\n".join(context_parts)
        else:
            context_str = "ï¼ˆçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼‰"
        
        # æ„å»ºç³»ç»Ÿ prompt å’Œç”¨æˆ·æŸ¥è¯¢
        system_prompt = get_template('chat').format(context_str=context_str)
        user_query = f"ç”¨æˆ·é—®é¢˜ï¼š{final_query}\n\nè¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"
        
        # Step 5: æ ¹æ®æ¨¡å‹ç±»å‹ç»„è£…æ¶ˆæ¯ï¼ˆé€šç”¨æ¨¡å‹ï¼šsystem+userï¼Œæ¨ç†æ¨¡å‹ï¼šåˆå¹¶åˆ°userï¼‰
        messages = build_chat_messages(system_prompt, user_query)
        
        last_token_time = time.time()
        token_count = 0
        last_chunk = None
        miss_log_count = 0
        
        logger.debug("ğŸš€ å¼€å§‹ç›´æ¥æµå¼è°ƒç”¨ DeepSeek API")
        
        # ç›´æ¥è°ƒç”¨ DeepSeek çš„ stream_chatï¼ˆç»•è¿‡ LlamaIndex ç¼“å†²ï¼‰
        for chunk in llm.stream_chat(messages):
            # æå–æ¨ç†é“¾å†…å®¹ï¼ˆæµå¼ï¼‰
            chunk_reasoning = extract_reasoning_from_stream_chunk(chunk)
            if chunk_reasoning:
                reasoning_content += chunk_reasoning
            
            # æå– token å†…å®¹ï¼ˆå¢é‡ï¼‰
            chunk_text = ""
            
            # è°ƒè¯•ï¼šè®°å½• chunk çš„ç»“æ„
            if token_count == 0:
                logger.debug(f"ğŸ” Chunk ç»“æ„æ£€æŸ¥: hasattr(chunk, 'delta')={hasattr(chunk, 'delta')}, hasattr(chunk, 'message')={hasattr(chunk, 'message')}")
                if hasattr(chunk, 'delta'):
                    delta = chunk.delta
                    logger.debug(f"ğŸ” Delta ç»“æ„: {dir(delta)}")
                    if hasattr(delta, 'content'):
                        logger.debug(f"ğŸ” Delta.content ç±»å‹: {type(delta.content)}, å€¼: {repr(delta.content)}")
                if hasattr(chunk, 'message'):
                    message = chunk.message
                    logger.debug(f"ğŸ” Message ç»“æ„: {dir(message)}")
                    if hasattr(message, 'content'):
                        logger.debug(f"ğŸ” Message.content ç±»å‹: {type(message.content)}, å€¼é•¿åº¦: {len(str(message.content)) if message.content else 0}")
            
            # æ–¹æ³•1ï¼šä¼˜å…ˆä½¿ç”¨ delta.contentï¼ˆå¢é‡ï¼‰
            if hasattr(chunk, 'delta'):
                delta = chunk.delta
                if isinstance(delta, str):
                    if delta:
                        chunk_text = delta
                elif isinstance(delta, dict):
                    delta_content = delta.get('content')
                    if delta_content:
                        chunk_text = str(delta_content)
                elif hasattr(delta, 'content') and delta.content:
                    chunk_text = str(delta.content)
                    if len(chunk_text) > 50:
                        logger.warning(f"âš ï¸ Delta.content é•¿åº¦å¼‚å¸¸: {len(chunk_text)} å­—ç¬¦ï¼Œå¯èƒ½æ˜¯ç´¯åŠ çš„ï¼å†…å®¹: {chunk_text[:50]}...")
                elif hasattr(delta, 'text') and delta.text:
                    chunk_text = str(delta.text)
            
            # æ–¹æ³•2ï¼šå¦‚æœ delta æ²¡æœ‰å‘½ä¸­ï¼Œå°è¯•ä» message.content è®¡ç®—å¢é‡
            if not chunk_text and hasattr(chunk, 'message'):
                message = chunk.message
                current_content = None
                if isinstance(message, str):
                    current_content = message
                elif hasattr(message, 'content') and message.content:
                    current_content = message.content
                
                if current_content:
                    current_content = str(current_content)
                    if full_answer and current_content.startswith(full_answer):
                        chunk_text = current_content[len(full_answer):]
                        if not chunk_text:
                            continue
                    elif not full_answer:
                        chunk_text = current_content
                    else:
                        logger.warning(f"âš ï¸ Message.content æ ¼å¼å¼‚å¸¸: å½“å‰é•¿åº¦={len(current_content)}, ä¹‹å‰é•¿åº¦={len(full_answer)}")
                        if len(current_content) > len(full_answer):
                            chunk_text = current_content[len(full_answer):]
                        else:
                            chunk_text = current_content
                            full_answer = ""
            
            # æ–¹æ³•3ï¼šæ£€æŸ¥ raw å“åº”ï¼ˆOpenAI æ ¼å¼ï¼‰
            if not chunk_text and hasattr(chunk, 'raw'):
                raw = chunk.raw
                if isinstance(raw, dict):
                    choices = raw.get('choices', [])
                    if choices and len(choices) > 0:
                        choice = choices[0]
                        delta = choice.get('delta', {})
                        if isinstance(delta, dict):
                            chunk_text = delta.get('content', '')
                            if chunk_text:
                                chunk_text = str(chunk_text)
            
            if not chunk_text and miss_log_count < 3:
                delta_obj = getattr(chunk, 'delta', None)
                message_obj = getattr(chunk, 'message', None)
                raw_obj = getattr(chunk, 'raw', None)
                msg_content = None
                if isinstance(message_obj, str):
                    msg_content = message_obj
                elif hasattr(message_obj, 'content'):
                    msg_content = message_obj.content
                logger.debug(
                    "stream_chunk_no_token",
                    delta_type=type(delta_obj).__name__,
                    delta_len=len(str(delta_obj)) if delta_obj else 0,
                    message_type=type(message_obj).__name__,
                    message_len=len(str(msg_content)) if msg_content else 0,
                    raw_type=type(raw_obj).__name__,
                    raw_keys=list(raw_obj.keys()) if isinstance(raw_obj, dict) else None,
                )
                miss_log_count += 1
            
            if chunk_text:
                token_count += 1
                current_time = time.time()
                time_since_last = current_time - last_token_time
                last_token_time = current_time
                
                if token_count <= 5 or time_since_last > 0.1:
                    logger.debug(f"ğŸ”¤ Token #{token_count} '{chunk_text[:20]}...' åˆ°è¾¾ï¼Œé—´éš”: {time_since_last*1000:.1f}ms")
                
                full_answer += chunk_text
                yield {'type': 'token', 'data': chunk_text}
            
            last_chunk = chunk
        
        logger.debug(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œå…± {token_count} ä¸ª token")
        
        # Step 6: æ ¼å¼åŒ–ç­”æ¡ˆ
        full_answer = formatter.format(full_answer, None)
        
        # Step 7: æå–æœ€ç»ˆæ¨ç†é“¾ï¼ˆä»æœ€åä¸€ä¸ª chunkï¼‰
        if last_chunk:
            final_reasoning = extract_reasoning_content(last_chunk)
            if final_reasoning:
                reasoning_content = final_reasoning
        
        # è¿”å›å¼•ç”¨æ¥æº
        if sources:
            yield {'type': 'sources', 'data': sources}
        
        # è¿”å›æ¨ç†é“¾ï¼ˆç­”æ¡ˆå®Œæˆåï¼Œéæµå¼ï¼‰
        if reasoning_content:
            yield {'type': 'reasoning', 'data': reasoning_content}
        
        # è¿”å›å®Œæˆäº‹ä»¶
        yield {
            'type': 'done',
            'data': {
                'answer': full_answer,
                'sources': sources,
                'reasoning_content': reasoning_content if reasoning_content else None,
            }
        }
        
    except Exception as e:
        logger.error(f"æµå¼æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
        yield {
            'type': 'error',
            'data': {'message': str(e)}
        }
        raise
