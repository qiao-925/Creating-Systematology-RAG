"""
RAGå¼•æ“å·¥å…·æ¨¡å— - å·¥å…·å‡½æ•°é›†åˆï¼šæä¾›æ ¼å¼åŒ–ã€å…œåº•å¤„ç†ã€è¿½è¸ªä¿¡æ¯æ”¶é›†ç­‰è¾…åŠ©åŠŸèƒ½

ä¸»è¦åŠŸèƒ½ï¼š
- format_sources()ï¼šæ ¼å¼åŒ–å¼•ç”¨æ¥æºä¸ºå¯è¯»æ–‡æœ¬
- handle_fallback()ï¼šå¤„ç†å…œåº•é€»è¾‘ï¼Œå½“ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼æˆ–æ— ç»“æœæ—¶ä½¿ç”¨LLMç›´æ¥å›ç­”
- collect_trace_info()ï¼šæ”¶é›†æŸ¥è¯¢è¿‡ç¨‹çš„è¯¦ç»†è¿½è¸ªä¿¡æ¯
- extract_sources_from_response()ï¼šä»å“åº”å¯¹è±¡ä¸­æå–å¼•ç”¨æ¥æº

ç‰¹æ€§ï¼š
- å‹å¥½çš„å¼•ç”¨æ¥æºæ ¼å¼
- æ™ºèƒ½å…œåº•æœºåˆ¶
- è¯¦ç»†çš„è¿½è¸ªä¿¡æ¯æ”¶é›†
- æ€§èƒ½ç»Ÿè®¡
"""

import time
from typing import List, Tuple, Optional, Dict, Any

from backend.infrastructure.logger import get_logger

logger = get_logger('rag_engine')


def extract_sources_from_response(response) -> List[dict]:
    """ä»å“åº”å¯¹è±¡ä¸­æå–å¼•ç”¨æ¥æº
    
    Args:
        response: æŸ¥è¯¢å“åº”å¯¹è±¡ï¼ˆLlamaIndex Responseï¼‰
        
    Returns:
        å¼•ç”¨æ¥æºåˆ—è¡¨
    """
    sources = []
    if hasattr(response, 'source_nodes') and response.source_nodes:
        logger.info(f"ğŸ” æ£€ç´¢åˆ° {len(response.source_nodes)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
        
        for i, node in enumerate(response.source_nodes, 1):
            try:
                # æå–å…ƒæ•°æ®
                metadata = {}
                if hasattr(node, 'node') and hasattr(node.node, 'metadata'):
                    metadata_raw = node.node.metadata
                    if isinstance(metadata_raw, dict):
                        metadata = metadata_raw
                
                # æå–æ–‡æœ¬
                text = ""
                if hasattr(node, 'node'):
                    text = node.node.text if hasattr(node.node, 'text') else str(node)
                else:
                    text = str(node)
                
                # æå–åˆ†æ•°
                score = node.score if hasattr(node, 'score') else None
                
                source = {
                    'index': i,
                    'text': text,
                    'score': score,
                    'metadata': metadata,
                }
                sources.append(source)
                
                # æ‰“å°è°ƒè¯•ä¿¡æ¯
                score_str = f" (ç›¸ä¼¼åº¦: {score:.3f})" if score is not None else ""
                title = metadata.get('title') or metadata.get('file_name') or metadata.get('file_path', 'æœªçŸ¥').split('/')[-1] if metadata.get('file_path') else 'Unknown'
                logger.debug(f"  [{i}] {title}{score_str}")
                
            except Exception as e:
                logger.warning(f"æå–æ¥æº {i} å¤±è´¥: {e}")
                continue
    
    return sources


def format_sources(sources: List[dict]) -> str:
    """æ ¼å¼åŒ–å¼•ç”¨æ¥æºä¸ºå¯è¯»æ–‡æœ¬
    
    Args:
        sources: å¼•ç”¨æ¥æºåˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    if not sources:
        return "ï¼ˆæ— å¼•ç”¨æ¥æºï¼‰"
    
    formatted = "\n\nğŸ“š å¼•ç”¨æ¥æºï¼š\n"
    for source in sources:
        formatted += f"\n[{source['index']}] "
        
        # æ·»åŠ æ–‡æ¡£ä¿¡æ¯
        metadata = source['metadata']
        if 'title' in metadata:
            formatted += f"{metadata['title']}"
        elif 'file_name' in metadata:
            formatted += f"{metadata['file_name']}"
        elif 'url' in metadata:
            formatted += f"{metadata['url']}"
        
        # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°
        if source['score'] is not None:
            formatted += f" (ç›¸ä¼¼åº¦: {source['score']:.2f})"
        
        # å®Œæ•´æ˜¾ç¤ºæ–‡æœ¬å†…å®¹
        formatted += f"\n   {source['text']}"
    
    return formatted




def handle_fallback(
    answer: str,
    sources: List[dict],
    question: str,
    llm,
    similarity_threshold: float
) -> Tuple[str, Optional[str]]:
    """å¤„ç†å…œåº•é€»è¾‘
    
    Args:
        answer: åŸå§‹ç­”æ¡ˆ
        sources: å¼•ç”¨æ¥æºåˆ—è¡¨
        question: ç”¨æˆ·é—®é¢˜
        llm: LLMå®ä¾‹
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        
    Returns:
        (å¤„ç†åçš„ç­”æ¡ˆ, å…œåº•åŸå› )
    """
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    scores_list = [s['score'] for s in sources if s.get('score') is not None]
    scores_none_count = len(sources) - len(scores_list)
    
    min_score = min(scores_list) if scores_list else None
    avg_score = sum(scores_list) / len(scores_list) if scores_list else None
    max_score_logged = max(scores_list) if scores_list else None
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    logger.info(f"ğŸ“Š æ£€ç´¢ç»Ÿè®¡:")
    logger.info(f"   æ£€ç´¢åˆ° {len(sources)} ä¸ªchunk")
    logger.info(f"   ç›¸ä¼¼åº¦åˆ†æ•°: {len(scores_list)} ä¸ªæœ‰æ•ˆ, {scores_none_count} ä¸ªä¸ºç©º")
    if scores_list:
        logger.info(f"   èŒƒå›´: {min_score:.3f} ~ {max_score_logged:.3f}, å¹³å‡: {avg_score:.3f}")
    logger.info(f"   é˜ˆå€¼: {similarity_threshold}")
    
    # åˆ¤å®šæ˜¯å¦éœ€è¦å…œåº•
    fallback_reason = None
    if not sources:
        fallback_reason = "no_sources"
    elif (max_score_logged is not None) and (max_score_logged < similarity_threshold):
        fallback_reason = f"low_similarity({max_score_logged:.2f}<{similarity_threshold})"
    elif not answer or not answer.strip():
        fallback_reason = "empty_answer"
    
    if fallback_reason:
        logger.info(f"ğŸ›Ÿ  è§¦å‘å…œåº•ç”Ÿæˆï¼ˆåŸå› : {fallback_reason}ï¼‰")
        
        # çº¯LLMå®šä¹‰ç±»å›ç­”æç¤ºè¯
        fallback_prompt = (
            "ä½ æ˜¯ä¸€ä½ç³»ç»Ÿç§‘å­¦é¢†åŸŸçš„èµ„æ·±ä¸“å®¶ã€‚å½“å‰æœªæ£€ç´¢åˆ°è¶³å¤Ÿé«˜ç›¸å…³çš„çŸ¥è¯†åº“å†…å®¹ï¼Œ"
            "è¯·åŸºäºé€šç”¨å­¦æœ¯çŸ¥è¯†ä¸å¸¸è§æ•™æï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼Œç»™å‡ºæ¸…æ™°ã€ç»“æ„åŒ–ã€å¯è‡ªæ´½çš„è§£é‡Šã€‚\n\n"
            "è¦æ±‚ï¼š\n"
            "1) å…ˆç»™å‡ºç®€æ˜å®šä¹‰/æ ¸å¿ƒæ€æƒ³ï¼Œå†ç»™å‡ºå…³é”®è¦ç‚¹æ¡ç›®ï¼›\n"
            "2) ä¿æŒä¸¥è°¨ã€ä¸­ç«‹ï¼Œä¸æé€ å…·ä½“å¼•ç”¨ï¼›\n"
            "3) å¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼›\n"
            "4) æœ«å°¾å¢åŠ ä¸€è¡Œæç¤ºï¼šâ€˜æ³¨ï¼šæœªæ£€ç´¢åˆ°è¶³å¤Ÿé«˜ç›¸å…³èµ„æ–™ï¼Œæœ¬å›ç­”åŸºäºé€šç”¨çŸ¥è¯†æ¨ç†ï¼Œå¯èƒ½ä¸å«å¼•ç”¨ã€‚â€™\n\n"
            f"ç”¨æˆ·é—®é¢˜ï¼š{question}\n"
            "å›ç­”ï¼š"
        )
        try:
            llm_start = time.time()
            llm_resp = llm.complete(fallback_prompt)
            llm_time = time.time() - llm_start
            new_answer = (llm_resp.text or "").strip()
            if new_answer:
                answer = new_answer
            else:
                answer = (
                    "æŠ±æ­‰ï¼Œæœªæ£€ç´¢åˆ°ä¸è¯¥é—®é¢˜é«˜åº¦ç›¸å…³çš„èµ„æ–™ã€‚åŸºäºä¸€èˆ¬çŸ¥è¯†ï¼š\n"
                    "- è¯¥é—®é¢˜å±äºé€šè¯†ç±»ä¸»é¢˜ï¼Œå»ºè®®è¿›ä¸€æ­¥ç»†åŒ–èŒƒå›´ï¼›\n"
                    "- å¦‚éœ€æƒå¨æ¥æºï¼Œå¯æä¾›æ›´å…·ä½“çš„å…³é”®è¯ä»¥ä¾¿æ£€ç´¢ã€‚\n\n"
                    "æ³¨ï¼šæœªæ£€ç´¢åˆ°è¶³å¤Ÿé«˜ç›¸å…³èµ„æ–™ï¼Œæœ¬å›ç­”åŸºäºé€šç”¨çŸ¥è¯†æ¨ç†ï¼Œå¯èƒ½ä¸å«å¼•ç”¨ã€‚"
                )
            logger.info(f"å…œåº•ç”Ÿæˆå®Œæˆ: length={len(answer)}, llm_time={llm_time:.2f}s")
        except Exception as fe:
            logger.error(f"å…œåº•ç”Ÿæˆå¤±è´¥: {fe}")
            answer = (
                "æŠ±æ­‰ï¼Œå½“å‰æ— æ³•ç”Ÿæˆé«˜è´¨é‡ç­”æ¡ˆã€‚\n"
                "- å»ºè®®è°ƒæ•´æé—®æ–¹å¼æˆ–è¡¥å……ä¸Šä¸‹æ–‡ï¼›\n"
                "- ç¨åå¯é‡è¯•ä»¥è·å–æ›´ç¨³å®šç»“æœã€‚\n\n"
                "æ³¨ï¼šæœªæ£€ç´¢åˆ°è¶³å¤Ÿé«˜ç›¸å…³èµ„æ–™ï¼Œæœ¬å›ç­”åŸºäºé€šç”¨çŸ¥è¯†æ¨ç†ï¼Œå¯èƒ½ä¸å«å¼•ç”¨ã€‚"
            )
    
    return answer, fallback_reason


def collect_trace_info(
    trace_info: Dict[str, Any],
    retrieval_time: float,
    sources: List[dict],
    similarity_top_k: int,
    similarity_threshold: float,
    model: str,
    answer: str,
    fallback_reason: Optional[str]
) -> Dict[str, Any]:
    """æ”¶é›†è¿½è¸ªä¿¡æ¯
    
    Args:
        trace_info: è¿½è¸ªä¿¡æ¯å­—å…¸
        retrieval_time: æ£€ç´¢è€—æ—¶
        sources: å¼•ç”¨æ¥æºåˆ—è¡¨
        similarity_top_k: Top Kå€¼
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        model: æ¨¡å‹åç§°
        answer: ç­”æ¡ˆæ–‡æœ¬
        fallback_reason: å…œåº•åŸå› 
        
    Returns:
        å®Œæ•´çš„è¿½è¸ªä¿¡æ¯å­—å…¸
    """
    # ä½¿ç”¨å‰é¢å·²è®¡ç®—çš„ç»Ÿè®¡æ•°æ®
    _scores = [s['score'] for s in sources if s.get('score') is not None]
    _avg = sum(_scores) / len(_scores) if _scores else 0.0
    _min = min(_scores) if _scores else 0.0
    _max = max(_scores) if _scores else 0.0
    _hq = len([s for s in sources if (s.get('score') is not None) and (s.get('score') >= similarity_threshold)])
    _none_count = len(sources) - len(_scores)
    
    trace_info["retrieval"] = {
        "time_cost": round(retrieval_time, 2),
        "top_k": similarity_top_k,
        "chunks_retrieved": len(sources),
        "chunks": sources,
        "avg_score": round(_avg, 3),
        "min_score": round(_min, 3),
        "max_score": round(_max, 3),
        "threshold": similarity_threshold,
        "high_quality_count": _hq,
        "numeric_scores_count": len(_scores),
        "scores_none_count": _none_count,
    }
    
    trace_info["llm_generation"] = {
        "model": model,
        "response_length": len(answer),
        "fallback_used": bool(fallback_reason),
        "fallback_reason": fallback_reason,
    }
    
    trace_info["total_time"] = round(time.time() - trace_info["start_time"], 2)
    
    return trace_info
