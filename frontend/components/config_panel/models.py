"""
é…ç½®æ•°æ®æ¨¡å‹ - AppConfig å’Œ LLM é¢„è®¾å®šä¹‰

ä¸»è¦åŠŸèƒ½ï¼š
- AppConfig: åº”ç”¨è¿è¡Œæ—¶é…ç½®ï¼Œä½œä¸º session_state çš„è¯»å†™æ¡¥æ¢
- LLM_PRESETS: LLM é¢„è®¾é…ç½®ï¼ˆç²¾ç¡®/å¹³è¡¡/åˆ›æ„ï¼‰
- get_preset_params(): è·å–é¢„è®¾å‚æ•°
"""

from dataclasses import dataclass
from typing import Dict, Any, Optional

import streamlit as st

from backend.infrastructure.config import config


# LLM é¢„è®¾å®šä¹‰
LLM_PRESETS: Dict[str, Dict[str, Any]] = {
    "precise": {
        "name": "ğŸ¯ ç²¾ç¡®æ¨¡å¼",
        "description": "é€‚åˆä»£ç ã€æŠ€æœ¯é—®ç­”ã€æ•°æ®åˆ†æ",
        "temperature": 0.3,
        "max_tokens": 4096,
    },
    "balanced": {
        "name": "âš–ï¸ å¹³è¡¡æ¨¡å¼",
        "description": "é»˜è®¤æ¨¡å¼ï¼Œé€‚åˆå¤§å¤šæ•°åœºæ™¯",
        "temperature": 0.7,
        "max_tokens": 4096,
    },
    "creative": {
        "name": "ğŸ’¡ åˆ›æ„æ¨¡å¼",
        "description": "é€‚åˆå¤´è„‘é£æš´ã€åˆ›æ„å†™ä½œ",
        "temperature": 1.3,
        "max_tokens": 8192,
    },
}


def get_preset_params(preset_key: str) -> Dict[str, Any]:
    """è·å–é¢„è®¾å‚æ•°
    
    Args:
        preset_key: é¢„è®¾é”®åï¼ˆprecise/balanced/creativeï¼‰
        
    Returns:
        é¢„è®¾å‚æ•°å­—å…¸ï¼ŒåŒ…å« temperature å’Œ max_tokens
    """
    preset = LLM_PRESETS.get(preset_key, LLM_PRESETS["balanced"])
    return {
        "temperature": preset["temperature"],
        "max_tokens": preset["max_tokens"],
    }


@dataclass
class AppConfig:
    """åº”ç”¨è¿è¡Œæ—¶é…ç½®
    
    ä½œä¸º session_state çš„è¯»å†™æ¡¥æ¢ï¼Œä¸åœ¨ session_state ä¸­å­˜å‚¨å®ä¾‹æœ¬èº«ã€‚
    """
    
    # æ¨¡å‹é…ç½®
    selected_model: str = "deepseek-chat"
    llm_preset: str = "balanced"
    
    # RAG é…ç½®
    retrieval_strategy: str = "vector"
    use_agentic_rag: bool = False
    similarity_top_k: int = 3
    similarity_threshold: float = 0.4
    enable_rerank: bool = False
    
    # æ˜¾ç¤ºé…ç½®
    show_reasoning: bool = True
    debug_mode: bool = False
    
    @classmethod
    def from_session_state(cls) -> "AppConfig":
        """ä» session_state è¯»å–é…ç½®"""
        return cls(
            selected_model=st.session_state.get(
                'selected_model', config.get_default_llm_id()
            ),
            llm_preset=st.session_state.get('llm_preset', 'balanced'),
            retrieval_strategy=st.session_state.get(
                'retrieval_strategy', config.RETRIEVAL_STRATEGY
            ),
            use_agentic_rag=st.session_state.get('use_agentic_rag', False),
            similarity_top_k=st.session_state.get(
                'similarity_top_k', config.SIMILARITY_TOP_K
            ),
            similarity_threshold=st.session_state.get(
                'similarity_threshold', config.SIMILARITY_THRESHOLD
            ),
            enable_rerank=st.session_state.get(
                'enable_rerank', config.ENABLE_RERANK
            ),
            show_reasoning=st.session_state.get(
                'show_reasoning', config.DEEPSEEK_ENABLE_REASONING_DISPLAY
            ),
            debug_mode=st.session_state.get('debug_mode_enabled', False),
        )
    
    def save_to_session_state(self) -> None:
        """å†™å…¥ session_state"""
        st.session_state.selected_model = self.selected_model
        st.session_state.llm_preset = self.llm_preset
        st.session_state.retrieval_strategy = self.retrieval_strategy
        st.session_state.use_agentic_rag = self.use_agentic_rag
        st.session_state.similarity_top_k = self.similarity_top_k
        st.session_state.similarity_threshold = self.similarity_threshold
        st.session_state.enable_rerank = self.enable_rerank
        st.session_state.show_reasoning = self.show_reasoning
        st.session_state.debug_mode_enabled = self.debug_mode
    
    def get_llm_temperature(self) -> Optional[float]:
        """è·å–å½“å‰é¢„è®¾çš„ temperature
        
        Returns:
            temperature å€¼ï¼Œæ¨ç†æ¨¡å‹è¿”å› None
        """
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ¨ç†æ¨¡å‹
        model_config = config.get_llm_model_config(self.selected_model)
        if model_config and model_config.supports_reasoning:
            return None
        
        preset_params = get_preset_params(self.llm_preset)
        return preset_params["temperature"]
    
    def get_llm_max_tokens(self) -> int:
        """è·å–å½“å‰é¢„è®¾çš„ max_tokens"""
        preset_params = get_preset_params(self.llm_preset)
        return preset_params["max_tokens"]
