"""
å¯¹è¯æ˜¾ç¤ºç»„ä»¶
å‚è€ƒ Streamlit AI Assistant è®¾è®¡ï¼šå•åˆ—å±…ä¸­å¸ƒå±€
"""

import streamlit as st
from frontend.utils.state import initialize_sources_map
from frontend.utils.sources import format_answer_with_citation_links
from frontend.components.sources_panel import display_sources_below_message
from frontend.components.observability_summary import render_observability_summary
from frontend.components.observer_renderers import render_llamadebug_full_info, render_ragas_full_info
from frontend.config import SUGGESTION_QUESTIONS
from backend.infrastructure.config import config
from backend.infrastructure.logger import get_logger

logger = get_logger('app')


def _clear_conversation(chat_manager) -> None:
    """æ¸…ç©ºå¯¹è¯çš„å›è°ƒå‡½æ•°"""
    if chat_manager:
        chat_manager.start_session()
    st.session_state.messages = []
    if 'initial_question' in st.session_state:
        st.session_state.initial_question = None
    if 'initial_question_input' in st.session_state:
        st.session_state.initial_question_input = ""
    if 'selected_suggestion' in st.session_state:
        st.session_state.selected_suggestion = None
    if 'selected_question' in st.session_state:
        st.session_state.selected_question = None
    if 'pending_query' in st.session_state:
        st.session_state.pending_query = None
    if 'keyword_cloud_selected' in st.session_state:
        st.session_state.keyword_cloud_selected = []
    if 'keyword_cloud_generated' in st.session_state:
        st.session_state.keyword_cloud_generated = []
    if 'keyword_cloud_loading' in st.session_state:
        st.session_state.keyword_cloud_loading = False
    # æ¸…ç©ºå¼•ç”¨æ¥æºæ˜ å°„
    if 'current_sources_map' in st.session_state:
        st.session_state.current_sources_map = {}
    if 'current_reasoning_map' in st.session_state:
        st.session_state.current_reasoning_map = {}
    # æ¸…ç©ºè§‚å¯Ÿå™¨æ—¥å¿—
    if 'llama_debug_logs' in st.session_state:
        st.session_state.llama_debug_logs = []
    if 'ragas_logs' in st.session_state:
        st.session_state.ragas_logs = []


def _on_settings_click() -> None:
    """è®¾ç½®æŒ‰é’®ç‚¹å‡»å›è°ƒ"""
    st.session_state.show_settings_dialog = True


def _render_title_row(chat_manager) -> None:
    """æ¸²æŸ“æ ‡é¢˜è¡Œï¼ˆæ ‡é¢˜ + Restart + è®¾ç½®æŒ‰é’®ï¼‰
    
    å‚è€ƒ Streamlit AI Assistant å¸ƒå±€ï¼š
    - æ ‡é¢˜å±…å·¦ï¼ˆå®½åº¦è‡ªé€‚åº”ï¼‰
    - Restart æŒ‰é’®å’Œè®¾ç½®æŒ‰é’®åœ¨å³ä¾§
    """
    # å¦‚æœç”¨æˆ·åˆšè§¦å‘æé—®ï¼ˆä½†æ¶ˆæ¯å°šæœªå†™å…¥ messagesï¼‰ï¼Œä¹Ÿè¦ç«‹å³æ˜¾ç¤º Restart
    has_messages = bool(st.session_state.get('messages'))
    if not has_messages:
        has_messages = any(
            st.session_state.get(key)
            for key in (
                "initial_question",
                "selected_suggestion",
                "selected_question",
                "pending_query",
            )
        )
    
    title_row = st.container()
    with title_row:
        col_title, col_restart, col_settings = st.columns([8, 1, 1])
        
        with col_title:
            st.title("âœ¨ ")
            st.title(config.APP_TITLE, anchor=False)
        
        with col_restart:
            if has_messages:
                st.button(
                    "",
                    icon=":material/refresh:",
                    on_click=_clear_conversation,
                    args=(chat_manager,),
                    key="restart_button",
                    help="Restart"
                )

        with col_settings:
            st.button(
                "",
                icon=":material/settings:",
                on_click=_on_settings_click,
                key="settings_button_top",
                help="è®¾ç½®"
            )


def render_chat_interface(rag_service, chat_manager) -> None:
    """æ¸²æŸ“å¯¹è¯ç•Œé¢
    
    å‚è€ƒ Streamlit AI Assistant è®¾è®¡ï¼š
    - æ ‡é¢˜è¡Œï¼ˆæ ‡é¢˜ + Restart + è®¾ç½®ï¼‰
    - æ— å¯¹è¯ï¼šå»ºè®®é—®é¢˜ + è¾“å…¥æ¡†
    - æœ‰å¯¹è¯ï¼šå¯¹è¯æ°”æ³¡ + è¾“å…¥æ¡†
    
    Args:
        rag_service: RAGæœåŠ¡å®ä¾‹
        chat_manager: å¯¹è¯ç®¡ç†å™¨å®ä¾‹
    """
    # ç»Ÿä¸€å¤„ç†ä¼šè¯åŠ è½½ï¼ˆä¼˜åŒ–ï¼šå‡å°‘ rerun æ¬¡æ•°ï¼‰
    if st.session_state.get('session_loading_pending') or st.session_state.get('load_session_id'):
        from frontend.components.session_loader import load_history_session
        if load_history_session(chat_manager):
            st.rerun()
    
    # æ³¨å…¥å…¨å±€JavaScriptè„šæœ¬ï¼ˆä»…ä¸€æ¬¡ï¼Œå¿…é¡»åœ¨æ¸²æŸ“ä»»ä½•æ¶ˆæ¯å‰ï¼‰
    if not st.session_state.get('citation_script_injected', False):
        from frontend.utils.sources import inject_citation_script
        st.markdown(inject_citation_script(), unsafe_allow_html=True)
        st.session_state.citation_script_injected = True
    
    # æ¸²æŸ“æ ‡é¢˜è¡Œï¼ˆæ ‡é¢˜ + Restart + è®¾ç½®æŒ‰é’®ï¼‰
    _render_title_row(chat_manager)
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ˜¾ç¤ºè®¾ç½®å¼¹çª—
    if st.session_state.get("show_settings_dialog", False):
        from frontend.components.settings_dialog import show_settings_dialog
        show_settings_dialog()
        st.session_state.show_settings_dialog = False
    
    # åˆå§‹åŒ–æ¥æºæ˜ å°„
    initialize_sources_map()

    # Quick start placeholder: clear stale first-screen content on reruns
    quick_start_ph = st.empty()

    # ??????????????? + ?????
    if not st.session_state.messages:
        user_just_asked_initial = bool(st.session_state.get("initial_question"))
        user_just_clicked_suggestion = bool(st.session_state.get("selected_suggestion"))
        user_just_selected_question = bool(st.session_state.get("selected_question"))
        user_has_pending_query = bool(st.session_state.get("pending_query"))
        user_first_interaction = (
            user_just_asked_initial
            or user_just_clicked_suggestion
            or user_just_selected_question
            or user_has_pending_query
        )

        if not user_first_interaction:
            from frontend.components.quick_start import render_quick_start
            with quick_start_ph.container():
                render_quick_start()
            # ???? demo??????????????????????????/??
            st.stop()
        else:
            # ???? messages ??????????
            quick_start_ph.empty()
            if user_just_asked_initial:
                prompt = st.session_state.initial_question
            elif user_just_clicked_suggestion:
                selected_label = st.session_state.selected_suggestion
                prompt = SUGGESTION_QUESTIONS.get(selected_label, selected_label)
            else:
                prompt = None

            if prompt:
                st.session_state.messages.append({"role": "user", "content": prompt})
                st.session_state.pending_query = prompt
                st.session_state.initial_question = None
                st.session_state.selected_suggestion = None
                if 'initial_question_input' in st.session_state:
                    st.session_state.initial_question_input = ""
    else:
        # ??????????????
        quick_start_ph.empty()

    render_chat_history()

    
    # åº•éƒ¨è¾“å…¥æ¡†ï¼ˆæœ‰å¯¹è¯å†å²æ—¶æ˜¾ç¤ºï¼‰
    user_message = st.chat_input("è¾“å…¥è¿½é—®...")
    if user_message:
        st.session_state.pending_query = user_message


def render_chat_history() -> None:
    """æ¸²æŸ“å¯¹è¯å†å²ï¼ˆst.chat_message æ°”æ³¡ + å»¶ç»­å—ï¼‰"""
    from frontend.utils.helpers import generate_message_id
    for idx, msg in enumerate(st.session_state.messages):
        message_id = generate_message_id(idx, msg)
        role = msg["role"]
        if role == "user":
            with st.chat_message("user"):
                st.markdown(msg["content"])
        else:
            if "sources" in msg and msg["sources"]:
                formatted_content = format_answer_with_citation_links(
                    msg["content"],
                    msg["sources"],
                    message_id=message_id
                )
            else:
                formatted_content = msg["content"]
            with st.chat_message("assistant"):
                st.container()  # Fix ghost message bug.
                st.markdown(formatted_content, unsafe_allow_html=True)
            render_assistant_continuation(idx, message_id, msg)
        st.session_state.current_sources_map = st.session_state.current_sources_map
        st.session_state.current_reasoning_map = st.session_state.current_reasoning_map


def render_assistant_continuation(message_index: int, message_id: str, msg: dict) -> None:
    """æ¸²æŸ“åŠ©æ‰‹æ¶ˆæ¯å»¶ç»­å—ï¼ˆè§‚å¯Ÿå™¨ã€æ¨ç†ã€å¼•ç”¨æ¥æºï¼‰ï¼Œæ ·å¼ä¸æ°”æ³¡ç»Ÿä¸€ç”± CP4 CSS å¤„ç†ã€‚"""
    st.markdown(
        f"<div class='message-continuation-anchor' data-message-id='{message_id}'></div>",
        unsafe_allow_html=True,
    )
    with st.chat_message("assistant"):
        _render_observer_info(message_index)
        reasoning_content = msg.get("reasoning_content")
        if reasoning_content:
            with st.expander("ğŸ§  æ¨ç†è¿‡ç¨‹", expanded=False):
                st.markdown(f"```\n{reasoning_content}\n```")
        elif config.DEEPSEEK_ENABLE_REASONING_DISPLAY:
            logger.debug(f"æ¶ˆæ¯ {message_id} æ²¡æœ‰æ¨ç†é“¾å†…å®¹")
        sources = st.session_state.current_sources_map.get(message_id, [])
        if sources:
            st.markdown("#### ğŸ“š å¼•ç”¨æ¥æº")
            display_sources_below_message(sources, message_id=message_id)


def _render_observer_info(message_index: int) -> None:
    """æ¸²æŸ“è§‚å¯Ÿå™¨ä¿¡æ¯ï¼ˆåœ¨ç­”æ¡ˆå‰æ˜¾ç¤ºï¼‰
    
    Args:
        message_index: æ¶ˆæ¯ç´¢å¼•ï¼ˆassistantæ¶ˆæ¯çš„ç´¢å¼•ï¼‰
    """
    # åˆå§‹åŒ–æ—¥å¿—å­˜å‚¨
    if 'llama_debug_logs' not in st.session_state:
        st.session_state.llama_debug_logs = []
    if 'ragas_logs' not in st.session_state:
        st.session_state.ragas_logs = []
    
    # è·å–è§‚å¯Ÿå™¨æ—¥å¿—
    debug_logs = st.session_state.llama_debug_logs
    ragas_logs = st.session_state.ragas_logs
    
    # è®¡ç®—assistantæ¶ˆæ¯çš„æ•°é‡ï¼ˆç”¨äºåŒ¹é…æ—¥å¿—ï¼‰
    assistant_count = sum(1 for msg in st.session_state.messages[:message_index+1] if msg.get("role") == "assistant")
    
    # æ‰¾åˆ°å¯¹åº”çš„æ—¥å¿—ï¼ˆé€šè¿‡assistantæ¶ˆæ¯æ•°é‡åŒ¹é…ï¼‰
    debug_log = None
    ragas_log = None
    
    # å¦‚æœæ—¥å¿—æ•°é‡è¶³å¤Ÿï¼Œä½¿ç”¨å¯¹åº”çš„æ—¥å¿—
    if len(debug_logs) >= assistant_count:
        debug_log = debug_logs[assistant_count - 1]
    elif len(debug_logs) > 0:
        # å¦åˆ™ä½¿ç”¨æœ€æ–°çš„æ—¥å¿—
        debug_log = debug_logs[-1]
    
    if len(ragas_logs) >= assistant_count:
        ragas_log = ragas_logs[assistant_count - 1]
    elif len(ragas_logs) > 0:
        ragas_log = ragas_logs[-1]
    
    # æ˜¾ç¤ºè§‚å¯Ÿå™¨ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰- åˆ†å±‚å±•ç¤º
    if debug_log or ragas_log:
        # L0 + L1: æ™ºèƒ½æ‘˜è¦ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼Œé›†æˆ RAGASï¼‰
        if debug_log:
            render_observability_summary(debug_log, ragas_log=ragas_log, show_l2=False)
        
        # L2: å®Œæ•´é“¾è·¯ï¼ˆæŠ˜å ï¼Œä¾›å¼€å‘è€…è°ƒè¯•ï¼‰
        with st.expander("ğŸ”¬ å®Œæ•´é“¾è·¯è¯¦æƒ…ï¼ˆå¼€å‘è€…ï¼‰", expanded=False):
            if debug_log:
                render_llamadebug_full_info(debug_log)
            
            if ragas_log:
                st.divider()
                render_ragas_full_info(ragas_log)

