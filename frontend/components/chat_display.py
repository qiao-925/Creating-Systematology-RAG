"""
å¯¹è¯æ˜¾ç¤ºç»„ä»¶
"""

import streamlit as st
from typing import Optional
from frontend.utils.helpers import get_chat_title
from frontend.utils.sources import convert_sources_to_dict
from frontend.utils.state import initialize_sources_map
from frontend.utils.sources import format_answer_with_citation_links
from frontend.components.sources_panel import display_sources_below_message
from frontend.components.observability_summary import render_observability_summary
from backend.infrastructure.config import config
from backend.infrastructure.logger import get_logger

logger = get_logger('app')


def render_chat_interface(rag_service, chat_manager) -> None:
    """æ¸²æŸ“å¯¹è¯ç•Œé¢
    
    ä¼˜åŒ–ï¼šç»Ÿä¸€å¤„ç†ä¼šè¯åŠ è½½å’Œrerunï¼Œå‡å°‘é‡å¤æ¸²æŸ“ã€‚
    
    Args:
        rag_service: RAGæœåŠ¡å®ä¾‹
        chat_manager: å¯¹è¯ç®¡ç†å™¨å®ä¾‹
    """
    # ç»Ÿä¸€å¤„ç†ä¼šè¯åŠ è½½ï¼ˆä¼˜åŒ–ï¼šå‡å°‘ rerun æ¬¡æ•°ï¼‰
    if st.session_state.get('session_loading_pending') or st.session_state.get('load_session_id'):
        from frontend.components.session_loader import load_history_session
        if load_history_session(chat_manager):
            st.rerun()
    
    # æ³¨å…¥å…¨å±€JavaScriptè„šæœ¬ï¼ˆä»…ä¸€æ¬¡ï¼Œå¿…é¡»åœ¨æ¸²æŸ“ä»»ä½•æ¶ˆæ¯å‰ï¼‰
    if not st.session_state.get('citation_script_injected', False):
        from frontend.utils.sources import inject_citation_script
        st.markdown(inject_citation_script(), unsafe_allow_html=True)
        st.session_state.citation_script_injected = True
    
    # æ˜¾ç¤ºæ ‡é¢˜
    chat_title = get_chat_title(st.session_state.messages)
    if chat_title:
        st.subheader(chat_title)
        st.markdown("---")
    
    # åˆå§‹åŒ–æ¥æºæ˜ å°„
    initialize_sources_map()
    
    # æ— å¯¹è¯å†å²ï¼šæ˜¾ç¤ºå¿«é€Ÿå¼€å§‹
    if not st.session_state.messages:
        from frontend.components.quick_start import render_quick_start
        render_quick_start()
        return
    
    # æœ‰å¯¹è¯å†å²ï¼šæ˜¾ç¤ºå¯¹è¯
    render_chat_history()


def render_chat_history() -> None:
    """æ¸²æŸ“å¯¹è¯å†å²"""
    # æ˜¾ç¤ºå¯¹è¯å†å²
    from frontend.utils.helpers import generate_message_id
    for idx, message in enumerate(st.session_state.messages):
        message_id = generate_message_id(idx, message)
        with st.chat_message(message["role"]):
            # å¦‚æœæ˜¯AIå›ç­”ï¼Œå…ˆæ˜¾ç¤ºè§‚å¯Ÿå™¨ä¿¡æ¯
            if message["role"] == "assistant":
                _render_observer_info(idx)
            
            # å¦‚æœæ˜¯AIå›ç­”ä¸”åŒ…å«å¼•ç”¨ï¼Œä½¿ç”¨å¸¦é“¾æ¥çš„æ ¼å¼
            if message["role"] == "assistant" and "sources" in message and message["sources"]:
                formatted_content = format_answer_with_citation_links(
                    message["content"],
                    message["sources"],
                    message_id=message_id
                )
                st.markdown(formatted_content, unsafe_allow_html=True)
            else:
                st.markdown(message["content"])
            
            # æ˜¾ç¤ºæ¨ç†é“¾ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼Œå¦‚æœå­˜åœ¨ï¼‰
            if message["role"] == "assistant":
                reasoning_content = message.get("reasoning_content")
                # è°ƒè¯•ï¼šæ£€æŸ¥æ¨ç†é“¾æ˜¯å¦å­˜åœ¨
                if reasoning_content:
                    with st.expander("ğŸ§  æ¨ç†è¿‡ç¨‹", expanded=False):
                        st.markdown(f"```\n{reasoning_content}\n```")
                else:
                    # è°ƒè¯•ï¼šæ˜¾ç¤ºä¸ºä»€ä¹ˆæ²¡æœ‰æ¨ç†é“¾
                    if config.DEEPSEEK_ENABLE_REASONING_DISPLAY:
                        # åªåœ¨å¯ç”¨æ˜¾ç¤ºæ—¶æ‰æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
                        logger.debug(f"æ¶ˆæ¯ {message_id} æ²¡æœ‰æ¨ç†é“¾å†…å®¹")
        
        # åœ¨æ¶ˆæ¯ä¸‹æ–¹æ˜¾ç¤ºå¼•ç”¨æ¥æºï¼ˆå¦‚æœæœ‰ï¼‰
        if message["role"] == "assistant":
            sources = st.session_state.current_sources_map.get(message_id, [])
            if sources:
                # æ˜¾ç¤ºå¼•ç”¨æ¥æºæ ‡é¢˜
                st.markdown("#### ğŸ“š å¼•ç”¨æ¥æº")
                # æ˜¾ç¤ºå¼•ç”¨æ¥æºè¯¦æƒ…
                display_sources_below_message(sources, message_id=message_id)
        
        # æ›´æ–°session_stateä¸­çš„æ˜ å°„ï¼ˆç¡®ä¿åŒæ­¥ï¼‰
        st.session_state.current_sources_map = st.session_state.current_sources_map
        st.session_state.current_reasoning_map = st.session_state.current_reasoning_map


def _render_observer_info(message_index: int) -> None:
    """æ¸²æŸ“è§‚å¯Ÿå™¨ä¿¡æ¯ï¼ˆåœ¨ç­”æ¡ˆå‰æ˜¾ç¤ºï¼‰
    
    Args:
        message_index: æ¶ˆæ¯ç´¢å¼•ï¼ˆassistantæ¶ˆæ¯çš„ç´¢å¼•ï¼‰
    """
    # åˆå§‹åŒ–æ—¥å¿—å­˜å‚¨
    if 'llama_debug_logs' not in st.session_state:
        st.session_state.llama_debug_logs = []
    if 'ragas_logs' not in st.session_state:
        st.session_state.ragas_logs = []
    
    # è·å–è§‚å¯Ÿå™¨æ—¥å¿—
    debug_logs = st.session_state.llama_debug_logs
    ragas_logs = st.session_state.ragas_logs
    
    # è®¡ç®—assistantæ¶ˆæ¯çš„æ•°é‡ï¼ˆç”¨äºåŒ¹é…æ—¥å¿—ï¼‰
    assistant_count = sum(1 for msg in st.session_state.messages[:message_index+1] if msg.get("role") == "assistant")
    
    # æ‰¾åˆ°å¯¹åº”çš„æ—¥å¿—ï¼ˆé€šè¿‡assistantæ¶ˆæ¯æ•°é‡åŒ¹é…ï¼‰
    debug_log = None
    ragas_log = None
    
    # å¦‚æœæ—¥å¿—æ•°é‡è¶³å¤Ÿï¼Œä½¿ç”¨å¯¹åº”çš„æ—¥å¿—
    if len(debug_logs) >= assistant_count:
        debug_log = debug_logs[assistant_count - 1]
    elif len(debug_logs) > 0:
        # å¦åˆ™ä½¿ç”¨æœ€æ–°çš„æ—¥å¿—
        debug_log = debug_logs[-1]
    
    if len(ragas_logs) >= assistant_count:
        ragas_log = ragas_logs[assistant_count - 1]
    elif len(ragas_logs) > 0:
        ragas_log = ragas_logs[-1]
    
    # æ˜¾ç¤ºè§‚å¯Ÿå™¨ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰- åˆ†å±‚å±•ç¤º
    if debug_log or ragas_log:
        # L0 + L1: æ™ºèƒ½æ‘˜è¦ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼Œé›†æˆ RAGASï¼‰
        if debug_log:
            render_observability_summary(debug_log, ragas_log=ragas_log, show_l2=False)
        
        # L2: å®Œæ•´é“¾è·¯ï¼ˆæŠ˜å ï¼Œä¾›å¼€å‘è€…è°ƒè¯•ï¼‰
        with st.expander("ğŸ”¬ å®Œæ•´é“¾è·¯è¯¦æƒ…ï¼ˆå¼€å‘è€…ï¼‰", expanded=False):
            if debug_log:
                _render_llamadebug_full_info(debug_log)
            
            if ragas_log:
                st.divider()
                _render_ragas_full_info(ragas_log)


def _render_llamadebug_full_info(debug_log: dict) -> None:
    """æŒ‰æ‰§è¡Œæµç¨‹æ¸²æŸ“ LlamaDebug å…¨é‡ä¿¡æ¯"""
    
    # ========== é˜¶æ®µ1: æŸ¥è¯¢å¼€å§‹ ==========
    st.markdown("##### ğŸ“ 1. æŸ¥è¯¢é˜¶æ®µ")
    if debug_log.get('query'):
        st.markdown(f"**åŸå§‹æŸ¥è¯¢**: `{debug_log['query']}`")
    
    # æŸ¥è¯¢å¤„ç†ç»“æœï¼ˆæ–°å¢ï¼‰
    query_processing = debug_log.get('query_processing')
    if query_processing:
        col1, col2 = st.columns(2)
        with col1:
            if query_processing.get('rewritten_queries'):
                rewritten = query_processing['rewritten_queries']
                if len(rewritten) > 0:
                    st.markdown(f"**æ”¹å†™åçš„æŸ¥è¯¢**: `{rewritten[0]}`")
                    if len(rewritten) > 1:
                        with st.expander(f"å…¶ä»–æ”¹å†™ç‰ˆæœ¬ ({len(rewritten)-1} ä¸ª)", expanded=False):
                            for i, q in enumerate(rewritten[1:], 2):
                                st.markdown(f"**ç‰ˆæœ¬ {i}**: `{q}`")
        
        with col2:
            if query_processing.get('processing_method'):
                method = query_processing['processing_method']
                method_label = "ç®€å•æŸ¥è¯¢ï¼ˆè·³è¿‡LLMï¼‰" if method == "simple" else "LLMå¤„ç†"
                st.markdown(f"**å¤„ç†æ–¹å¼**: {method_label}")
        
        # æ„å›¾ç†è§£ç»“æœï¼ˆæ–°å¢ï¼‰
        understanding = query_processing.get('understanding')
        if understanding:
            with st.expander("ğŸ§  æŸ¥è¯¢æ„å›¾ç†è§£", expanded=False):
                if isinstance(understanding, dict):
                    if understanding.get('query_type'):
                        st.markdown(f"**æŸ¥è¯¢ç±»å‹**: `{understanding['query_type']}`")
                    if understanding.get('complexity'):
                        complexity = understanding['complexity']
                        complexity_label = {
                            'simple': 'ç®€å•',
                            'medium': 'ä¸­ç­‰',
                            'complex': 'å¤æ‚'
                        }.get(complexity, complexity)
                        st.markdown(f"**å¤æ‚åº¦**: {complexity_label}")
                    if understanding.get('intent'):
                        st.markdown(f"**æŸ¥è¯¢æ„å›¾**: {understanding['intent']}")
                    if understanding.get('entities'):
                        entities = understanding['entities']
                        if entities:
                            st.markdown(f"**å…³é”®å®ä½“**: {', '.join(entities)}")
                    if understanding.get('confidence') is not None:
                        st.markdown(f"**ç½®ä¿¡åº¦**: {understanding['confidence']:.2f}")
                else:
                    st.json(understanding)
    
    # é…ç½®ä¿¡æ¯ï¼ˆæ–°å¢ï¼‰
    if debug_log.get('llm_model') or debug_log.get('retrieval_strategy') or debug_log.get('top_k'):
        with st.expander("âš™ï¸ é…ç½®ä¿¡æ¯", expanded=False):
            col1, col2, col3 = st.columns(3)
            with col1:
                if debug_log.get('llm_model'):
                    st.markdown(f"**LLMæ¨¡å‹**: `{debug_log['llm_model']}`")
                if debug_log.get('llm_params'):
                    params = debug_log['llm_params']
                    if params.get('temperature') is not None:
                        st.markdown(f"**Temperature**: {params['temperature']}")
                    if params.get('max_tokens') is not None:
                        st.markdown(f"**Max Tokens**: {params['max_tokens']}")
            with col2:
                if debug_log.get('retrieval_strategy'):
                    st.markdown(f"**æ£€ç´¢ç­–ç•¥**: `{debug_log['retrieval_strategy']}`")
            with col3:
                if debug_log.get('top_k'):
                    st.markdown(f"**Top K**: {debug_log['top_k']}")
    
    # åŸºç¡€ç»Ÿè®¡
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("äº‹ä»¶æ€»æ•°", debug_log.get('events_count', 0))
    with col2:
        st.metric("LLMè°ƒç”¨", debug_log.get('llm_calls', 0))
    with col3:
        st.metric("æ£€ç´¢è°ƒç”¨", debug_log.get('retrieval_calls', 0))
    with col4:
        total_time = debug_log.get('total_time', 0)
        if total_time:
            st.metric("æ€»è€—æ—¶", f"{total_time:.3f}s")
    
    # äº‹ä»¶ç±»å‹ç»Ÿè®¡
    if debug_log.get('event_type_counts'):
        st.markdown("**äº‹ä»¶ç±»å‹ç»Ÿè®¡**:")
        event_counts = debug_log['event_type_counts']
        cols = st.columns(min(len(event_counts), 5))
        for idx, (event_type, count) in enumerate(list(event_counts.items())[:5]):
            with cols[idx % 5]:
                st.markdown(f"- `{event_type}`: {count}")
    
    st.divider()
    
    # ========== é˜¶æ®µ2: æ£€ç´¢é˜¶æ®µ ==========
    st.markdown("##### ğŸ” 2. æ£€ç´¢é˜¶æ®µ")
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"**æ£€ç´¢è°ƒç”¨æ¬¡æ•°**: {debug_log.get('retrieval_calls', 0)}")
        if debug_log.get('stage_times'):
            retrieval_time = sum(
                time for event_type, time in debug_log['stage_times'].items()
                if 'retriev' in event_type.lower() or 'retrieve' in event_type.lower()
            )
            if retrieval_time > 0:
                st.markdown(f"**æ£€ç´¢è€—æ—¶**: {retrieval_time:.3f}s")
    
    with col2:
        st.markdown(f"**å¼•ç”¨æ¥æºæ•°**: {debug_log.get('sources_count', 0)}")
    
    # æ£€ç´¢æŸ¥è¯¢
    if debug_log.get('retrieval_queries'):
        with st.expander(f"ğŸ“‹ æ£€ç´¢æŸ¥è¯¢ ({len(debug_log['retrieval_queries'])} ä¸ª)", expanded=False):
            for i, query in enumerate(debug_log['retrieval_queries'], 1):
                st.markdown(f"**æ£€ç´¢æŸ¥è¯¢ {i}**:")
                st.code(query, language=None)
    
    # æ£€ç´¢åˆ°çš„èŠ‚ç‚¹
    if debug_log.get('retrieved_nodes'):
        with st.expander(f"ğŸ“„ æ£€ç´¢åˆ°çš„èŠ‚ç‚¹ ({len(debug_log['retrieved_nodes'])} ä¸ª)", expanded=False):
            for i, node in enumerate(debug_log['retrieved_nodes'], 1):
                st.markdown(f"**èŠ‚ç‚¹ {i}**:")
                st.text(node[:500] + "..." if len(node) > 500 else node)
    
    # å¼•ç”¨æ¥æºè¯¦æƒ…
    if debug_log.get('sources'):
        with st.expander(f"ğŸ“š å¼•ç”¨æ¥æºè¯¦æƒ… ({len(debug_log['sources'])} ä¸ª)", expanded=False):
            for i, source in enumerate(debug_log['sources'], 1):
                st.markdown(f"**æ¥æº {i}**:")
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.text(source.get('text', '')[:300] + "..." if len(source.get('text', '')) > 300 else source.get('text', ''))
                with col2:
                    if source.get('score') is not None:
                        st.metric("ç›¸ä¼¼åº¦", f"{source['score']:.4f}")
                    if source.get('id'):
                        st.caption(f"ID: {source['id']}")
                    if source.get('metadata'):
                        with st.expander("å…ƒæ•°æ®", expanded=False):
                            st.json(source['metadata'])
    
    st.divider()
    
    # ========== é˜¶æ®µ3: LLMè°ƒç”¨é˜¶æ®µ ==========
    st.markdown("##### ğŸ¤– 3. LLMè°ƒç”¨é˜¶æ®µ")
    
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("LLMè°ƒç”¨æ¬¡æ•°", debug_log.get('llm_calls', 0))
    with col2:
        if debug_log.get('prompt_tokens', 0) > 0:
            st.metric("Prompt Tokens", debug_log['prompt_tokens'])
    with col3:
        if debug_log.get('completion_tokens', 0) > 0:
            st.metric("Completion Tokens", debug_log['completion_tokens'])
    with col4:
        if debug_log.get('total_tokens', 0) > 0:
            st.metric("Total Tokens", debug_log['total_tokens'])
    
    # LLMè°ƒç”¨è€—æ—¶
    if debug_log.get('stage_times'):
        llm_time = sum(
            time for event_type, time in debug_log['stage_times'].items()
            if 'llm' in event_type.lower()
        )
        if llm_time > 0:
            st.markdown(f"**LLMè°ƒç”¨æ€»è€—æ—¶**: {llm_time:.3f}s")
    
    # LLM Prompts
    if debug_log.get('llm_prompts'):
        with st.expander(f"ğŸ’¬ LLM Prompts ({len(debug_log['llm_prompts'])} ä¸ª)", expanded=False):
            for i, prompt in enumerate(debug_log['llm_prompts'], 1):
                st.markdown(f"**Prompt {i}**:")
                st.code(prompt, language=None)
    
    # LLM Responses
    if debug_log.get('llm_responses'):
        with st.expander(f"ğŸ“¤ LLM Responses ({len(debug_log['llm_responses'])} ä¸ª)", expanded=False):
            for i, response in enumerate(debug_log['llm_responses'], 1):
                st.markdown(f"**Response {i}**:")
                st.code(response, language=None)
    
    st.divider()
    
    # ========== é˜¶æ®µ4: ç”Ÿæˆé˜¶æ®µ ==========
    st.markdown("##### âœ¨ 4. ç”Ÿæˆé˜¶æ®µ")
    
    col1, col2 = st.columns(2)
    with col1:
        st.markdown(f"**ç­”æ¡ˆé•¿åº¦**: {debug_log.get('answer_length', 0)} å­—ç¬¦")
    with col2:
        if debug_log.get('stage_times'):
            gen_time = sum(
                time for event_type, time in debug_log['stage_times'].items()
                if 'synthesize' in event_type.lower() or 'generate' in event_type.lower()
            )
            if gen_time > 0:
                st.markdown(f"**ç”Ÿæˆè€—æ—¶**: {gen_time:.3f}s")
    
    # ç­”æ¡ˆé¢„è§ˆ
    if debug_log.get('answer'):
        with st.expander("ğŸ“„ ç­”æ¡ˆé¢„è§ˆ", expanded=False):
            st.markdown(debug_log['answer'])
    
    st.divider()
    
    # ========== é˜¶æ®µ5: æ€§èƒ½æŒ‡æ ‡ ==========
    st.markdown("##### â±ï¸ 5. æ€§èƒ½æŒ‡æ ‡")
    
    if debug_log.get('stage_times'):
        st.markdown("**å„é˜¶æ®µè€—æ—¶æ˜ç»†**:")
        stage_times = debug_log['stage_times']
        for event_type, duration in sorted(stage_times.items(), key=lambda x: x[1], reverse=True):
            st.markdown(f"- `{event_type}`: {duration:.3f}s")
    
    st.divider()
    
    # ========== é˜¶æ®µ6: äº‹ä»¶è¯¦æƒ… ==========
    st.markdown("##### ğŸ” 6. äº‹ä»¶è¯¦æƒ…")
    
    if debug_log.get('event_pairs'):
        with st.expander(f"ğŸ“‹ äº‹ä»¶å¯¹è¯¦æƒ… ({len(debug_log['event_pairs'])} ä¸ª)", expanded=False):
            for i, pair in enumerate(debug_log['event_pairs'], 1):
                event_type = pair.get('event_type', 'Unknown')
                duration = pair.get('duration')
                
                with st.expander(f"äº‹ä»¶å¯¹ {i}: {event_type}" + (f" ({duration:.3f}s)" if duration else ""), expanded=False):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**å¼€å§‹äº‹ä»¶**:")
                        if pair.get('start_event'):
                            st.code(pair['start_event'], language=None)
                        if pair.get('start_time'):
                            st.caption(f"æ—¶é—´: {pair['start_time']}")
                    
                    with col2:
                        st.markdown("**ç»“æŸäº‹ä»¶**:")
                        if pair.get('end_event'):
                            st.code(pair['end_event'], language=None)
                        if pair.get('end_time'):
                            st.caption(f"æ—¶é—´: {pair['end_time']}")
                    
                    # Payloadè¯¦æƒ…
                    if pair.get('payload'):
                        with st.expander("Payloadè¯¦æƒ…", expanded=False):
                            st.json(pair['payload'])
    
    # ========== é˜¶æ®µ7: é”™è¯¯å’Œè­¦å‘Š ==========
    errors = debug_log.get('errors', [])
    warnings = debug_log.get('warnings', [])
    
    if errors or warnings:
        st.markdown("##### âš ï¸ 7. é”™è¯¯å’Œè­¦å‘Š")
        
        if errors:
            st.error(f"âŒ é”™è¯¯ ({len(errors)} ä¸ª)")
            for i, error in enumerate(errors, 1):
                st.markdown(f"**é”™è¯¯ {i}**: {error}")
        
        if warnings:
            st.warning(f"âš ï¸ è­¦å‘Š ({len(warnings)} ä¸ª)")
            for i, warning in enumerate(warnings, 1):
                st.markdown(f"**è­¦å‘Š {i}**: {warning}")


def _render_ragas_full_info(ragas_log: dict) -> None:
    """æŒ‰æ‰§è¡Œæµç¨‹æ¸²æŸ“ RAGAS å…¨é‡ä¿¡æ¯"""
    
    is_pending = ragas_log.get('pending_evaluation', False)
    status_icon = "â³" if is_pending else "âœ…"
    
    # ========== é˜¶æ®µ1: æ•°æ®æ”¶é›†é˜¶æ®µ ==========
    st.markdown("##### ğŸ“¥ 1. æ•°æ®æ”¶é›†é˜¶æ®µ")
    
    st.markdown(f"**çŠ¶æ€**: {status_icon} {'å¾…è¯„ä¼°' if is_pending else 'å·²è¯„ä¼°'}")
    
    # æ•°æ®ç»Ÿè®¡
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ç­”æ¡ˆé•¿åº¦", f"{ragas_log.get('answer_length', 0)} å­—ç¬¦")
    with col2:
        st.metric("ä¸Šä¸‹æ–‡æ•°", ragas_log.get('contexts_count', 0))
    with col3:
        st.metric("æ¥æºæ•°", ragas_log.get('sources_count', 0))
    with col4:
        if ragas_log.get('timestamp'):
            from datetime import datetime
            try:
                ts = datetime.fromisoformat(ragas_log['timestamp'].replace('Z', '+00:00'))
                st.caption(f"æ”¶é›†æ—¶é—´: {ts.strftime('%H:%M:%S')}")
            except:
                st.caption(f"æ—¶é—´: {ragas_log['timestamp']}")
    
    # æŸ¥è¯¢å†…å®¹
    if ragas_log.get('query'):
        st.markdown("**ğŸ“ æŸ¥è¯¢å†…å®¹**:")
        st.code(ragas_log['query'], language=None)
    
    # ç­”æ¡ˆå†…å®¹
    if ragas_log.get('answer'):
        with st.expander(f"ğŸ“„ ç­”æ¡ˆå†…å®¹ ({ragas_log.get('answer_length', 0)} å­—ç¬¦)", expanded=False):
            st.markdown(ragas_log['answer'])
    
    # ä¸Šä¸‹æ–‡è¯¦æƒ…
    if ragas_log.get('contexts'):
        st.markdown(f"**ğŸ“š ä¸Šä¸‹æ–‡æ•°æ® ({len(ragas_log['contexts'])} ä¸ª)**:")
        with st.expander(f"æŸ¥çœ‹æ‰€æœ‰ä¸Šä¸‹æ–‡", expanded=False):
            for i, ctx in enumerate(ragas_log['contexts'], 1):
                st.markdown(f"**ä¸Šä¸‹æ–‡ {i}** ({len(ctx)} å­—ç¬¦):")
                st.text(ctx[:800] + "..." if len(ctx) > 800 else ctx)
                st.divider()
    
    # æ¥æºè¯¦æƒ…
    if ragas_log.get('sources'):
        st.markdown(f"**ğŸ”— æ¥æºè¯¦æƒ… ({len(ragas_log['sources'])} ä¸ª)**:")
        with st.expander(f"æŸ¥çœ‹æ‰€æœ‰æ¥æº", expanded=False):
            for i, source in enumerate(ragas_log['sources'], 1):
                st.markdown(f"**æ¥æº {i}**:")
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.text(source.get('text', '')[:400] + "..." if len(source.get('text', '')) > 400 else source.get('text', ''))
                with col2:
                    if source.get('score') is not None:
                        st.metric("ç›¸ä¼¼åº¦", f"{source['score']:.4f}")
                    if source.get('metadata'):
                        with st.expander("å…ƒæ•°æ®", expanded=False):
                            st.json(source['metadata'])
                st.divider()
    elif ragas_log.get('sources_count', 0) > 0:
        st.markdown(f"**ğŸ”— æ¥æºç»Ÿè®¡**: {ragas_log['sources_count']} ä¸ªæ¥æº")
    
    # Ground Truthï¼ˆå¦‚æœæœ‰ï¼‰
    if ragas_log.get('ground_truth'):
        with st.expander("ğŸ¯ Ground Truthï¼ˆçœŸå€¼ï¼‰", expanded=False):
            st.markdown(ragas_log['ground_truth'])
    
    # Trace IDï¼ˆå¦‚æœæœ‰ï¼‰
    if ragas_log.get('trace_id'):
        st.caption(f"Trace ID: {ragas_log['trace_id']}")
    
    st.divider()
    
    # ========== é˜¶æ®µ2: æ‰¹é‡è¯„ä¼°çŠ¶æ€ ==========
    st.markdown("##### ğŸ“Š 2. æ‰¹é‡è¯„ä¼°çŠ¶æ€")
    
    # è®¡ç®—å¾…è¯„ä¼°æ•°æ®é‡
    if 'ragas_logs' in st.session_state:
        pending_count = sum(
            1 for log in st.session_state.ragas_logs 
            if log.get('pending_evaluation', True)
        )
        evaluated_count = len(st.session_state.ragas_logs) - pending_count
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("æ€»è®°å½•æ•°", len(st.session_state.ragas_logs))
        with col2:
            st.metric("å¾…è¯„ä¼°", pending_count, delta=None)
        with col3:
            st.metric("å·²è¯„ä¼°", evaluated_count, delta=None)
        
        # æ‰¹é‡è¯„ä¼°è¿›åº¦
        batch_size = ragas_log.get('evaluation_batch_size', 10)  # ä»è¯„ä¼°ç»“æœè·å–ï¼Œæˆ–ä½¿ç”¨é»˜è®¤å€¼10
        if is_pending:
            progress = min(pending_count / batch_size, 1.0)
            st.progress(progress)
            st.info(f"â³ å½“å‰è®°å½•å¾…è¯„ä¼°ï¼Œæ‰¹é‡è¯„ä¼°å°†åœ¨è¾¾åˆ° {batch_size} æ¡æ•°æ®æ—¶è‡ªåŠ¨è§¦å‘ï¼ˆå½“å‰: {pending_count}/{batch_size}ï¼‰")
            
            # æ˜¾ç¤ºè¯„ä¼°é˜Ÿåˆ—ä¿¡æ¯
            if pending_count > 0:
                st.markdown(f"**è¯„ä¼°é˜Ÿåˆ—**: è¿˜æœ‰ {pending_count - 1} æ¡è®°å½•åœ¨é˜Ÿåˆ—ä¸­ç­‰å¾…")
        else:
            st.success("âœ… æ­¤è®°å½•å·²å®Œæˆè¯„ä¼°")
            if ragas_log.get('evaluation_timestamp'):
                from datetime import datetime
                try:
                    eval_ts = datetime.fromisoformat(ragas_log['evaluation_timestamp'].replace('Z', '+00:00'))
                    st.caption(f"è¯„ä¼°æ—¶é—´: {eval_ts.strftime('%Y-%m-%d %H:%M:%S')}")
                except:
                    st.caption(f"è¯„ä¼°æ—¶é—´: {ragas_log['evaluation_timestamp']}")
    
    st.divider()
    
    # ========== é˜¶æ®µ3: è¯„ä¼°æŒ‡æ ‡è¯¦æƒ… ==========
    st.markdown("##### ğŸ“ˆ 3. è¯„ä¼°æŒ‡æ ‡è¯¦æƒ…")
    
    if not is_pending and ragas_log.get('evaluation_result'):
        eval_result = ragas_log['evaluation_result']
        
        if isinstance(eval_result, dict):
            # æ˜¾ç¤ºæ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
            st.markdown("**è¯„ä¼°æŒ‡æ ‡æ¦‚è§ˆ**:")
            
            # æŒ‰æŒ‡æ ‡åˆ†ç»„æ˜¾ç¤º
            metrics_list = list(eval_result.items())
            num_cols = min(len(metrics_list), 5)
            cols = st.columns(num_cols)
            
            for idx, (metric, value) in enumerate(metrics_list):
                with cols[idx % num_cols]:
                    value_str = f"{value:.4f}" if isinstance(value, (int, float)) else str(value)
                    if isinstance(value, (int, float)):
                        # æ ¹æ®å€¼æ˜¾ç¤ºä¸åŒé¢œè‰²å’ŒçŠ¶æ€
                        if value >= 0.8:
                            color = "ğŸŸ¢"
                            status = "ä¼˜ç§€"
                        elif value >= 0.6:
                            color = "ğŸŸ¡"
                            status = "è‰¯å¥½"
                        else:
                            color = "ğŸ”´"
                            status = "éœ€æ”¹è¿›"
                        
                        st.metric(f"{color} {metric}", value_str, delta=status)
                    else:
                        st.markdown(f"**{metric}**: {value_str}")
            
            st.divider()
            
            # è¯¦ç»†æŒ‡æ ‡è¯´æ˜
            st.markdown("**æŒ‡æ ‡è¯´æ˜**:")
            metric_descriptions = {
                "faithfulness": "å¿ å®åº¦ï¼šç­”æ¡ˆæ˜¯å¦åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡",
                "context_precision": "ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼šæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç›¸å…³",
                "context_recall": "ä¸Šä¸‹æ–‡å¬å›ç‡ï¼šæ˜¯å¦æ£€ç´¢åˆ°æ‰€æœ‰ç›¸å…³ä¿¡æ¯",
                "answer_relevancy": "ç­”æ¡ˆç›¸å…³æ€§ï¼šç­”æ¡ˆæ˜¯å¦å›ç­”äº†æŸ¥è¯¢",
                "context_relevancy": "ä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼šä¸Šä¸‹æ–‡æ˜¯å¦ä¸æŸ¥è¯¢ç›¸å…³",
            }
            
            for metric, value in eval_result.items():
                description = metric_descriptions.get(metric, "è¯„ä¼°æŒ‡æ ‡")
                value_str = f"{value:.4f}" if isinstance(value, (int, float)) else str(value)
                
                with st.expander(f"ğŸ“Š {metric}: {value_str}", expanded=False):
                    st.markdown(f"**è¯´æ˜**: {description}")
                    if isinstance(value, (int, float)):
                        # æ˜¾ç¤ºè¿›åº¦æ¡
                        st.progress(value)
                        if value >= 0.8:
                            st.success(f"âœ… ä¼˜ç§€ ({value:.2%})")
                        elif value >= 0.6:
                            st.warning(f"âš ï¸ è‰¯å¥½ ({value:.2%})")
                        else:
                            st.error(f"âŒ éœ€æ”¹è¿› ({value:.2%})")
        else:
            st.markdown("**è¯„ä¼°ç»“æœ**:")
            st.text(str(eval_result)[:1000])
    elif is_pending:
        st.info("â³ æ­¤è®°å½•å°šæœªè¯„ä¼°ï¼Œç­‰å¾…æ‰¹é‡è¯„ä¼°è§¦å‘")
    else:
        st.warning("âš ï¸ æš‚æ— è¯„ä¼°ç»“æœ")
    
    st.divider()
    
    # ========== é˜¶æ®µ4: è¯„ä¼°æ•°æ®è´¨é‡ ==========
    st.markdown("##### ğŸ” 4. è¯„ä¼°æ•°æ®è´¨é‡")
    
    # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
    quality_checks = []
    
    if ragas_log.get('query'):
        quality_checks.append(("âœ…", "æŸ¥è¯¢æ•°æ®", "å·²æ”¶é›†", f"{len(ragas_log['query'])} å­—ç¬¦"))
    else:
        quality_checks.append(("âŒ", "æŸ¥è¯¢æ•°æ®", "ç¼ºå¤±", ""))
    
    if ragas_log.get('answer'):
        quality_checks.append(("âœ…", "ç­”æ¡ˆæ•°æ®", "å·²æ”¶é›†", f"{ragas_log.get('answer_length', 0)} å­—ç¬¦"))
    else:
        quality_checks.append(("âŒ", "ç­”æ¡ˆæ•°æ®", "ç¼ºå¤±", ""))
    
    if ragas_log.get('contexts_count', 0) > 0:
        total_ctx_len = sum(len(ctx) for ctx in ragas_log.get('contexts', []))
        quality_checks.append(("âœ…", "ä¸Šä¸‹æ–‡æ•°æ®", f"å·²æ”¶é›† ({ragas_log.get('contexts_count', 0)} ä¸ª)", f"æ€»é•¿åº¦: {total_ctx_len} å­—ç¬¦"))
    else:
        quality_checks.append(("âš ï¸", "ä¸Šä¸‹æ–‡æ•°æ®", "ä¸ºç©º", ""))
    
    if ragas_log.get('sources_count', 0) > 0:
        quality_checks.append(("âœ…", "æ¥æºæ•°æ®", f"å·²æ”¶é›† ({ragas_log.get('sources_count', 0)} ä¸ª)", ""))
    else:
        quality_checks.append(("âš ï¸", "æ¥æºæ•°æ®", "ä¸ºç©º", ""))
    
    if ragas_log.get('ground_truth'):
        quality_checks.append(("âœ…", "Ground Truth", "å·²æä¾›", ""))
    else:
        quality_checks.append(("â„¹ï¸", "Ground Truth", "æœªæä¾›ï¼ˆå¯é€‰ï¼‰", ""))
    
    # æ˜¾ç¤ºè´¨é‡æ£€æŸ¥ç»“æœ
    for icon, check_name, status, detail in quality_checks:
        if detail:
            st.markdown(f"{icon} **{check_name}**: {status} - {detail}")
        else:
            st.markdown(f"{icon} **{check_name}**: {status}")
    
    # æ•°æ®è´¨é‡è¯„åˆ†ï¼ˆåªè®¡ç®—å¿…éœ€é¡¹ï¼‰
    required_checks = [c for c in quality_checks if c[0] != "â„¹ï¸"]  # æ’é™¤å¯é€‰é¡¹
    quality_score = sum(1 for icon, _, _, _ in required_checks if icon == "âœ…") / len(required_checks) if required_checks else 0
    passed_checks = sum(1 for icon, _, _, _ in required_checks if icon == "âœ…")
    st.markdown(f"**æ•°æ®å®Œæ•´æ€§**: {quality_score:.0%} ({passed_checks}/{len(required_checks)} å¿…éœ€é¡¹é€šè¿‡)")
    st.progress(quality_score)
    
    # æ•°æ®è´¨é‡å»ºè®®
    if quality_score < 1.0:
        st.warning("âš ï¸ æ•°æ®ä¸å®Œæ•´ï¼Œå¯èƒ½å½±å“è¯„ä¼°å‡†ç¡®æ€§")
    elif quality_score == 1.0 and ragas_log.get('ground_truth'):
        st.success("âœ… æ•°æ®å®Œæ•´ä¸”åŒ…å« Ground Truthï¼Œè¯„ä¼°ç»“æœæœ€å‡†ç¡®")
    elif quality_score == 1.0:
        st.info("â„¹ï¸ æ•°æ®å®Œæ•´ï¼Œä½†ç¼ºå°‘ Ground Truthï¼Œéƒ¨åˆ†æŒ‡æ ‡å¯èƒ½æ— æ³•è®¡ç®—")

