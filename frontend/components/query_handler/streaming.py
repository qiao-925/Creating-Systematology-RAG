"""
æµå¼æŸ¥è¯¢å¤„ç†ç»„ä»¶
"""

import streamlit as st
import asyncio
from typing import Optional
from frontend.utils.sources import convert_sources_to_dict
from frontend.utils.state import save_message_to_history
from frontend.utils.sources import format_answer_with_citation_links
from frontend.components.query_handler.common import display_reasoning, display_sources, save_to_chat_manager
from backend.infrastructure.logger import get_logger

logger = get_logger('app')


def handle_streaming_query(chat_manager, prompt: str) -> None:
    """å¤„ç†æµå¼æŸ¥è¯¢
    
    Args:
        chat_manager: å¯¹è¯ç®¡ç†å™¨å®ä¾‹
        prompt: ç”¨æˆ·æŸ¥è¯¢
    """
    with st.chat_message("assistant"):
            # åˆ›å»ºæ¶ˆæ¯å ä½ç¬¦ç”¨äºæµå¼æ›´æ–°
            message_placeholder = st.empty()
            
            try:
                full_answer = ""
                local_sources = []
                reasoning_content = None
                
                # å¼‚æ­¥æµå¼å¤„ç†
                async def process_stream():
                    nonlocal full_answer, local_sources, reasoning_content
                    async for chunk in chat_manager.stream_chat(prompt):
                        if chunk['type'] == 'token':
                            full_answer += chunk['data']
                            # å®æ—¶æ›´æ–°æ˜¾ç¤ºï¼ˆå¸¦å…‰æ ‡æ•ˆæœï¼‰
                            message_placeholder.markdown(full_answer + "â–Œ")
                        elif chunk['type'] == 'sources':
                            local_sources = chunk['data']
                        elif chunk['type'] == 'reasoning':
                            reasoning_content = chunk['data']
                        elif chunk['type'] == 'done':
                            # æµå¼å®Œæˆï¼Œç§»é™¤å…‰æ ‡
                            if 'answer' in chunk['data']:
                                full_answer = chunk['data']['answer']
                            if 'sources' in chunk['data']:
                                local_sources = chunk['data']['sources']
                            if 'reasoning_content' in chunk['data']:
                                reasoning_content = chunk['data']['reasoning_content']
                            message_placeholder.markdown(full_answer)
                        elif chunk['type'] == 'error':
                            st.error(f"âŒ æµå¼å¯¹è¯å¤±è´¥: {chunk['data'].get('message', 'Unknown error')}")
                            return
                
                # è¿è¡Œå¼‚æ­¥æµå¼å¤„ç†
                _run_async_stream(process_stream)
                
                # è½¬æ¢å¼•ç”¨æ¥æºæ ¼å¼
                local_sources = convert_sources_to_dict(local_sources)
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æ¨ç†é“¾æå–æƒ…å†µ
                logger.info(f"ğŸ” æ¨ç†é“¾æå–æ£€æŸ¥: reasoning_content={reasoning_content is not None}, é•¿åº¦={len(reasoning_content) if reasoning_content else 0}")
                if reasoning_content:
                    logger.info(f"âœ… æ¨ç†é“¾å†…å®¹é¢„è§ˆï¼ˆå‰100å­—ç¬¦ï¼‰: {reasoning_content[:100]}...")
                else:
                    logger.warning("âš ï¸ å“åº”ä¸­æ²¡æœ‰æ¨ç†é“¾å†…å®¹ï¼Œæ£€æŸ¥ï¼š1) æ˜¯å¦ä½¿ç”¨ deepseek-reasoner æ¨¡å‹ 2) API æ˜¯å¦è¿”å›äº†æ¨ç†é“¾")
                
                # ä¿å­˜åˆ°æ¶ˆæ¯å†å²
                if full_answer:
                    save_message_to_history(full_answer, local_sources, reasoning_content)
                
                # æ˜¾ç¤ºå¸¦å¼•ç”¨çš„æ ¼å¼åŒ–å†…å®¹ï¼ˆå¦‚æœæœ‰æ¥æºï¼‰
                from frontend.utils.helpers import generate_message_id
                msg_idx = len(st.session_state.messages)
                message_id = generate_message_id(msg_idx, full_answer)
                
                if local_sources:
                    formatted_content = format_answer_with_citation_links(
                        full_answer,
                        local_sources,
                        message_id=message_id
                    )
                    message_placeholder.markdown(formatted_content, unsafe_allow_html=True)
                
                # æ˜¾ç¤ºæ¨ç†é“¾å’Œå¼•ç”¨æ¥æº
                display_reasoning(reasoning_content)
                display_sources(local_sources, message_id)
                
                # ä¿å­˜åˆ°ChatManagerä¼šè¯
                save_to_chat_manager(chat_manager, prompt, full_answer, local_sources, reasoning_content)
                
            except Exception as e:
                import traceback
                st.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
                st.error(traceback.format_exc())
            finally:
                st.session_state.is_thinking = False


def _run_async_stream(coro) -> None:
    """è¿è¡Œå¼‚æ­¥æµå¼å¤„ç†ï¼ˆå¤„ç†äº‹ä»¶å¾ªç¯å†²çªï¼‰
    
    Args:
        coro: åç¨‹å‡½æ•°
    """
    try:
        # å°è¯•ä½¿ç”¨ nest_asyncioï¼ˆå¦‚æœå·²å®‰è£…ï¼‰
        import nest_asyncio
        nest_asyncio.apply()
        # ä½¿ç”¨å½“å‰äº‹ä»¶å¾ªç¯
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # å¦‚æœäº‹ä»¶å¾ªç¯æ­£åœ¨è¿è¡Œï¼Œåˆ›å»ºä»»åŠ¡
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, coro())
                future.result()
        else:
            asyncio.run(coro())
    except ImportError:
        # å¦‚æœæ²¡æœ‰ nest_asyncioï¼Œç›´æ¥è¿è¡Œ
        asyncio.run(coro())
    except RuntimeError:
        # å¦‚æœäº‹ä»¶å¾ªç¯å·²å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            loop.run_until_complete(coro())
        finally:
            loop.close()

