#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆé…ç½®æ£€æŸ¥å·¥å…·ï¼ˆä¸ä¾èµ–å…¶ä»–æ¨¡å—ï¼‰
"""

import os
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

print("\n" + "=" * 60)
print("ğŸ” HuggingFace é…ç½®æ£€æŸ¥")
print("=" * 60)

# è¯»å–é…ç½®
hf_endpoint = os.getenv("HF_ENDPOINT", "https://hf-mirror.com")
hf_offline = os.getenv("HF_OFFLINE_MODE", "false").lower() == "true"
embedding_model = os.getenv("EMBEDDING_MODEL", "BAAI/bge-base-zh-v1.5")

print("\nğŸ“‹ ç¯å¢ƒé…ç½®:")
print(f"   EMBEDDING_MODEL: {embedding_model}")
print(f"   HF_ENDPOINT: {hf_endpoint}")
print(f"   HF_OFFLINE_MODE: {hf_offline}")

# æ£€æŸ¥ç¼“å­˜
cache_root = Path.home() / ".cache" / "huggingface" / "hub"
model_cache_name = embedding_model.replace("/", "--")
cache_dir = cache_root / f"models--{model_cache_name}"

print(f"\nğŸ’¾ ç¼“å­˜çŠ¶æ€:")
print(f"   ç¼“å­˜ç›®å½•: {cache_dir}")
if cache_dir.exists():
    print(f"   çŠ¶æ€: âœ… å·²å­˜åœ¨")
    print(f"   æç¤º: åç»­ä½¿ç”¨æ— éœ€è”ç½‘")
else:
    print(f"   çŠ¶æ€: âš ï¸  ä¸å­˜åœ¨")
    print(f"   æç¤º: é¦–æ¬¡ä½¿ç”¨å°†ä»é•œåƒä¸‹è½½ï¼ˆçº¦400MBï¼‰")

print(f"\nğŸŒ ç½‘ç»œé…ç½®:")
if hf_offline:
    print(f"   æ¨¡å¼: ğŸ“´ ç¦»çº¿æ¨¡å¼")
    print(f"   è¡Œä¸º: ä»…ä½¿ç”¨æœ¬åœ°ç¼“å­˜")
else:
    print(f"   æ¨¡å¼: ğŸŒ åœ¨çº¿æ¨¡å¼")
    print(f"   é•œåƒ: {hf_endpoint}")

print("\n" + "=" * 60)
print("âœ… é…ç½®æ£€æŸ¥å®Œæˆ")
print("=" * 60)

print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
if not cache_dir.exists():
    print("   1. ç¡®ä¿ç½‘ç»œç•…é€š")
    print("   2. å¯åŠ¨åº”ç”¨: streamlit run app.py")
    print("   3. é¦–æ¬¡å¯åŠ¨ä¼šè‡ªåŠ¨ä»é•œåƒä¸‹è½½æ¨¡å‹")
    print("   4. ä¸‹è½½å®Œæˆåå°†ç¼“å­˜åˆ°æœ¬åœ°")
else:
    print("   1. æ¨¡å‹å·²ç¼“å­˜ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨")
    print("   2. å¯åŠ¨åº”ç”¨: streamlit run app.py")
    print("   3. åŠ è½½é€Ÿåº¦ï¼šç§’çº§å¯åŠ¨ âš¡")

print()

