# DeepSeek API配置
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_API_BASE=https://api.deepseek.com/v1

# 模型配置
LLM_MODEL=deepseek-chat
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B

# HuggingFace模型镜像配置（解决国内网络访问慢的问题）
# 默认使用 hf-mirror.com 国内镜像加速下载
# 可选值：
#   - https://hf-mirror.com (默认，HF-Mirror镜像，推荐)
#   - https://www.modelscope.cn/models (ModelScope镜像)
#   - 留空则使用官方地址 huggingface.co（较慢）
HF_ENDPOINT=https://hf-mirror.com

# HuggingFace离线模式（强制使用本地缓存，不联网）
# true: 离线模式，仅使用本地缓存（如无缓存会自动切换在线模式并警告）
# false: 在线模式，优先使用缓存，必要时联网检查更新
HF_OFFLINE_MODE=false

# 向量数据库配置
VECTOR_STORE_PATH=./vector_store
# 默认集合名称（未登录时使用，登录后使用用户专属集合）
CHROMA_COLLECTION_NAME=default

# 文档配置
RAW_DATA_PATH=./data/raw
PROCESSED_DATA_PATH=./data/processed

# 索引配置
CHUNK_SIZE=512
CHUNK_OVERLAP=50
SIMILARITY_TOP_K=3

# Embedding性能优化配置
# 批处理大小：一次处理的文本数量，越大速度越快但占用更多内存
# 建议值：CPU环境=5-10, GPU环境=10-50（取决于GPU显存）
EMBED_BATCH_SIZE=10

# 最大文本长度：超过此长度的文本会被截断（避免OOM）
# 建议值：512-1024（根据模型和硬件调整）
EMBED_MAX_LENGTH=512

# 检索质量控制
# 相似度阈值（0-1），低于此值会提示检索质量较低，答案更多依赖模型推理
SIMILARITY_THRESHOLD=0.5

# 应用配置
APP_TITLE=系统科学知识库RAG
APP_PORT=8501

# 开发模式（支持热加载 src 模块）
# true: 开发模式，修改 src/*.py 文件后点击 Rerun 即可生效
# false: 生产模式，模块只加载一次
DEV_MODE=true

# GitHub数据源配置（仅支持公开仓库）
GITHUB_DEFAULT_BRANCH=main

# ============================================
# 测试配置（GitHub端到端测试）
# ============================================
# GitHub端到端测试使用的仓库配置
# 默认使用 octocat/Hello-World（GitHub官方示例仓库）
# 可以通过环境变量或 .env 文件覆盖
TEST_GITHUB_OWNER=qiao-925
TEST_GITHUB_REPO=Creating-Systematology
TEST_GITHUB_BRANCH=main

# ============================================
# 缓存配置（GitHub导入和向量化流程优化）
# ============================================
# 是否启用步骤级缓存（true/false）
# true: 启用缓存，检查并复用已完成的步骤（仓库克隆、文档解析、向量化）
#       - 已克隆的仓库不会重新克隆（commit SHA匹配时）
#       - 已解析的文档会复用解析结果
#       - 已向量化的内容会跳过重复计算
# false: 禁用缓存，强制重新执行所有步骤（适合测试环境需要重置时）
# 默认值: true（推荐在开发/测试时启用以加快迭代速度）
ENABLE_CACHE=true

# 缓存状态文件路径（相对路径相对于项目根目录）
# 存储各步骤的完成状态和元数据，用于断点续传
CACHE_STATE_PATH=./data/cache_state.json

# 维基百科配置
# 是否启用维基百科知识增强（true/false）
ENABLE_WIKIPEDIA=true

# 是否根据查询语言自动选择维基百科语言（true/false）
# true: 自动检测（中文查询→中文维基，英文查询→英文维基）
# false: 始终使用预设语言
WIKIPEDIA_AUTO_LANG=true

# 触发维基百科查询的相关度阈值（0-1）
# 当本地检索结果的最高相关度低于此阈值时，会触发维基百科补充查询
WIKIPEDIA_THRESHOLD=0.6

# 维基百科最多返回结果数（建议1-3）
WIKIPEDIA_MAX_RESULTS=2

# 预索引的核心概念列表（逗号分隔）
# 构建索引时会预先加载这些维基百科页面
# 注意：页面标题需要与维基百科页面完全匹配
WIKIPEDIA_PRELOAD_CONCEPTS=系统科学,钱学森,系统工程,控制论,信息论,复杂系统

# ============================================
# 调试与可观测性配置
# ============================================
# 注意：以下功能通过 Web 界面控制，无需环境变量配置
# 
# 1. Phoenix 可视化平台（Web界面"🔍 调试模式"→"📊 Phoenix可视化平台"）
#    - 实时追踪 RAG 查询流程
#    - 向量空间可视化
#    - 性能分析和统计
#    - 访问地址：http://localhost:6006
#
# 2. LlamaDebugHandler 调试（Web界面"🔍 调试模式"→"🐛 LlamaDebugHandler调试"）
#    - 输出详细的执行日志到控制台
#    - 显示 LLM 调用的完整 prompt 和响应
#
# 3. 查询追踪信息（Web界面"🔍 调试模式"→"📈 查询追踪信息"）
#    - 收集各环节耗时、相似度统计等指标
#    - Web 界面实时显示