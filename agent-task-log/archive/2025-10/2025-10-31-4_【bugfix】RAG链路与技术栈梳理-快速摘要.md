# 2025-10-31 【bugfix】RAG 链路与技术栈梳理（快速摘要）

**【Task Type】**: bugfix
- 日期：2025-10-31
- 目的：系统化梳理当前端到端 RAG 流程与技术栈，为后续链路优化定界
- 参考：`docs/ARCHITECTURE.md`、`src/data_loader.py`、`src/indexer.py`、`src/query_engine.py`、`src/chat_manager.py`、`src/config.py`

## 一、端到端流程（E2E）
1) 数据加载（DataLoader）
   - 本地 Markdown、网页（SimpleWebPageReader）、GitHub（GitLoader + 本地克隆）
   - 文本清理与元数据增强（标题提取、路径、source_type 等）
2) 索引构建（IndexManager）
   - SentenceSplitter 分块 → HuggingFaceEmbedding → Chroma 持久化
   - 支持批处理、断点续传、维度校验与自动纠错
3) 查询引擎（QueryEngine）
   - CitationQueryEngine：向量检索 → 上下文构建 → DeepSeek 生成 → 引用溯源
   - 低相关/无来源/空答案 → 方案A兜底：纯LLM定义类回答（新增）
4) 对话管理（ChatManager）
   - CondensePlusContextChatEngine：上下文凝练 + 检索 + 生成 + 记忆
   - 会话持久化：JSON 文件，按用户隔离
5) UI/CLI
   - Streamlit 多页面（app.py + pages）/ CLI（main.py）
6) 可观测性与调试
   - LlamaDebugHandler 打印追踪；Phoenix（可视化追踪，Arize）

## 二、关键实现引用（便于定位）
- DeepSeek 初始化与引用引擎：
```69:88:src/query_engine.py
print(f"🤖 初始化DeepSeek LLM: {self.model}")
self.llm = DeepSeek(...)
self.query_engine = CitationQueryEngine.from_args(
    self.index,
    llm=self.llm,
    similarity_top_k=self.similarity_top_k,
    citation_chunk_size=self.citation_chunk_size,
)
```
- 检索→提取答案与引用：
```126:145:src/query_engine.py
response: Response = self.query_engine.query(question)
answer = str(response)
sources = []
if hasattr(response, 'source_nodes') and response.source_nodes:
    ...  # 提取 text/score/metadata
```
- 质量评估 + 方案A兜底（新增）：
```146:181:src/query_engine.py
high_quality_sources = ...
max_score = ...
...  # 统计 min/avg/max、日志输出
if fallback_reason:
    # 纯LLM定义类回答 + 双重兜底占位
    llm_resp = self.llm.complete(fallback_prompt)
    answer = new_answer or 最小可用占位文本
```
- 分块与向量化到 Chroma：
```838:846:src/indexer.py
self._index = VectorStoreIndex.from_documents(
    documents,
    storage_context=self.storage_context,
    show_progress=show_progress,
)
```

## 三、技术栈（RAG相关）
- RAG框架：LlamaIndex（QueryEngine、ChatEngine、VectorStoreIndex、Readers）
- 向量存储：Chroma + PersistentClient（`vector_store/` 持久化）
- Embedding：HuggingFaceEmbedding（默认 `Qwen/Qwen3-Embedding-0.6B`；镜像/离线支持）
- LLM：DeepSeek API（OpenAI 兼容接口）
- 检索：相似度检索（top_k 默认 3），可扩展 Reranker（未启用）
- 对话：CondensePlusContextChatEngine（记忆 + 历史压缩）
- 可观测性：LlamaDebugHandler（控制台）、Phoenix（Web可视化，OpenTelemetry）
- 前端：Streamlit 多页面；命令行：main.py

## 四、关键参数（来自 `src/config.py`）
- `SIMILARITY_TOP_K`：默认 3（检索候选）
- `SIMILARITY_THRESHOLD`：默认 0.5（检索质量判定/兜底触发）
- 分块：`CHUNK_SIZE=512`、`CHUNK_OVERLAP=50`
- Embedding：`EMBED_BATCH_SIZE`、`EMBED_MAX_LENGTH`
- Wikipedia：`ENABLE_WIKIPEDIA`、`WIKIPEDIA_THRESHOLD=0.6`（Hybrid 引擎已有实现）

## 五、当前链路的显著特点
- 优点：
  - 模块化清晰，替换/扩展点充分（DB、LLM、Reader、分块策略均易替换）
  - 可观测性到位（日志 + Phoenix + trace 字段），便于学习与诊断
  - 索引构建具备工程弹性（批处理、断点续传、维度校验/修复）
- 局限：
  - 检索阶段尚未接入 Reranker/Hybrid Search（仅向量召回）
  - Query Rewrite/关键词扩展未启用，易受用户表述影响
  - 生成阶段未做结构化输出/格式校验（可通过 OutputParser/JSONSchema）

## 六、优化边界（为下一步方案做准备）
- 检索增强：Reranker（BGE/RocketQAv2）、Hybrid（BM25 + 向量）、自适应 top_k
- Query 侧：重写/扩展、自动语言检测、更强的关键词抽取（已在 Hybrid 中局部使用）
- 生成侧：结构化输出约束、引用一致性校验、长度与事实性控制
- 兜底策略：与 Hybrid 联动；基于类型分类（定义/过程/比较/因果）选模板
- 指标与观测：贴合任务的质量指标（命中率、引用覆盖率、忠实度、回答可读性）

> 本摘要用于“系统性优化”的定界参考；如确认，我将输出一份分阶段的优化方案（依赖关系拆解、影响评估、测试要点）。
