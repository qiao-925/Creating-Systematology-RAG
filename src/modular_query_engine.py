"""
æ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“
æ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥å’Œåå¤„ç†æ¨¡å—çš„çµæ´»ç»„åˆ
"""

import time
from typing import List, Optional, Tuple, Dict, Any
from llama_index.core import VectorStoreIndex, Settings
from llama_index.core.retrievers import (
    VectorIndexRetriever,
    QueryFusionRetriever,
)
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import (
    SimilarityPostprocessor,
    SentenceTransformerRerank,
)
from llama_index.llms.deepseek import DeepSeek

from src.config import config
from src.indexer import IndexManager
from src.logger import setup_logger
from src.response_formatter import ResponseFormatter
from src.observers.manager import ObserverManager
from src.observers.factory import create_observer_from_config

logger = setup_logger('modular_query_engine')


class ModularQueryEngine:
    """æ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“ï¼ˆå·¥å‚æ¨¡å¼ï¼‰"""
    
    # æ”¯æŒçš„æ£€ç´¢ç­–ç•¥
    SUPPORTED_STRATEGIES = ["vector", "bm25", "hybrid"]
    
    def __init__(
        self,
        index_manager: IndexManager,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        retrieval_strategy: Optional[str] = None,
        similarity_top_k: Optional[int] = None,
        enable_rerank: Optional[bool] = None,
        rerank_top_n: Optional[int] = None,
        similarity_cutoff: Optional[float] = None,
        enable_markdown_formatting: bool = True,
        observer_manager: Optional[ObserverManager] = None,  # æ–°å¢ï¼šè§‚å¯Ÿå™¨ç®¡ç†å™¨
        **kwargs
    ):
        """åˆå§‹åŒ–æ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“
        
        Args:
            index_manager: ç´¢å¼•ç®¡ç†å™¨
            api_key: DeepSeek APIå¯†é’¥
            model: æ¨¡å‹åç§°
            retrieval_strategy: æ£€ç´¢ç­–ç•¥ ("vector"|"bm25"|"hybrid")
            similarity_top_k: æ£€ç´¢æ–‡æ¡£æ•°é‡
            enable_rerank: æ˜¯å¦å¯ç”¨é‡æ’åº
            rerank_top_n: é‡æ’åºä¿ç•™æ–‡æ¡£æ•°
            similarity_cutoff: ç›¸ä¼¼åº¦è¿‡æ»¤é˜ˆå€¼
            enable_markdown_formatting: æ˜¯å¦å¯ç”¨Markdownæ ¼å¼åŒ–
        """
        self.index_manager = index_manager
        self.index = index_manager.get_index()
        
        # é…ç½®å‚æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
        self.retrieval_strategy = retrieval_strategy or config.RETRIEVAL_STRATEGY
        self.similarity_top_k = similarity_top_k or config.SIMILARITY_TOP_K
        self.enable_rerank = enable_rerank if enable_rerank is not None else config.ENABLE_RERANK
        self.rerank_top_n = rerank_top_n or config.RERANK_TOP_N
        self.similarity_cutoff = similarity_cutoff or config.SIMILARITY_CUTOFF
        
        # éªŒè¯ç­–ç•¥
        if self.retrieval_strategy not in self.SUPPORTED_STRATEGIES:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ£€ç´¢ç­–ç•¥: {self.retrieval_strategy}. "
                f"æ”¯æŒçš„ç­–ç•¥: {self.SUPPORTED_STRATEGIES}"
            )
        
        # åˆå§‹åŒ–å“åº”æ ¼å¼åŒ–å™¨
        self.formatter = ResponseFormatter(enable_formatting=enable_markdown_formatting)
        
        # åˆå§‹åŒ–è§‚å¯Ÿå™¨ç®¡ç†å™¨
        if observer_manager is not None:
            self.observer_manager = observer_manager
            logger.info(f"âœ… ä½¿ç”¨æä¾›çš„è§‚å¯Ÿå™¨ç®¡ç†å™¨: {len(observer_manager.observers)} ä¸ªè§‚å¯Ÿå™¨")
        else:
            # ä»é…ç½®åˆ›å»ºè§‚å¯Ÿå™¨
            self.observer_manager = create_observer_from_config()
            logger.info(f"âœ… ä»é…ç½®åˆ›å»ºè§‚å¯Ÿå™¨ç®¡ç†å™¨: {len(self.observer_manager.observers)} ä¸ªè§‚å¯Ÿå™¨")
        
        # è·å–æ‰€æœ‰å›è°ƒå¤„ç†å™¨
        callback_handlers = self.observer_manager.get_callback_handlers()
        if callback_handlers:
            from llama_index.core.callbacks import CallbackManager
            Settings.callback_manager = CallbackManager(callback_handlers)
            logger.info(f"âœ… è®¾ç½® {len(callback_handlers)} ä¸ªå›è°ƒå¤„ç†å™¨åˆ° LlamaIndex")
        
        # é…ç½® LLM
        self.api_key = api_key or config.DEEPSEEK_API_KEY
        self.model = model or config.LLM_MODEL
        if not self.api_key:
            raise ValueError("æœªè®¾ç½®DEEPSEEK_API_KEY")
        
        self.llm = DeepSeek(
            api_key=self.api_key,
            model=self.model,
            temperature=0.5,
            max_tokens=4096,
        )
        
        # åˆ›å»ºæ£€ç´¢å™¨
        self.retriever = self._create_retriever()
        
        # åˆ›å»ºåå¤„ç†å™¨
        self.postprocessors = self._create_postprocessors()
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        self.query_engine = RetrieverQueryEngine.from_args(
            retriever=self.retriever,
            llm=self.llm,
            node_postprocessors=self.postprocessors,
        )
        
        logger.info(
            f"æ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“åˆå§‹åŒ–å®Œæˆ: "
            f"ç­–ç•¥={self.retrieval_strategy}, "
            f"top_k={self.similarity_top_k}, "
            f"é‡æ’åº={self.enable_rerank}, "
            f"ç›¸ä¼¼åº¦é˜ˆå€¼={self.similarity_cutoff}"
        )
        print(f"âœ… æ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        print(f"   æ£€ç´¢ç­–ç•¥: {self.retrieval_strategy}")
        print(f"   Top-K: {self.similarity_top_k}")
        print(f"   é‡æ’åº: {'å¯ç”¨' if self.enable_rerank else 'ç¦ç”¨'}")
        print(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_cutoff}")
    
    def _create_retriever(self):
        """åˆ›å»ºæ£€ç´¢å™¨ï¼ˆæ ¹æ®ç­–ç•¥ï¼‰"""
        if self.retrieval_strategy == "vector":
            logger.info("åˆ›å»ºå‘é‡æ£€ç´¢å™¨")
            return VectorIndexRetriever(
                index=self.index,
                similarity_top_k=self.similarity_top_k,
            )
        
        elif self.retrieval_strategy == "bm25":
            logger.info("åˆ›å»ºBM25æ£€ç´¢å™¨")
            try:
                from llama_index.retrievers.bm25 import BM25Retriever
            except ImportError:
                logger.error("BM25Retrieveræœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install llama-index-retrievers-bm25")
                raise ImportError(
                    "BM25Retrieveræœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install llama-index-retrievers-bm25"
                )
            
            # ä»ç´¢å¼•ä¸­è·å–æ‰€æœ‰èŠ‚ç‚¹
            nodes = list(self.index.docstore.docs.values())
            
            return BM25Retriever.from_defaults(
                nodes=nodes,
                similarity_top_k=self.similarity_top_k,
            )
        
        elif self.retrieval_strategy == "hybrid":
            logger.info("åˆ›å»ºæ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡+BM25ï¼‰")
            try:
                from llama_index.retrievers.bm25 import BM25Retriever
            except ImportError:
                logger.warning("BM25Retrieveræœªå®‰è£…ï¼Œé™çº§ä¸ºçº¯å‘é‡æ£€ç´¢")
                print("âš ï¸  BM25æœªå®‰è£…ï¼Œé™çº§ä¸ºå‘é‡æ£€ç´¢")
                return VectorIndexRetriever(
                    index=self.index,
                    similarity_top_k=self.similarity_top_k,
                )
            
            # å‘é‡æ£€ç´¢å™¨
            vector_retriever = VectorIndexRetriever(
                index=self.index,
                similarity_top_k=self.similarity_top_k,
            )
            
            # BM25æ£€ç´¢å™¨
            nodes = list(self.index.docstore.docs.values())
            bm25_retriever = BM25Retriever.from_defaults(
                nodes=nodes,
                similarity_top_k=self.similarity_top_k,
            )
            
            # èåˆæ£€ç´¢å™¨
            return QueryFusionRetriever(
                retrievers=[vector_retriever, bm25_retriever],
                similarity_top_k=self.similarity_top_k,
                num_queries=1,  # ä¸ç”Ÿæˆé¢å¤–æŸ¥è¯¢
                mode="reciprocal_rerank",  # å€’æ•°æ’åèåˆ
                use_async=False,
            )
    
    def _create_postprocessors(self) -> List:
        """åˆ›å»ºåå¤„ç†å™¨ï¼ˆé“¾å¼ç»„åˆï¼‰"""
        postprocessors = []
        
        # 1. ç›¸ä¼¼åº¦è¿‡æ»¤ï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰
        postprocessors.append(
            SimilarityPostprocessor(similarity_cutoff=self.similarity_cutoff)
        )
        logger.info(f"æ·»åŠ ç›¸ä¼¼åº¦è¿‡æ»¤å™¨: cutoff={self.similarity_cutoff}")
        
        # 2. é‡æ’åºï¼ˆå¯é€‰ï¼‰
        if self.enable_rerank:
            try:
                # å°è¯•ä½¿ç”¨ç»Ÿä¸€çš„Embeddingå®ä¾‹
                embedding_instance = self.index_manager.get_embedding_instance()
                
                if embedding_instance is not None:
                    # ä¼˜å…ˆä½¿ç”¨ç»Ÿä¸€çš„Embeddingå®ä¾‹
                    logger.info(f"é‡æ’åºä½¿ç”¨ç»Ÿä¸€Embeddingå®ä¾‹: {embedding_instance.get_model_name()}")
                    
                    # è·å–LlamaIndexå…¼å®¹çš„æ¨¡å‹
                    if hasattr(embedding_instance, 'get_llama_index_embedding'):
                        rerank_embedding = embedding_instance.get_llama_index_embedding()
                        # ä½¿ç”¨åº•å±‚æ¨¡å‹åç§°
                        rerank_model = rerank_embedding.model_name
                    else:
                        # ç›´æ¥ä½¿ç”¨ï¼ˆå‡è®¾å·²å…¼å®¹ï¼‰
                        rerank_model = config.RERANK_MODEL or config.EMBEDDING_MODEL
                else:
                    # é™çº§ï¼šä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹åç§°
                    rerank_model = config.RERANK_MODEL or config.EMBEDDING_MODEL
                    logger.info(f"é‡æ’åºä½¿ç”¨é…ç½®æ¨¡å‹: {rerank_model}")
                
                postprocessors.append(
                    SentenceTransformerRerank(
                        model=rerank_model,
                        top_n=self.rerank_top_n,
                    )
                )
                logger.info(f"æ·»åŠ é‡æ’åºæ¨¡å—: model={rerank_model}, top_n={self.rerank_top_n}")
            except Exception as e:
                logger.warning(f"é‡æ’åºæ¨¡å—åˆå§‹åŒ–å¤±è´¥ï¼Œè·³è¿‡: {e}")
                print(f"âš ï¸  é‡æ’åºæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
        
        return postprocessors
    
    def query(
        self, 
        question: str, 
        collect_trace: bool = False
    ) -> Tuple[str, List[dict], Optional[Dict[str, Any]]]:
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå…¼å®¹ç°æœ‰APIï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            collect_trace: æ˜¯å¦æ”¶é›†è¿½è¸ªä¿¡æ¯
            
        Returns:
            (ç­”æ¡ˆæ–‡æœ¬, å¼•ç”¨æ¥æºåˆ—è¡¨, è¿½è¸ªä¿¡æ¯)
        """
        trace_info = None
        
        # é€šçŸ¥è§‚å¯Ÿå™¨ï¼šæŸ¥è¯¢å¼€å§‹
        trace_ids = self.observer_manager.on_query_start(question)
        
        try:
            logger.info(f"æ‰§è¡ŒæŸ¥è¯¢: {question}")
            print(f"\nğŸ’¬ æŸ¥è¯¢: {question}")
            
            if collect_trace:
                trace_info = {
                    "query": question,
                    "strategy": self.retrieval_strategy,
                    "start_time": time.time(),
                    "observer_trace_ids": trace_ids,  # è®°å½•è§‚å¯Ÿå™¨è¿½è¸ªID
                }
            
            # æ‰§è¡ŒæŸ¥è¯¢
            retrieval_start = time.time()
            response = self.query_engine.query(question)
            retrieval_time = time.time() - retrieval_start
            
            # æå–ç­”æ¡ˆ
            answer = str(response)
            answer = self.formatter.format(answer, None)
            
            # æå–å¼•ç”¨æ¥æº
            sources = []
            if hasattr(response, 'source_nodes') and response.source_nodes:
                logger.info(f"æ£€ç´¢åˆ° {len(response.source_nodes)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                print(f"ğŸ” æ£€ç´¢åˆ° {len(response.source_nodes)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                
                for i, node in enumerate(response.source_nodes, 1):
                    try:
                        metadata = node.node.metadata if hasattr(node, 'node') and hasattr(node.node, 'metadata') else {}
                        if not isinstance(metadata, dict):
                            metadata = {}
                    except Exception:
                        metadata = {}
                    
                    score = node.score if hasattr(node, 'score') else None
                    
                    source = {
                        'index': i,
                        'text': node.node.text if hasattr(node, 'node') else '',
                        'score': score,
                        'metadata': metadata,
                    }
                    sources.append(source)
                    
                    # æ‰“å°ç®€è¦ä¿¡æ¯
                    score_str = f"{score:.4f}" if score is not None else "N/A"
                    file_name = metadata.get('file_name', metadata.get('file_path', 'æœªçŸ¥').split('/')[-1])
                    print(f"  [{i}] {file_name} (åˆ†æ•°: {score_str})")
            
            # è¿½è¸ªä¿¡æ¯
            if collect_trace and trace_info:
                trace_info["retrieval_time"] = round(retrieval_time, 2)
                trace_info["chunks_retrieved"] = len(sources)
                trace_info["total_time"] = round(time.time() - trace_info["start_time"], 2)
            
            print(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(sources)} ä¸ªå¼•ç”¨æ¥æº")
            
            # é€šçŸ¥è§‚å¯Ÿå™¨ï¼šæŸ¥è¯¢ç»“æŸ
            self.observer_manager.on_query_end(
                query=question,
                answer=answer,
                sources=sources,
                trace_ids=trace_ids,
                retrieval_time=retrieval_time if 'retrieval_time' in locals() else None,
            )
            
            return answer, sources, trace_info
            
        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    async def stream_query(self, question: str):
        """å¼‚æ­¥æµå¼æŸ¥è¯¢ï¼ˆç”¨äºWebåº”ç”¨ï¼‰"""
        # TODO: å®ç°æµå¼æŸ¥è¯¢
        raise NotImplementedError("æµå¼æŸ¥è¯¢æš‚æœªå®ç°")


def create_modular_query_engine(
    index_manager: IndexManager,
    strategy: Optional[str] = None,
    **kwargs
) -> ModularQueryEngine:
    """åˆ›å»ºæ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        index_manager: ç´¢å¼•ç®¡ç†å™¨
        strategy: æ£€ç´¢ç­–ç•¥ ("vector"|"bm25"|"hybrid")
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ModularQueryEngineå®ä¾‹
    """
    return ModularQueryEngine(
        index_manager=index_manager,
        retrieval_strategy=strategy,
        **kwargs
    )

