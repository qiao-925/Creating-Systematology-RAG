"""
æ•°æ®åŠ è½½å™¨æ¨¡å—
ä½¿ç”¨ LlamaIndex å®˜æ–¹ Reader ç»„ä»¶ï¼Œæ”¯æŒä»å¤šç§æ•°æ®æºåŠ è½½æ–‡æ¡£

æ¶æ„ï¼š
- æ•°æ®æ¥æºå±‚ï¼ˆdata_source/ï¼‰ï¼šä»ä¸åŒæ•°æ®æºè·å–æ–‡ä»¶è·¯å¾„
- æ•°æ®è§£æå±‚ï¼ˆdata_parser/ï¼‰ï¼šç»Ÿä¸€ä½¿ç”¨ SimpleDirectoryReader è§£ææ–‡ä»¶
- å…¼å®¹å±‚ï¼ˆæœ¬æ¨¡å—ï¼‰ï¼šæä¾›å‘åå…¼å®¹çš„å‡½æ•°æ¥å£
"""

import os
import re
from pathlib import Path
from typing import List, Optional

from tqdm import tqdm
from llama_index.core import SimpleDirectoryReader
from llama_index.core.schema import Document as LlamaDocument

try:
    from llama_index.readers.web import SimpleWebPageReader
except ImportError:
    SimpleWebPageReader = None

try:
    from langchain_community.document_loaders import GitLoader
except ImportError:
    GitLoader = None

try:
    from src.git_repository_manager import GitRepositoryManager
except ImportError:
    GitRepositoryManager = None

# æ–°æ¶æ„å¯¼å…¥
from src.data_source import DataSource, GitHubSource, LocalFileSource, WebSource
from src.data_parser import DocumentParser

from src.logger import setup_logger
from src.config import config

# åˆ›å»ºæ—¥å¿—å™¨
logger = setup_logger('data_loader')


def safe_print(text: str):
    """å®‰å…¨æ‰“å°ï¼ˆå¤„ç†ç¼–ç é—®é¢˜ï¼‰"""
    try:
        print(text)
    except UnicodeEncodeError:
        # åœ¨ Windows GBK ç¼–ç ä¸‹ï¼Œå›é€€åˆ° ASCII å‹å¥½çš„è¾“å‡º
        text = text.encode('ascii', 'replace').decode('ascii')
        print(text)


class DocumentProcessor:
    """æ–‡æ¡£é¢„å¤„ç†å™¨"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'[ \t]+', ' ', text)
        
        # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = [line.strip() for line in text.split('\n')]
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™æœ€å¤šä¸€ä¸ªç©ºè¡Œï¼Œå³æœ€å¤šä¸¤ä¸ªè¿ç»­æ¢è¡Œï¼‰
        cleaned_lines = []
        prev_empty = False
        for line in lines:
            if line:  # éç©ºè¡Œ
                cleaned_lines.append(line)
                prev_empty = False
            else:  # ç©ºè¡Œ
                if not prev_empty:  # åªä¿ç•™ç¬¬ä¸€ä¸ªç©ºè¡Œ
                    cleaned_lines.append(line)
                    prev_empty = True
                # è¿ç»­çš„ç©ºè¡Œè·³è¿‡
        
        text = '\n'.join(cleaned_lines)
        return text.strip()
    
    @staticmethod
    def enrich_metadata(document: LlamaDocument, additional_metadata: dict) -> LlamaDocument:
        """ä¸ºæ–‡æ¡£æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
        
        Args:
            document: åŸå§‹æ–‡æ¡£
            additional_metadata: è¦æ·»åŠ çš„å…ƒæ•°æ®
            
        Returns:
            æ›´æ–°åçš„æ–‡æ¡£
        """
        document.metadata.update(additional_metadata)
        return document
    
    @staticmethod
    def filter_by_length(documents: List[LlamaDocument], 
                        min_length: int = 50) -> List[LlamaDocument]:
        """è¿‡æ»¤æ‰å¤ªçŸ­çš„æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            min_length: æœ€å°é•¿åº¦
            
        Returns:
            è¿‡æ»¤åçš„æ–‡æ¡£åˆ—è¡¨
        """
        filtered = [doc for doc in documents if len(doc.text) >= min_length]
        
        removed_count = len(documents) - len(filtered)
        if removed_count > 0:
            safe_print(f"âš ï¸  è¿‡æ»¤æ‰ {removed_count} ä¸ªè¿‡çŸ­çš„æ–‡æ¡£")
        
        return filtered
    
    @staticmethod
    def extract_title_from_markdown(content: str) -> Optional[str]:
        """ä»Markdownå†…å®¹ä¸­æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜ï¼‰
        
        Args:
            content: Markdown æ–‡æœ¬å†…å®¹
            
        Returns:
            æå–çš„æ ‡é¢˜ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
        """
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('# '):
                return line[2:].strip()
        return None


def load_documents_from_source(
    source: DataSource,
    clean: bool = True,
    show_progress: bool = True,
    cache_manager=None,
    task_id: Optional[str] = None
) -> List[LlamaDocument]:
    """ä»æ•°æ®æºåŠ è½½æ–‡æ¡£ï¼ˆç»Ÿä¸€å…¥å£å‡½æ•°ï¼‰
    
    æ–°æ¶æ„çš„ç»Ÿä¸€å…¥å£ï¼Œæ•´åˆæ•°æ®æ¥æºå±‚å’Œè§£æå±‚
    
    Args:
        source: æ•°æ®æºå¯¹è±¡ï¼ˆGitHubSource, LocalFileSource, WebSourceç­‰ï¼‰
        clean: æ˜¯å¦æ¸…ç†æ–‡æœ¬
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        cache_manager: ç¼“å­˜ç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼Œç”¨äºç¼“å­˜ï¼‰
        
    Returns:
        æ–‡æ¡£åˆ—è¡¨
    """
    if not NEW_ARCHITECTURE_AVAILABLE:
        logger.error("æ–°æ¶æ„æœªå¯ç”¨")
        return []
    
    try:
        import time
        total_start_time = time.time()
        
        # æ­¥éª¤1: ä»æ•°æ®æºè·å–æ–‡ä»¶è·¯å¾„
        logger.info(f"å¼€å§‹ä»æ•°æ®æºè·å–æ–‡ä»¶è·¯å¾„ (source_type: {type(source).__name__})")
        if show_progress:
            safe_print(f"ğŸ“‚ æ­£åœ¨è·å–æ–‡ä»¶è·¯å¾„...")
        
        source_start_time = time.time()
        # å¦‚æœæ˜¯ GitHubSourceï¼Œä¼ é€’ç¼“å­˜ç®¡ç†å™¨
        if isinstance(source, GitHubSource):
            source_files = source.get_file_paths(cache_manager=cache_manager, task_id=task_id)
        else:
            source_files = source.get_file_paths()
        source_elapsed = time.time() - source_start_time
        
        if not source_files:
            logger.warning("æ•°æ®æºæœªè¿”å›ä»»ä½•æ–‡ä»¶")
            if show_progress:
                safe_print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
            return []
        
        logger.info(f"æ•°æ®æºè¿”å› {len(source_files)} ä¸ªæ–‡ä»¶ (è€—æ—¶: {source_elapsed:.2f}s)")
        if show_progress:
            safe_print(f"âœ… æ‰¾åˆ° {len(source_files)} ä¸ªæ–‡ä»¶")
        
        # æ­¥éª¤2: æ„å»ºæ–‡ä»¶è·¯å¾„åˆ—è¡¨å’Œå…ƒæ•°æ®æ˜ å°„
        logger.debug("æ„å»ºæ–‡ä»¶è·¯å¾„åˆ—è¡¨å’Œå…ƒæ•°æ®æ˜ å°„")
        file_paths = [sf.path for sf in source_files]
        # æ„å»ºå…ƒæ•°æ®æ˜ å°„ï¼Œç¡®ä¿åŒ…å« source_typeï¼ˆå®ƒæ˜¯ SourceFile çš„å±æ€§ï¼Œä¸æ˜¯ metadata çš„ä¸€éƒ¨åˆ†ï¼‰
        metadata_map = {}
        for sf in source_files:
            metadata_map[sf.path] = {
                **sf.metadata,
                'source_type': sf.source_type  # ç¡®ä¿ source_type åŒ…å«åœ¨å…ƒæ•°æ®ä¸­
            }
        logger.debug(f"å…ƒæ•°æ®æ˜ å°„åŒ…å« {len(metadata_map)} ä¸ªæ¡ç›®")
        
        # æ­¥éª¤3: ä½¿ç”¨è§£æå™¨è§£ææ–‡ä»¶
        if show_progress:
            safe_print(f"ğŸ“„ æ­£åœ¨è§£ææ–‡ä»¶...")
        
        parser_start_time = time.time()
        parser = DocumentParser()
        documents = parser.parse_files(
            file_paths, 
            metadata_map, 
            clean=clean,
            cache_manager=cache_manager,
            task_id=task_id
        )
        parser_elapsed = time.time() - parser_start_time
        
        if not documents:
            logger.warning(f"è§£æå™¨æœªè¿”å›ä»»ä½•æ–‡æ¡£ (è¾“å…¥æ–‡ä»¶æ•°: {len(file_paths)})")
            if show_progress:
                safe_print("âš ï¸  æœªèƒ½è§£æä»»ä½•æ–‡æ¡£")
            return []
        
        logger.info(f"è§£æå™¨è¿”å› {len(documents)} ä¸ªæ–‡æ¡£ (è€—æ—¶: {parser_elapsed:.2f}s)")
        
        # æ­¥éª¤4: å¯é€‰çš„æ–‡æœ¬æ¸…ç†
        clean_elapsed = 0.0
        if clean:
            logger.debug("å¼€å§‹æ–‡æœ¬æ¸…ç†")
            clean_start_time = time.time()
            processor = DocumentProcessor()
            cleaned_documents = []
            for doc in documents:
                cleaned_text = processor.clean_text(doc.text)
                cleaned_doc = LlamaDocument(
                    text=cleaned_text,
                    metadata=doc.metadata,
                    id_=doc.id_
                )
                cleaned_documents.append(cleaned_doc)
            documents = cleaned_documents
            clean_elapsed = time.time() - clean_start_time
            logger.debug(f"æ–‡æœ¬æ¸…ç†å®Œæˆ (è€—æ—¶: {clean_elapsed:.2f}s)")
        else:
            logger.debug("è·³è¿‡æ–‡æœ¬æ¸…ç†")
        
        total_elapsed = time.time() - total_start_time
        
        if show_progress:
            safe_print(f"âœ… æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        
        success_rate = (len(documents) / len(source_files) * 100) if source_files else 0
        logger.info(
            f"æ–‡æ¡£åŠ è½½å®Œæˆ: "
            f"æºæ–‡ä»¶æ•°={len(source_files)}, "
            f"è§£ææ–‡æ¡£æ•°={len(documents)}, "
            f"æˆåŠŸç‡={success_rate:.1f}%, "
            f"æ€»è€—æ—¶={total_elapsed:.2f}s "
            f"(è·å–è·¯å¾„={source_elapsed:.2f}s, "
            f"è§£æ={parser_elapsed:.2f}s, "
            f"æ¸…ç†={clean_elapsed:.2f}s)"
        )
        
        return documents
        
    except Exception as e:
        logger.error(f"ä»æ•°æ®æºåŠ è½½æ–‡æ¡£å¤±è´¥: {e}")
        if show_progress:
            safe_print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return []


def load_documents_from_directory(directory_path: str | Path, 
                                 recursive: bool = True,
                                 clean: bool = True,
                                 required_exts: Optional[List[str]] = None) -> List[LlamaDocument]:
    """ä»ç›®å½•åŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼ˆä½¿ç”¨å®˜æ–¹ SimpleDirectoryReaderï¼‰
    
    Args:
        directory_path: ç›®å½•è·¯å¾„
        recursive: æ˜¯å¦é€’å½’åŠ è½½
        clean: æ˜¯å¦æ¸…ç†æ–‡æœ¬
        required_exts: æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ˆé»˜è®¤ï¼š[".md", ".markdown"]ï¼‰
        
    Returns:
        Documentå¯¹è±¡åˆ—è¡¨
    """
    # ä½¿ç”¨æ–°æ¶æ„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if NEW_ARCHITECTURE_AVAILABLE:
        try:
            source = LocalFileSource(
                source=directory_path,
                recursive=recursive
            )
            documents = load_documents_from_source(source, clean=clean, show_progress=True)
            
            # åº”ç”¨æ‰©å±•åè¿‡æ»¤ï¼ˆå¦‚æœéœ€è¦ï¼‰
            if required_exts and documents:
                filtered_docs = []
                for doc in documents:
                    file_path = doc.metadata.get('file_path', '')
                    if any(file_path.endswith(ext) for ext in required_exts):
                        filtered_docs.append(doc)
                documents = filtered_docs
            
            # ä¸º Markdown æ–‡ä»¶æå–æ ‡é¢˜ï¼ˆä¿æŒåŸæœ‰è¡Œä¸ºï¼‰
            for doc in documents:
                file_name = doc.metadata.get('file_name', '')
                if any(file_name.endswith(ext) for ext in ['.md', '.markdown']):
                    title = DocumentProcessor.extract_title_from_markdown(doc.text)
                    if not title:
                        title = Path(file_name).stem if file_name else "æœªå‘½å"
                    doc.metadata.update({
                        "title": title,
                        "source_type": doc.metadata.get("source_type", "markdown"),
                    })
            
            return documents
        except Exception as e:
            logger.warning(f"æ–°æ¶æ„åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ—§å®ç°: {e}")
    
    # å›é€€åˆ°æ—§å®ç°
    directory_path = Path(directory_path)
    required_exts = required_exts or [".md", ".markdown"]
    
    # éªŒè¯ç›®å½•
    if not directory_path.exists() or not directory_path.is_dir():
        safe_print(f"âŒ ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®å½•: {directory_path}")
        logger.error(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        return []
    
    try:
        logger.info(f"å¼€å§‹åŠ è½½ç›®å½•: {directory_path}, é€’å½’: {recursive}")
        
        # ä½¿ç”¨ SimpleDirectoryReader åŠ è½½æ–‡æ¡£
        reader = SimpleDirectoryReader(
            input_dir=str(directory_path),
            recursive=recursive,
            required_exts=required_exts,
            filename_as_id=True,
            errors='ignore',  # å¿½ç•¥ä¸å¯è¯»æ–‡ä»¶
        )
        
        documents = reader.load_data()
        
        if not documents:
            safe_print(f"âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼ˆæ”¯æŒæ ¼å¼ï¼š{', '.join(required_exts)}ï¼‰")
            logger.warning(f"ç›®å½•ä¸ºç©º: {directory_path}")
            return []
        
        # å¢å¼ºå…ƒæ•°æ®
        for doc in documents:
            # æå–æ–‡ä»¶è·¯å¾„ä¿¡æ¯
            file_path = doc.metadata.get('file_path', '')
            file_name = doc.metadata.get('file_name', '')
            
            # ä¸º Markdown æ–‡ä»¶æå–æ ‡é¢˜
            if any(file_name.endswith(ext) for ext in ['.md', '.markdown']):
                title = DocumentProcessor.extract_title_from_markdown(doc.text)
                if not title:
                    title = Path(file_name).stem if file_name else "æœªå‘½å"
                
                doc.metadata.update({
                    "title": title,
                    "source_type": "markdown",
                })
            
            # ç¡®ä¿åŸºç¡€å…ƒæ•°æ®å­˜åœ¨
            if not doc.metadata.get('file_path'):
                doc.metadata['file_path'] = file_path
            if not doc.metadata.get('file_name'):
                doc.metadata['file_name'] = file_name
            
            # è¾“å‡ºåŠ è½½è¿›åº¦
            safe_print(f"âœ… å·²åŠ è½½: {file_name}")
        
        safe_print(f"\nğŸ“š æ€»å…±åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
        logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        
        # å¯é€‰çš„æ–‡æœ¬æ¸…ç†
        if clean:
            processor = DocumentProcessor()
            cleaned_documents = []
            for doc in documents:
                cleaned_text = processor.clean_text(doc.text)
                cleaned_doc = LlamaDocument(
                    text=cleaned_text,
                    metadata=doc.metadata,
                    id_=doc.id_
                )
                cleaned_documents.append(cleaned_doc)
            return cleaned_documents
        
        return documents
        
    except Exception as e:
        safe_print(f"âŒ åŠ è½½ç›®å½•å¤±è´¥: {e}")
        logger.error(f"åŠ è½½ç›®å½•å¤±è´¥ {directory_path}: {e}")
        return []


def load_documents_from_urls(urls: List[str], 
                            clean: bool = True) -> List[LlamaDocument]:
    """ä»URLåˆ—è¡¨åŠ è½½æ–‡æ¡£ï¼ˆä½¿ç”¨å®˜æ–¹ SimpleWebPageReaderï¼‰
    
    Args:
        urls: URLåˆ—è¡¨
        clean: æ˜¯å¦æ¸…ç†æ–‡æœ¬
        
    Returns:
        Documentå¯¹è±¡åˆ—è¡¨
    """
    if SimpleWebPageReader is None:
        safe_print("âŒ ç¼ºå°‘ä¾èµ–ï¼šllama-index-readers-web")
        safe_print("   å®‰è£…ï¼špip install llama-index-readers-web")
        logger.error("SimpleWebPageReader æœªå®‰è£…")
        return []
    
    # ä½¿ç”¨æ–°æ¶æ„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if NEW_ARCHITECTURE_AVAILABLE:
        try:
            source = WebSource(urls=urls)
            documents = load_documents_from_source(source, clean=clean, show_progress=True)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            source.cleanup()
            
            return documents
        except Exception as e:
            logger.warning(f"æ–°æ¶æ„åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ—§å®ç°: {e}")
    
    # å›é€€åˆ°æ—§å®ç°
    if not urls:
        safe_print("âš ï¸  URL åˆ—è¡¨ä¸ºç©º")
        return []
    
    try:
        logger.info(f"å¼€å§‹åŠ è½½ {len(urls)} ä¸ªç½‘é¡µ")
        
        # ä½¿ç”¨ SimpleWebPageReader åŠ è½½ç½‘é¡µ
        reader = SimpleWebPageReader(html_to_text=True)
        documents = reader.load_data(urls)
        
        if not documents:
            safe_print("âš ï¸  æœªæˆåŠŸåŠ è½½ä»»ä½•ç½‘é¡µ")
            logger.warning("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•ç½‘é¡µ")
            return []
        
        # å¢å¼ºå…ƒæ•°æ®
        for i, doc in enumerate(documents):
            url = urls[i] if i < len(urls) else "unknown"
            
            # ç¡®ä¿æœ‰ source_type
            doc.metadata.update({
                "source_type": "web",
                "url": url,
            })
            
            safe_print(f"âœ… å·²åŠ è½½: {url}")
        
        safe_print(f"\nğŸŒ æ€»å…±åŠ è½½äº† {len(documents)} ä¸ªç½‘é¡µ")
        logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªç½‘é¡µ")
        
        # å¯é€‰çš„æ–‡æœ¬æ¸…ç†
        if clean:
            processor = DocumentProcessor()
            cleaned_documents = []
            for doc in documents:
                cleaned_text = processor.clean_text(doc.text)
                cleaned_doc = LlamaDocument(
                    text=cleaned_text,
                    metadata=doc.metadata,
                    id_=doc.id_
                )
                cleaned_documents.append(cleaned_doc)
            return cleaned_documents
        
        return documents
        
    except Exception as e:
        safe_print(f"âŒ åŠ è½½ç½‘é¡µå¤±è´¥: {e}")
        logger.error(f"åŠ è½½ç½‘é¡µå¤±è´¥: {e}")
        return []


def _build_file_filter(
    filter_directories: Optional[List[str]] = None,
    filter_file_extensions: Optional[List[str]] = None
):
    """æ„å»ºæ–‡ä»¶è¿‡æ»¤å™¨å‡½æ•°
    
    å°†ç”¨æˆ·å‹å¥½çš„å‚æ•°æ ¼å¼è½¬æ¢ä¸º LangChain GitLoader éœ€è¦çš„ lambda å‡½æ•°
    
    Args:
        filter_directories: åªåŒ…å«æŒ‡å®šç›®å½•çš„æ–‡ä»¶ï¼ˆä¾‹å¦‚: ["docs", "examples"]ï¼‰
        filter_file_extensions: åªåŒ…å«æŒ‡å®šæ‰©å±•åçš„æ–‡ä»¶ï¼ˆä¾‹å¦‚: [".md", ".py"]ï¼‰
        
    Returns:
        æ–‡ä»¶è¿‡æ»¤å™¨å‡½æ•° file_filter(file_path: str) -> bool
    """
    def file_filter(file_path: str) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«åŠ è½½
        
        Args:
            file_path: ç›¸å¯¹äºä»“åº“æ ¹ç›®å½•çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦åŠ è½½è¯¥æ–‡ä»¶
        """
        # é»˜è®¤æ’é™¤çš„ç›®å½•å’Œæ–‡ä»¶
        excluded_dirs = {'.git', '__pycache__', 'node_modules', '.venv', 'venv', '.pytest_cache'}
        excluded_exts = {'.pyc', '.pyo', '.lock', '.log'}
        
        # æ£€æŸ¥æ˜¯å¦åœ¨æ’é™¤ç›®å½•ä¸­
        path_parts = file_path.split('/')
        if any(part in excluded_dirs for part in path_parts):
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ’é™¤çš„æ‰©å±•å
        if any(file_path.endswith(ext) for ext in excluded_exts):
            return False
        
        # å¦‚æœæŒ‡å®šäº†ç›®å½•è¿‡æ»¤
        if filter_directories:
            if not any(file_path.startswith(d.rstrip('/') + '/') or file_path.startswith(d.rstrip('/')) 
                      for d in filter_directories):
                return False
        
        # å¦‚æœæŒ‡å®šäº†æ‰©å±•åè¿‡æ»¤
        if filter_file_extensions:
            if not any(file_path.endswith(ext) for ext in filter_file_extensions):
                return False
        
        return True
    
    return file_filter


def _convert_langchain_to_llama_doc(
    lc_doc,
    owner: str,
    repo: str,
    branch: str
) -> LlamaDocument:
    """å°† LangChain Document è½¬æ¢ä¸º LlamaIndex LlamaDocument
    
    Args:
        lc_doc: LangChain Document å¯¹è±¡
        owner: ä»“åº“æ‰€æœ‰è€…
        repo: ä»“åº“åç§°
        branch: åˆ†æ”¯åç§°
        
    Returns:
        LlamaDocument å¯¹è±¡
    """
    # ä» LangChain Document æå–ä¿¡æ¯
    file_path = lc_doc.metadata.get('file_path', lc_doc.metadata.get('source', ''))
    file_name = lc_doc.metadata.get('file_name', '')
    
    # å¦‚æœæ²¡æœ‰ file_nameï¼Œä» file_path ä¸­æå–
    if not file_name and file_path:
        file_name = file_path.split('/')[-1]
    
    # æ„å»º LlamaDocument
    return LlamaDocument(
        text=lc_doc.page_content,
        metadata={
            'file_path': file_path,
            'file_name': file_name,
            'source': lc_doc.metadata.get('source', ''),
            'source_type': 'github',
            'repository': f"{owner}/{repo}",
            'branch': branch,
            'url': f"https://github.com/{owner}/{repo}/blob/{branch}/{file_path}",
        },
        id_=f"github_{owner}_{repo}_{branch}_{file_path}"
    )


def load_documents_from_github(owner: str,
                               repo: str,
                               branch: Optional[str] = None,
                               clean: bool = True,
                               show_progress: bool = True,
                               filter_directories: Optional[List[str]] = None,
                               filter_file_extensions: Optional[List[str]] = None) -> List[LlamaDocument]:
    """ä»GitHubä»“åº“åŠ è½½æ–‡æ¡£ï¼ˆä½¿ç”¨ LangChain GitLoader + æœ¬åœ° Git å…‹éš†ï¼‰
    
    ä»…æ”¯æŒå…¬å¼€ä»“åº“ã€‚
    
    Args:
        owner: ä»“åº“æ‰€æœ‰è€…
        repo: ä»“åº“åç§°
        branch: åˆ†æ”¯åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ mainï¼‰
        clean: æ˜¯å¦æ¸…ç†æ–‡æœ¬
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        filter_directories: åªåŠ è½½æŒ‡å®šç›®å½•ï¼ˆåˆ—è¡¨æ ¼å¼ï¼Œå¦‚ ["docs", "examples"]ï¼‰
        filter_file_extensions: åªåŠ è½½æŒ‡å®šæ‰©å±•åï¼ˆåˆ—è¡¨æ ¼å¼ï¼Œå¦‚ [".md", ".py"]ï¼‰
        
    Returns:
        Documentå¯¹è±¡åˆ—è¡¨
        
    Notes:
        - é¦–æ¬¡åŠ è½½ä¼šå…‹éš†ä»“åº“åˆ°æœ¬åœ°ï¼ˆdata/github_repos/ï¼‰ï¼Œåç»­ä½¿ç”¨ git pull å¢é‡æ›´æ–°
        - ä»…æ”¯æŒå…¬å¼€ä»“åº“ï¼Œä¸æ”¯æŒç§æœ‰ä»“åº“
        - é»˜è®¤ä¼šè¿‡æ»¤æ‰ .git/, __pycache__, .pyc ç­‰æ–‡ä»¶
    """
    # ä½¿ç”¨æ–°æ¶æ„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if NEW_ARCHITECTURE_AVAILABLE:
        try:
            # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ç¼“å­˜ï¼‰
            cache_manager = None
            task_id = None
            from src.config import config
            if config.ENABLE_CACHE:
                try:
                    from src.cache_manager import CacheManager
                    cache_manager = CacheManager(config.CACHE_STATE_PATH)
                    task_id = cache_manager.get_task_id(
                        owner=owner,
                        repo=repo,
                        branch=branch or "main",
                        filter_directories=filter_directories,
                        filter_file_extensions=filter_file_extensions
                    )
                    task_key = cache_manager.get_task_key(owner, repo, branch or "main")
                    cache_manager.init_task(task_id, task_key)
                    logger.info(f"åˆå§‹åŒ–ç¼“å­˜ä»»åŠ¡: {task_id} ({task_key})")
                except Exception as e:
                    logger.warning(f"åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨å¤±è´¥ï¼Œç»§ç»­ä¸ä½¿ç”¨ç¼“å­˜: {e}")
            
            source = GitHubSource(
                owner=owner,
                repo=repo,
                branch=branch,
                filter_directories=filter_directories,
                filter_file_extensions=filter_file_extensions,
                show_progress=show_progress
            )
            documents = load_documents_from_source(
                source, 
                clean=clean, 
                show_progress=show_progress,
                cache_manager=cache_manager,
                task_id=task_id
            )
            return documents
        except Exception as e:
            logger.warning(f"æ–°æ¶æ„åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ—§å®ç°: {e}")
    
    # å›é€€åˆ°æ—§å®ç°
    if GitLoader is None:
        safe_print("âŒ ç¼ºå°‘ä¾èµ–ï¼šlangchain-community")
        safe_print("   å®‰è£…ï¼špip install langchain-community")
        logger.error("GitLoader æœªå®‰è£…")
        return []
    
    if GitRepositoryManager is None:
        safe_print("âŒ GitRepositoryManager æœªå®‰è£…")
        logger.error("GitRepositoryManager æœªå®‰è£…")
        return []
    
    try:
        branch = branch or "main"
        logger.info(f"å¼€å§‹åŠ è½½ GitHub ä»“åº“: {owner}/{repo}, åˆ†æ”¯: {branch}")
        
        if show_progress:
            safe_print(f"ğŸ“¦ æ­£åœ¨ä» GitHub åŠ è½½ {owner}/{repo} (åˆ†æ”¯: {branch})...")
        
        # æ­¥éª¤ 1: ä½¿ç”¨ GitRepositoryManager å…‹éš†æˆ–æ›´æ–°ä»“åº“
        git_manager = GitRepositoryManager(config.GITHUB_REPOS_PATH)
        
        if show_progress:
            safe_print(f"ğŸ”„ æ­£åœ¨å…‹éš†/æ›´æ–°ä»“åº“åˆ°æœ¬åœ°...")
        
        try:
            repo_path, commit_sha = git_manager.clone_or_update(
                owner=owner,
                repo=repo,
                branch=branch
            )
            logger.info(f"ä»“åº“è·¯å¾„: {repo_path}, Commit: {commit_sha[:8]}")
            
            if show_progress:
                safe_print(f"âœ… ä»“åº“å·²åŒæ­¥ (Commit: {commit_sha[:8]})")
                
        except RuntimeError as e:
            error_msg = str(e)
            if show_progress:
                safe_print(f"âŒ Git æ“ä½œå¤±è´¥: {error_msg}")
            logger.error(f"Git æ“ä½œå¤±è´¥: {error_msg}")
            return []
        
        # æ­¥éª¤ 2: æ„å»ºæ–‡ä»¶è¿‡æ»¤å™¨
        file_filter = _build_file_filter(filter_directories, filter_file_extensions)
        
        # æ­¥éª¤ 3: ä½¿ç”¨ LangChain GitLoader åŠ è½½æ–‡æ¡£
        if show_progress:
            safe_print(f"ğŸ“„ æ­£åœ¨åŠ è½½æ–‡æ¡£...")
        
        try:
            loader = GitLoader(
                repo_path=str(repo_path),
                branch=branch,
                file_filter=file_filter
            )
            
            lc_documents = loader.load()
            
        except Exception as e:
            error_msg = str(e)
            if show_progress:
                safe_print(f"âŒ åŠ è½½æ–‡æ¡£å¤±è´¥: {error_msg}")
            logger.error(f"GitLoader åŠ è½½å¤±è´¥: {error_msg}")
            return []
        
        if not lc_documents:
            logger.warning(f"ä»“åº“ {owner}/{repo} æ²¡æœ‰æ–‡æ¡£")
            if show_progress:
                safe_print(f"âš ï¸  ä»“åº“ä¸ºç©ºæˆ–æ²¡æœ‰ç¬¦åˆè¿‡æ»¤æ¡ä»¶çš„æ–‡ä»¶")
            return []
        
        if show_progress:
            safe_print(f"æ‰¾åˆ° {len(lc_documents)} ä¸ªæ–‡ä»¶")
        
        # æ­¥éª¤ 4: è½¬æ¢ LangChain Document -> LlamaIndex LlamaDocument
        iterator = tqdm(lc_documents, desc="è½¬æ¢æ–‡æ¡£", unit="ä¸ª") if show_progress else lc_documents
        
        processed_docs = []
        for lc_doc in iterator:
            try:
                llama_doc = _convert_langchain_to_llama_doc(lc_doc, owner, repo, branch)
                processed_docs.append(llama_doc)
            except Exception as e:
                logger.warning(f"è½¬æ¢æ–‡æ¡£å¤±è´¥: {e}, è·³è¿‡è¯¥æ–‡æ¡£")
                continue
        
        if show_progress:
            safe_print(f"âœ… æˆåŠŸåŠ è½½ {len(processed_docs)} ä¸ªæ–‡ä»¶")
        
        logger.info(f"æˆåŠŸåŠ è½½ {len(processed_docs)} ä¸ªæ–‡ä»¶ä» {owner}/{repo}")
        
        # æ­¥éª¤ 5: å¯é€‰çš„æ–‡æœ¬æ¸…ç†
        if clean:
            processor = DocumentProcessor()
            cleaned_documents = []
            for doc in processed_docs:
                cleaned_text = processor.clean_text(doc.text)
                cleaned_doc = LlamaDocument(
                    text=cleaned_text,
                    metadata=doc.metadata,
                    id_=doc.id_
                )
                cleaned_documents.append(cleaned_doc)
            return cleaned_documents
        
        return processed_docs
        
    except Exception as e:
        error_msg = _handle_github_error(e, owner, repo, show_progress)
        # å®‰å…¨è®°å½•æ—¥å¿—ï¼ˆå¤„ç† Unicode ç¼–ç é—®é¢˜ï¼‰
        try:
            logger.error(f"åŠ è½½å¤±è´¥ {owner}/{repo}: {error_msg}")
        except UnicodeEncodeError:
            # å¦‚æœç¼–ç å¤±è´¥ï¼Œä½¿ç”¨ ASCII å®‰å…¨çš„æ ¼å¼
            safe_error_msg = error_msg.encode('ascii', 'replace').decode('ascii')
            logger.error(f"åŠ è½½å¤±è´¥ {owner}/{repo}: {safe_error_msg}")
        return []


def sync_github_repository(
    owner: str,
    repo: str,
    branch: str,
    metadata_manager,
    show_progress: bool = True,
    filter_directories: Optional[List[str]] = None,
    filter_file_extensions: Optional[List[str]] = None
) -> tuple:
    """å¢é‡åŒæ­¥ GitHub ä»“åº“ï¼ˆä»…æ”¯æŒå…¬å¼€ä»“åº“ï¼‰
    
    ä½¿ç”¨ä¸¤çº§æ£€æµ‹æœºåˆ¶ï¼š
    1. å¿«é€Ÿæ£€æµ‹ï¼šæ¯”è¾ƒ commit SHAï¼Œæ— å˜åŒ–ç›´æ¥è·³è¿‡
    2. ç²¾ç»†æ£€æµ‹ï¼šæ–‡ä»¶çº§å“ˆå¸Œæ¯”å¯¹ï¼Œåªç´¢å¼•å˜æ›´æ–‡ä»¶
    
    Args:
        owner: ä»“åº“æ‰€æœ‰è€…
        repo: ä»“åº“åç§°
        branch: åˆ†æ”¯åç§°
        metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨å®ä¾‹
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
        filter_directories: åªåŠ è½½æŒ‡å®šç›®å½•ï¼ˆå¯é€‰ï¼‰
        filter_file_extensions: åªåŠ è½½æŒ‡å®šæ‰©å±•åï¼ˆå¯é€‰ï¼‰
        
    Returns:
        (æ‰€æœ‰æ–‡æ¡£åˆ—è¡¨, FileChangeå¯¹è±¡, commit_sha, cache_manager, task_id)
    """
    from src.metadata_manager import FileChange
    
    # åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ç¼“å­˜ï¼‰
    cache_manager = None
    task_id = None
    if config.ENABLE_CACHE:
        try:
            from src.cache_manager import CacheManager
            cache_manager = CacheManager(config.CACHE_STATE_PATH)
            task_id = cache_manager.get_task_id(
                owner=owner,
                repo=repo,
                branch=branch,
                filter_directories=filter_directories,
                filter_file_extensions=filter_file_extensions
            )
            task_key = cache_manager.get_task_key(owner, repo, branch)
            cache_manager.init_task(task_id, task_key)
            logger.info(f"åˆå§‹åŒ–ç¼“å­˜ä»»åŠ¡: {task_id} ({task_key})")
        except Exception as e:
            logger.warning(f"åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨å¤±è´¥ï¼Œç»§ç»­ä¸ä½¿ç”¨ç¼“å­˜: {e}")
    
    # æ­¥éª¤ 1: å…‹éš†/æ›´æ–°ä»“åº“ï¼Œè·å–æœ€æ–° commit SHA
    if GitRepositoryManager is None:
        logger.error("GitRepositoryManager æœªå®‰è£…")
        return [], FileChange(), None, cache_manager, task_id
    
    try:
        git_manager = GitRepositoryManager(config.GITHUB_REPOS_PATH)
        repo_path, commit_sha = git_manager.clone_or_update(
            owner=owner,
            repo=repo,
            branch=branch,
            cache_manager=cache_manager,
            task_id=task_id
        )
        
        if show_progress:
            safe_print(f"âœ… ä»“åº“å·²åŒæ­¥ (Commit: {commit_sha[:8]})")
        
    except RuntimeError as e:
        logger.error(f"Git æ“ä½œå¤±è´¥: {e}")
        if show_progress:
            safe_print(f"âŒ Git æ“ä½œå¤±è´¥: {e}")
        return [], FileChange(), None, cache_manager, task_id
    
    # æ­¥éª¤ 2: å¿«é€Ÿæ£€æµ‹ - æ£€æŸ¥ commit SHA æ˜¯å¦å˜åŒ–
    old_metadata = metadata_manager.get_repository_metadata(owner, repo, branch)
    
    if old_metadata:
        old_commit_sha = old_metadata.get('last_commit_sha', '')
        if old_commit_sha == commit_sha:
            # Commit æœªå˜åŒ–ï¼Œè·³è¿‡åŠ è½½
            if show_progress:
                safe_print(f"âœ… ä»“åº“æ— æ–°æäº¤ï¼Œè·³è¿‡åŠ è½½")
            logger.info(f"ä»“åº“ {owner}/{repo}@{branch} æ— æ–°æäº¤ (Commit: {commit_sha[:8]})")
            return [], FileChange(), commit_sha, cache_manager, task_id
    
    # æ­¥éª¤ 3: æœ‰æ–°æäº¤ï¼ŒåŠ è½½æ–‡æ¡£
    if show_progress:
        safe_print(f"\nğŸ“„ æ£€æµ‹åˆ°æ–°æäº¤ï¼Œæ­£åœ¨åŠ è½½æ–‡æ¡£...")
    
    documents = load_documents_from_github(
        owner=owner,
        repo=repo,
        branch=branch,
        clean=True,
        show_progress=show_progress,
        filter_directories=filter_directories,
        filter_file_extensions=filter_file_extensions
    )
    
    if not documents:
        logger.warning(f"æœªèƒ½åŠ è½½ä»»ä½•æ–‡æ¡£ä» {owner}/{repo}")
        return [], FileChange(), commit_sha, cache_manager, task_id
    
    # æ­¥éª¤ 4: ç²¾ç»†æ£€æµ‹ - æ–‡ä»¶çº§å˜æ›´
    if show_progress:
        safe_print(f"\nğŸ” æ­£åœ¨æ£€æµ‹æ–‡ä»¶å˜æ›´...")
    
    changes = metadata_manager.detect_changes(owner, repo, branch, documents)
    
    if show_progress:
        if changes.has_changes():
            safe_print(f"ğŸ“Š æ£€æµ‹ç»“æœ: {changes.summary()}")
        else:
            safe_print(f"âœ… æ²¡æœ‰æ£€æµ‹åˆ°æ–‡ä»¶å˜æ›´")
    
    return documents, changes, commit_sha, cache_manager, task_id


def _handle_github_error(error: Exception, owner: str, repo: str, show_progress: bool = True) -> str:
    """ç»Ÿä¸€ GitHub é”™è¯¯å¤„ç†
    
    Args:
        error: å¼‚å¸¸å¯¹è±¡
        owner: ä»“åº“æ‰€æœ‰è€…
        repo: ä»“åº“åç§°
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†é”™è¯¯æç¤º
        
    Returns:
        é”™è¯¯æè¿°å­—ç¬¦ä¸²ï¼ˆASCII å®‰å…¨ï¼‰
    """
    error_type = type(error).__name__
    error_str = str(error)
    
    # å¤„ç† UnicodeEncodeErrorï¼šå®‰å…¨è½¬æ¢é”™è¯¯æ¶ˆæ¯
    try:
        # å°è¯•ä½¿ç”¨åŸå§‹é”™è¯¯æ¶ˆæ¯
        _ = error_str.encode('ascii')
    except UnicodeEncodeError:
        # å¦‚æœåŒ…å«é ASCII å­—ç¬¦ï¼Œè¿›è¡Œè½¬æ¢
        error_str = error_str.encode('ascii', 'replace').decode('ascii')
    
    if not show_progress:
        return f"{error_type}: {error_str}"
    
    # GitHub API é”™è¯¯
    if "404" in error_str or "Not Found" in error_str:
        safe_print(f"âŒ ä»“åº“ä¸å­˜åœ¨: {owner}/{repo}")
        safe_print("   è¯·æ£€æŸ¥ï¼š1) ä»“åº“åæ‹¼å†™ 2) æ˜¯å¦ä¸ºç§æœ‰ä»“åº“ï¼ˆéœ€è¦Tokenï¼‰")
        return "ä»“åº“ä¸å­˜åœ¨(404)"
    
    elif "403" in error_str or "Forbidden" in error_str or "rate limit" in error_str.lower():
        safe_print(f"âŒ è®¿é—®è¢«æ‹’ç»: {owner}/{repo}")
        safe_print("   è¯·æ£€æŸ¥ï¼š1) Tokenæƒé™ 2) APIé™æµï¼ˆGitHubé™åˆ¶ï¼šæ¯å°æ—¶60æ¬¡ï¼Œæœ‰Tokenä¸º5000æ¬¡ï¼‰")
        safe_print("   å»ºè®®ï¼šé…ç½® GITHUB_TOKEN ç¯å¢ƒå˜é‡ä»¥æé«˜é™é¢")
        return "è®¿é—®è¢«æ‹’ç»(403)"
    
    elif "401" in error_str or "Unauthorized" in error_str or "Bad credentials" in error_str:
        safe_print(f"âŒ è®¤è¯å¤±è´¥")
        safe_print("   è¯·æ£€æŸ¥ï¼š1) Tokenæ˜¯å¦æ­£ç¡® 2) Tokenæ˜¯å¦è¿‡æœŸ")
        return "è®¤è¯å¤±è´¥(401)"
    
    # ç½‘ç»œç›¸å…³é”™è¯¯
    elif "timeout" in error_str.lower() or "timed out" in error_str.lower():
        safe_print(f"âŒ ç½‘ç»œè¶…æ—¶: {owner}/{repo}")
        safe_print("   å»ºè®®ï¼š1) æ£€æŸ¥ç½‘ç»œè¿æ¥ 2) ç¨åé‡è¯•")
        return "ç½‘ç»œè¶…æ—¶"
    
    elif "connection" in error_str.lower():
        safe_print(f"âŒ ç½‘ç»œè¿æ¥å¤±è´¥")
        safe_print("   å»ºè®®ï¼š1) æ£€æŸ¥ç½‘ç»œ 2) æ£€æŸ¥ä»£ç†è®¾ç½®")
        return "ç½‘ç»œè¿æ¥å¤±è´¥"
    
    # é€šç”¨é”™è¯¯
    else:
        safe_print(f"âŒ åŠ è½½å¤±è´¥: {error_type}: {error}")
        safe_print(f"   å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·æŠ¥å‘Šåˆ°é¡¹ç›® Issue")
        return f"{error_type}: {error_str}"


def parse_github_url(url: str) -> Optional[dict]:
    """è§£æ GitHub URL è·å–ä»“åº“ä¿¡æ¯
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - https://github.com/owner/repo
    - github.com/owner/repo
    - http://github.com/owner/repo
    - https://github.com/owner/repo/tree/branch
    
    Args:
        url: GitHub URL
        
    Returns:
        åŒ…å« owner, repo, branch çš„å­—å…¸ï¼Œè§£æå¤±è´¥è¿”å› None
    """
    from urllib.parse import urlparse
    
    try:
        # æ¸…ç† URL
        url = url.strip()
        
        # å¦‚æœæ²¡æœ‰åè®®ï¼Œè‡ªåŠ¨æ·»åŠ  https://
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        # è§£æ URL
        parsed = urlparse(url)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ GitHub
        if 'github.com' not in parsed.netloc:
            logger.warning(f"ä¸æ˜¯æœ‰æ•ˆçš„ GitHub URL: {url}")
            return None
        
        # è§£æè·¯å¾„: /owner/repo æˆ– /owner/repo/tree/branch
        path_parts = [p for p in parsed.path.split('/') if p]
        
        if len(path_parts) < 2:
            logger.warning(f"URL è·¯å¾„ä¸å®Œæ•´: {url}")
            return None
        
        owner = path_parts[0]
        repo = path_parts[1]
        
        # ç§»é™¤ .git åç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
        if repo.endswith('.git'):
            repo = repo[:-4]
        
        # æå–åˆ†æ”¯ï¼ˆå¦‚æœ URL ä¸­åŒ…å« /tree/branchï¼‰
        branch = None
        if len(path_parts) >= 4 and path_parts[2] == 'tree':
            branch = path_parts[3]
        
        result = {
            'owner': owner,
            'repo': repo,
            'branch': branch or 'main'  # é»˜è®¤ä½¿ç”¨ main åˆ†æ”¯
        }
        
        logger.info(f"è§£æ GitHub URL æˆåŠŸ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"è§£æ GitHub URL å¤±è´¥: {e}")
        return None


def load_documents_from_github_url(
    github_url: str,
    clean: bool = True,
    show_progress: bool = True
) -> List[LlamaDocument]:
    """ä» GitHub URL åŠ è½½æ–‡æ¡£ï¼ˆä»…æ”¯æŒå…¬å¼€ä»“åº“ï¼‰
    
    Args:
        github_url: GitHub ä»“åº“ URLï¼ˆå¦‚ï¼šhttps://github.com/owner/repoï¼‰
        clean: æ˜¯å¦æ¸…ç†æ–‡æœ¬
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
    Returns:
        Documentå¯¹è±¡åˆ—è¡¨
        
    Examples:
        >>> docs = load_documents_from_github_url(
        ...     "https://github.com/microsoft/TypeScript"
        ... )
    
    Note:
        - ä»…æ”¯æŒå…¬å¼€ä»“åº“
        - ç§æœ‰ä»“åº“å°†æ— æ³•è®¿é—®
    """
    # è§£æ URL
    repo_info = parse_github_url(github_url)
    if not repo_info:
        logger.error(f"æ— æ³•è§£æ GitHub URL: {github_url}")
        safe_print(f"âŒ æ— æ³•è§£æ GitHub URL: {github_url}")
        return []
    
    # è°ƒç”¨åŸæœ‰å‡½æ•°åŠ è½½æ–‡æ¡£
    return load_documents_from_github(
        owner=repo_info['owner'],
        repo=repo_info['repo'],
        branch=repo_info['branch'],
        clean=clean,
        show_progress=show_progress
    )


"""
æ•°æ®åŠ è½½å™¨æ¨¡å— - å‘åå…¼å®¹å±‚
å·²æ¨¡å—åŒ–æ‹†åˆ†ï¼Œæ­¤æ–‡ä»¶ä¿æŒå‘åå…¼å®¹
"""

# ä»æ–°æ¨¡å—å¯¼å…¥æ‰€æœ‰å…¬å¼€æ¥å£
from src.data_loader import (
    DocumentProcessor,
    safe_print,
    load_documents_from_source,
    load_documents_from_directory,
    load_documents_from_urls,
    load_documents_from_github,
    load_documents_from_github_url,
    sync_github_repository,
    parse_github_url,
)

__all__ = [
    'DocumentProcessor',
    'safe_print',
    'load_documents_from_source',
    'load_documents_from_directory',
    'load_documents_from_urls',
    'load_documents_from_github',
    'load_documents_from_github_url',
    'sync_github_repository',
    'parse_github_url',
]
