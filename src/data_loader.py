"""
æ•°æ®åŠ è½½å™¨æ¨¡å—
æ”¯æŒä»å¤šç§æ•°æ®æºåŠ è½½æ–‡æ¡£ï¼šMarkdownæ–‡ä»¶ã€ç½‘é¡µç­‰
"""

import re
from pathlib import Path
from typing import List, Optional
from urllib.parse import urlparse

import requests
from bs4 import BeautifulSoup
from llama_index.core import Document
from llama_index.core.schema import Document as LlamaDocument


class MarkdownLoader:
    """Markdownæ–‡ä»¶åŠ è½½å™¨"""
    
    def __init__(self):
        self.supported_extensions = [".md", ".markdown"]
    
    def load_file(self, file_path: Path) -> Optional[LlamaDocument]:
        """åŠ è½½å•ä¸ªMarkdownæ–‡ä»¶
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            LlamaDocumentå¯¹è±¡ï¼Œå¦‚æœåŠ è½½å¤±è´¥è¿”å›None
        """
        try:
            if not file_path.exists():
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return None
                
            if file_path.suffix.lower() not in self.supported_extensions:
                print(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.suffix}")
                return None
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # æå–æ ‡é¢˜ä½œä¸ºæ–‡æ¡£æ ‡é¢˜ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            title = self._extract_title(content)
            if not title:
                title = file_path.stem
            
            # åˆ›å»ºDocumentå¯¹è±¡
            doc = LlamaDocument(
                text=content,
                metadata={
                    "file_path": str(file_path),
                    "file_name": file_path.name,
                    "title": title,
                    "source_type": "markdown",
                }
            )
            
            return doc
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    def load_directory(self, directory_path: Path, recursive: bool = True) -> List[LlamaDocument]:
        """åŠ è½½ç›®å½•ä¸­çš„æ‰€æœ‰Markdownæ–‡ä»¶
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            recursive: æ˜¯å¦é€’å½’åŠ è½½å­ç›®å½•
            
        Returns:
            Documentå¯¹è±¡åˆ—è¡¨
        """
        documents = []
        
        if not directory_path.exists() or not directory_path.is_dir():
            print(f"âŒ ç›®å½•ä¸å­˜åœ¨æˆ–ä¸æ˜¯æœ‰æ•ˆç›®å½•: {directory_path}")
            return documents
        
        # æŸ¥æ‰¾æ‰€æœ‰Markdownæ–‡ä»¶
        pattern = "**/*" if recursive else "*"
        for ext in self.supported_extensions:
            for file_path in directory_path.glob(f"{pattern}{ext}"):
                if file_path.is_file():
                    doc = self.load_file(file_path)
                    if doc:
                        documents.append(doc)
                        print(f"âœ… å·²åŠ è½½: {file_path.name}")
        
        print(f"\nğŸ“š æ€»å…±åŠ è½½äº† {len(documents)} ä¸ªMarkdownæ–‡ä»¶")
        return documents
    
    def _extract_title(self, content: str) -> Optional[str]:
        """ä»Markdownå†…å®¹ä¸­æå–æ ‡é¢˜ï¼ˆç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜ï¼‰"""
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('# '):
                return line[2:].strip()
        return None


class WebLoader:
    """ç½‘é¡µå†…å®¹åŠ è½½å™¨"""
    
    def __init__(self, timeout: int = 10):
        self.timeout = timeout
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def load_url(self, url: str) -> Optional[LlamaDocument]:
        """ä»URLåŠ è½½ç½‘é¡µå†…å®¹
        
        Args:
            url: ç½‘é¡µURL
            
        Returns:
            LlamaDocumentå¯¹è±¡ï¼Œå¦‚æœåŠ è½½å¤±è´¥è¿”å›None
        """
        try:
            # éªŒè¯URL
            parsed = urlparse(url)
            if not parsed.scheme or not parsed.netloc:
                print(f"âŒ æ— æ•ˆçš„URL: {url}")
                return None
            
            # å‘é€HTTPè¯·æ±‚
            response = requests.get(url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()
            
            # è§£æHTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼æ ‡ç­¾
            for script in soup(["script", "style"]):
                script.decompose()
            
            # æå–æ–‡æœ¬å†…å®¹
            text = soup.get_text(separator='\n', strip=True)
            
            # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
            text = re.sub(r'\n\s*\n', '\n\n', text)
            
            # æå–æ ‡é¢˜
            title = soup.title.string if soup.title else parsed.netloc
            
            # åˆ›å»ºDocumentå¯¹è±¡
            doc = LlamaDocument(
                text=text,
                metadata={
                    "url": url,
                    "title": title,
                    "source_type": "web",
                    "domain": parsed.netloc,
                }
            )
            
            return doc
            
        except requests.RequestException as e:
            print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥ {url}: {e}")
            return None
        except Exception as e:
            print(f"âŒ å¤„ç†ç½‘é¡µå¤±è´¥ {url}: {e}")
            return None
    
    def load_urls(self, urls: List[str]) -> List[LlamaDocument]:
        """æ‰¹é‡åŠ è½½å¤šä¸ªURL
        
        Args:
            urls: URLåˆ—è¡¨
            
        Returns:
            Documentå¯¹è±¡åˆ—è¡¨
        """
        documents = []
        
        for url in urls:
            doc = self.load_url(url)
            if doc:
                documents.append(doc)
                print(f"âœ… å·²åŠ è½½: {url}")
        
        print(f"\nğŸŒ æ€»å…±åŠ è½½äº† {len(documents)} ä¸ªç½‘é¡µ")
        return documents


class DocumentProcessor:
    """æ–‡æ¡£é¢„å¤„ç†å™¨"""
    
    @staticmethod
    def clean_text(text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬
        """
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'[ \t]+', ' ', text)
        
        # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = [line.strip() for line in text.split('\n')]
        
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œï¼ˆä¿ç•™æœ€å¤šä¸€ä¸ªç©ºè¡Œï¼Œå³æœ€å¤šä¸¤ä¸ªè¿ç»­æ¢è¡Œï¼‰
        cleaned_lines = []
        prev_empty = False
        for line in lines:
            if line:  # éç©ºè¡Œ
                cleaned_lines.append(line)
                prev_empty = False
            else:  # ç©ºè¡Œ
                if not prev_empty:  # åªä¿ç•™ç¬¬ä¸€ä¸ªç©ºè¡Œ
                    cleaned_lines.append(line)
                    prev_empty = True
                # è¿ç»­çš„ç©ºè¡Œè·³è¿‡
        
        text = '\n'.join(cleaned_lines)
        return text.strip()
    
    @staticmethod
    def enrich_metadata(document: LlamaDocument, additional_metadata: dict) -> LlamaDocument:
        """ä¸ºæ–‡æ¡£æ·»åŠ é¢å¤–çš„å…ƒæ•°æ®
        
        Args:
            document: åŸå§‹æ–‡æ¡£
            additional_metadata: è¦æ·»åŠ çš„å…ƒæ•°æ®
            
        Returns:
            æ›´æ–°åçš„æ–‡æ¡£
        """
        document.metadata.update(additional_metadata)
        return document
    
    @staticmethod
    def filter_by_length(documents: List[LlamaDocument], 
                        min_length: int = 50) -> List[LlamaDocument]:
        """è¿‡æ»¤æ‰å¤ªçŸ­çš„æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            min_length: æœ€å°é•¿åº¦
            
        Returns:
            è¿‡æ»¤åçš„æ–‡æ¡£åˆ—è¡¨
        """
        filtered = [doc for doc in documents if len(doc.text) >= min_length]
        
        removed_count = len(documents) - len(filtered)
        if removed_count > 0:
            print(f"âš ï¸  è¿‡æ»¤æ‰ {removed_count} ä¸ªè¿‡çŸ­çš„æ–‡æ¡£")
        
        return filtered


# ä¾¿æ·å‡½æ•°
def load_documents_from_directory(directory_path: str | Path, 
                                 recursive: bool = True,
                                 clean: bool = True) -> List[LlamaDocument]:
    """ä»ç›®å½•åŠ è½½æ‰€æœ‰Markdownæ–‡æ¡£ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        directory_path: ç›®å½•è·¯å¾„
        recursive: æ˜¯å¦é€’å½’åŠ è½½
        clean: æ˜¯å¦æ¸…ç†æ–‡æœ¬
        
    Returns:
        Documentå¯¹è±¡åˆ—è¡¨
    """
    directory_path = Path(directory_path)
    loader = MarkdownLoader()
    documents = loader.load_directory(directory_path, recursive=recursive)
    
    if clean:
        processor = DocumentProcessor()
        # Document.text æ˜¯åªè¯»å±æ€§ï¼Œéœ€è¦åˆ›å»ºæ–°çš„ Document å¯¹è±¡
        cleaned_documents = []
        for doc in documents:
            cleaned_text = processor.clean_text(doc.text)
            cleaned_doc = LlamaDocument(
                text=cleaned_text,
                metadata=doc.metadata,
                id_=doc.id_
            )
            cleaned_documents.append(cleaned_doc)
        return cleaned_documents
    
    return documents


def load_documents_from_urls(urls: List[str], clean: bool = True) -> List[LlamaDocument]:
    """ä»URLåˆ—è¡¨åŠ è½½æ–‡æ¡£ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        urls: URLåˆ—è¡¨
        clean: æ˜¯å¦æ¸…ç†æ–‡æœ¬
        
    Returns:
        Documentå¯¹è±¡åˆ—è¡¨
    """
    loader = WebLoader()
    documents = loader.load_urls(urls)
    
    if clean:
        processor = DocumentProcessor()
        # Document.text æ˜¯åªè¯»å±æ€§ï¼Œéœ€è¦åˆ›å»ºæ–°çš„ Document å¯¹è±¡
        cleaned_documents = []
        for doc in documents:
            cleaned_text = processor.clean_text(doc.text)
            cleaned_doc = LlamaDocument(
                text=cleaned_text,
                metadata=doc.metadata,
                id_=doc.id_
            )
            cleaned_documents.append(cleaned_doc)
        return cleaned_documents
    
    return documents


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from src.config import config
    
    print("=== æµ‹è¯•MarkdownåŠ è½½å™¨ ===")
    documents = load_documents_from_directory(config.RAW_DATA_PATH)
    print(f"åŠ è½½äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    if documents:
        print("\nç¬¬ä¸€ä¸ªæ–‡æ¡£é¢„è§ˆ:")
        print(f"æ ‡é¢˜: {documents[0].metadata.get('title')}")
        print(f"å†…å®¹é•¿åº¦: {len(documents[0].text)}")
        print(f"å†…å®¹é¢„è§ˆ: {documents[0].text[:200]}...")

