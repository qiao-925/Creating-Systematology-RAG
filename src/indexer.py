"""
ç´¢å¼•æ„å»ºæ¨¡å—
è´Ÿè´£æ„å»ºå’Œç®¡ç†å‘é‡ç´¢å¼•ï¼Œé›†æˆChromaå‘é‡æ•°æ®åº“
"""

import os
from pathlib import Path
from typing import List, Optional, Tuple, Dict

import chromadb
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    Settings,
)
from llama_index.core.schema import Document as LlamaDocument
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore

from src.config import config
from src.logger import setup_logger

logger = setup_logger('indexer')

# å…¨å±€ embedding æ¨¡å‹ç¼“å­˜
_global_embed_model: Optional[HuggingFaceEmbedding] = None


def _setup_huggingface_env():
    """é…ç½® HuggingFace ç¯å¢ƒå˜é‡ï¼ˆé•œåƒå’Œç¦»çº¿æ¨¡å¼ï¼‰
    
    æ³¨æ„ï¼šç¯å¢ƒå˜é‡å·²åœ¨ src/__init__.py ä¸­é¢„è®¾ï¼Œè¿™é‡Œä»…ç”¨äºæ—¥å¿—è®°å½•å’Œç¡®è®¤
    """
    # è®¾ç½®é•œåƒåœ°å€
    if config.HF_ENDPOINT:
        os.environ['HF_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HUGGINGFACE_HUB_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HF_HUB_ENDPOINT'] = config.HF_ENDPOINT  # æ–°ç‰ˆæœ¬ä½¿ç”¨è¿™ä¸ª
        logger.info(f"ğŸŒ ä½¿ç”¨ HuggingFace é•œåƒ: {config.HF_ENDPOINT}")
    
    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    if config.HF_OFFLINE_MODE:
        os.environ['HF_HUB_OFFLINE'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '1'
        logger.info(f"ğŸ“´ å¯ç”¨ç¦»çº¿æ¨¡å¼ï¼ˆä»…ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")
    else:
        # ç¡®ä¿ç¦»çº¿æ¨¡å¼å…³é—­
        os.environ.pop('HF_HUB_OFFLINE', None)
        os.environ.pop('TRANSFORMERS_OFFLINE', None)


def load_embedding_model(model_name: Optional[str] = None, force_reload: bool = False) -> HuggingFaceEmbedding:
    """åŠ è½½ Embedding æ¨¡å‹ï¼ˆæ”¯æŒå…¨å±€å•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼ˆå³ä½¿å·²ç¼“å­˜ï¼‰
        
    Returns:
        HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    
    model_name = model_name or config.EMBEDDING_MODEL
    
    # å¦‚æœå·²ç»åŠ è½½è¿‡ä¸”æ¨¡å‹åç§°ç›¸åŒï¼Œç›´æ¥è¿”å›ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°åŠ è½½ï¼‰
    if _global_embed_model is not None and not force_reload:
        # æ£€æŸ¥ç¼“å­˜çš„æ¨¡å‹åç§°æ˜¯å¦ä¸æ–°é…ç½®ä¸€è‡´
        cached_model_name = getattr(_global_embed_model, 'model_name', None)
        if cached_model_name == model_name:
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„ Embedding æ¨¡å‹ï¼ˆå…¨å±€å˜é‡ï¼‰: {model_name}")
            logger.info(f"   æ¨¡å‹å¯¹è±¡ID: {id(_global_embed_model)}")
            return _global_embed_model
        else:
            # æ¨¡å‹åç§°ä¸ä¸€è‡´ï¼Œæ¸…é™¤ç¼“å­˜å¹¶é‡æ–°åŠ è½½
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹é…ç½®å˜æ›´: {cached_model_name} -> {model_name}")
            logger.info(f"   æ¸…é™¤æ—§æ¨¡å‹ç¼“å­˜ï¼Œé‡æ–°åŠ è½½æ–°æ¨¡å‹")
            _global_embed_model = None
    
    # å¦‚æœéœ€è¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œæ¸…é™¤ç¼“å­˜
    if force_reload:
        logger.info(f"ğŸ”„ å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å‹")
        _global_embed_model = None
    
    # é…ç½® HuggingFace ç¯å¢ƒå˜é‡
    _setup_huggingface_env()
    
    # åŠ è½½æ¨¡å‹
    logger.info(f"ğŸ“¦ å¼€å§‹åŠ è½½ Embedding æ¨¡å‹ï¼ˆå…¨æ–°åŠ è½½ï¼‰: {model_name}")
    
    try:
        # æ˜¾å¼æŒ‡å®šç¼“å­˜ç›®å½•ä»¥ç¡®ä¿ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        cache_folder = str(Path.home() / ".cache" / "huggingface")
        _global_embed_model = HuggingFaceEmbedding(
            model_name=model_name,
            trust_remote_code=True,
            cache_folder=cache_folder,
            embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†ï¼Œæå‡æ€§èƒ½
            max_length=config.EMBED_MAX_LENGTH,  # è®¾ç½®æœ€å¤§é•¿åº¦
        )
        logger.info(f"âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_folder}")
        logger.info(f"âš¡ æ‰¹å¤„ç†é…ç½®: batch_size={config.EMBED_BATCH_SIZE}, max_length={config.EMBED_MAX_LENGTH}")
    except Exception as e:
        # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ä¸”ç¼ºå°‘ç¼“å­˜ï¼Œå°è¯•åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
        if config.HF_OFFLINE_MODE and "offline" in str(e).lower():
            logger.warning(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æœ¬åœ°æ— ç¼“å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼å°è¯•ä¸‹è½½")
            os.environ.pop('HF_HUB_OFFLINE', None)
            
            try:
                cache_folder = str(Path.home() / ".cache" / "huggingface")
                _global_embed_model = HuggingFaceEmbedding(
                    model_name=model_name,
                    trust_remote_code=True,
                    cache_folder=cache_folder,
                    embed_batch_size=config.EMBED_BATCH_SIZE,
                    max_length=config.EMBED_MAX_LENGTH,
                )
                logger.info(f"âœ… Embedding æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ: {model_name}")
                logger.info(f"âš¡ æ‰¹å¤„ç†é…ç½®: batch_size={config.EMBED_BATCH_SIZE}, max_length={config.EMBED_MAX_LENGTH}")
            except Exception as retry_error:
                logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {retry_error}")
                raise
        else:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    return _global_embed_model


def set_global_embed_model(model: HuggingFaceEmbedding):
    """è®¾ç½®å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Args:
        model: HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    _global_embed_model = model
    logger.debug("ğŸ”§ è®¾ç½®å…¨å±€ Embedding æ¨¡å‹")


def get_global_embed_model() -> Optional[HuggingFaceEmbedding]:
    """è·å–å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Returns:
        å·²åŠ è½½çš„æ¨¡å‹å®ä¾‹ï¼Œå¦‚æœæœªåŠ è½½åˆ™è¿”å› None
    """
    return _global_embed_model


def clear_embedding_model_cache():
    """æ¸…é™¤å…¨å±€ Embedding æ¨¡å‹ç¼“å­˜
    
    ç”¨äºæ¨¡å‹åˆ‡æ¢æˆ–å¼ºåˆ¶é‡æ–°åŠ è½½åœºæ™¯
    """
    global _global_embed_model
    if _global_embed_model is not None:
        logger.info(f"ğŸ§¹ æ¸…é™¤ Embedding æ¨¡å‹ç¼“å­˜")
        _global_embed_model = None


def get_embedding_model_status() -> dict:
    """è·å– Embedding æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    
    Returns:
        åŒ…å«æ¨¡å‹çŠ¶æ€çš„å­—å…¸ï¼š
        {
            "loaded": bool,              # æ˜¯å¦å·²åŠ è½½
            "model_name": str,           # æ¨¡å‹åç§°
            "cache_dir": str,            # ç¼“å­˜ç›®å½•
            "cache_exists": bool,        # æœ¬åœ°ç¼“å­˜æ˜¯å¦å­˜åœ¨
            "offline_mode": bool,        # æ˜¯å¦ç¦»çº¿æ¨¡å¼
            "mirror": str,               # é•œåƒåœ°å€
        }
    """
    import os
    from pathlib import Path
    
    model_name = config.EMBEDDING_MODEL
    
    # æ£€æŸ¥ç¼“å­˜ç›®å½•
    cache_root = Path.home() / ".cache" / "huggingface" / "hub"
    # HuggingFace ç¼“å­˜æ ¼å¼: models--{org}--{model}
    model_cache_name = model_name.replace("/", "--")
    cache_dir = cache_root / f"models--{model_cache_name}"
    cache_exists = cache_dir.exists()
    
    return {
        "loaded": _global_embed_model is not None,
        "model_name": model_name,
        "cache_dir": str(cache_dir),
        "cache_exists": cache_exists,
        "offline_mode": config.HF_OFFLINE_MODE,
        "mirror": config.HF_ENDPOINT if config.HF_ENDPOINT else "huggingface.co (å®˜æ–¹)",
    }


class IndexManager:
    """ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(
        self,
        collection_name: Optional[str] = None,
        persist_dir: Optional[Path] = None,
        embedding_model: Optional[str] = None,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        embed_model_instance: Optional[HuggingFaceEmbedding] = None,
    ):
        """åˆå§‹åŒ–ç´¢å¼•ç®¡ç†å™¨
        
        Args:
            collection_name: Chromaé›†åˆåç§°
            persist_dir: å‘é‡å­˜å‚¨æŒä¹…åŒ–ç›®å½•
            embedding_model: Embeddingæ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å—é‡å å¤§å°
            embed_model_instance: é¢„åŠ è½½çš„Embeddingæ¨¡å‹å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        # ä½¿ç”¨é…ç½®æˆ–é»˜è®¤å€¼
        self.collection_name = collection_name or config.CHROMA_COLLECTION_NAME
        self.persist_dir = persist_dir or config.VECTOR_STORE_PATH
        self.embedding_model_name = embedding_model or config.EMBEDDING_MODEL
        self.chunk_size = chunk_size or config.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or config.CHUNK_OVERLAP
        
        # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
        self.persist_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„å®ä¾‹ï¼‰
        if embed_model_instance is not None:
            print(f"âœ… ä½¿ç”¨é¢„åŠ è½½çš„Embeddingæ¨¡å‹: {self.embedding_model_name}")
            self.embed_model = embed_model_instance
        else:
            # é…ç½® HuggingFace ç¯å¢ƒå˜é‡
            _setup_huggingface_env()
            
            print(f"ğŸ“¦ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {self.embedding_model_name}")
            
            try:
                cache_folder = str(Path.home() / ".cache" / "huggingface")
                self.embed_model = HuggingFaceEmbedding(
                    model_name=self.embedding_model_name,
                    trust_remote_code=True,
                    cache_folder=cache_folder,
                    embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†
                    max_length=config.EMBED_MAX_LENGTH,
                )
                print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE})")
            except Exception as e:
                # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ä¸”ç¼ºå°‘ç¼“å­˜ï¼Œå°è¯•åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
                if config.HF_OFFLINE_MODE and "offline" in str(e).lower():
                    print(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æœ¬åœ°æ— ç¼“å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼å°è¯•ä¸‹è½½...")
                    os.environ.pop('HF_HUB_OFFLINE', None)
                    
                    try:
                        cache_folder = str(Path.home() / ".cache" / "huggingface")
                        self.embed_model = HuggingFaceEmbedding(
                            model_name=self.embedding_model_name,
                            trust_remote_code=True,
                            cache_folder=cache_folder,
                            embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†
                            max_length=config.EMBED_MAX_LENGTH,
                        )
                        print(f"âœ… æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ (æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE})")
                    except Exception as retry_error:
                        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {retry_error}")
                        raise
                else:
                    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    raise
        
        # é…ç½®å…¨å±€Settings
        Settings.embed_model = self.embed_model
        Settings.chunk_size = self.chunk_size
        Settings.chunk_overlap = self.chunk_overlap
        
        # åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
        print(f"ğŸ—„ï¸  åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“: {self.persist_dir}")
        self.chroma_client = chromadb.PersistentClient(path=str(self.persist_dir))
        
        # è·å–æˆ–åˆ›å»ºé›†åˆ
        self.chroma_collection = self.chroma_client.get_or_create_collection(
            name=self.collection_name
        )
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
        
        # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )
        
        # ç´¢å¼•å¯¹è±¡ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._index: Optional[VectorStoreIndex] = None
        
        print("âœ… ç´¢å¼•ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def build_index(
        self,
        documents: List[LlamaDocument],
        show_progress: bool = True
    ) -> Tuple[VectorStoreIndex, Dict[str, List[str]]]:
        """æ„å»ºæˆ–æ›´æ–°ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            (VectorStoreIndexå¯¹è±¡, æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDçš„æ˜ å°„)
        """
        import time
        start_time = time.time()
        
        if not documents:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ç´¢å¼•")
            return self.get_index(), {}
        
        print(f"\nğŸ”¨ å¼€å§‹æ„å»ºç´¢å¼•ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
        print(f"   åˆ†å—å‚æ•°: size={self.chunk_size}, overlap={self.chunk_overlap}")
        print(f"   æ‰¹å¤„ç†é…ç½®: embed_batch_size={config.EMBED_BATCH_SIZE}")
        
        try:
            # å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç´¢å¼•
            if self._index is None:
                index_start_time = time.time()
                self._index = VectorStoreIndex.from_documents(
                    documents,
                    storage_context=self.storage_context,
                    show_progress=show_progress,
                )
                index_elapsed = time.time() - index_start_time
                print(f"âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ (è€—æ—¶: {index_elapsed:.2f}s)")
                logger.info(f"ç´¢å¼•åˆ›å»ºå®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, è€—æ—¶{index_elapsed:.2f}s, å¹³å‡{index_elapsed/len(documents):.3f}s/æ–‡æ¡£")
            else:
                # å¦‚æœç´¢å¼•å·²å­˜åœ¨ï¼Œæ‰¹é‡å¢é‡æ·»åŠ æ–‡æ¡£ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨insert_ref_docsæ‰¹é‡æ’å…¥ï¼‰
                insert_start_time = time.time()
                
                # ä½¿ç”¨insert_ref_docsæ‰¹é‡æ’å…¥ï¼Œæ€§èƒ½è¿œä¼˜äºé€ä¸ªinsert
                # LlamaIndexä¼šè‡ªåŠ¨æ‰¹å¤„ç†embeddingè®¡ç®—å’Œå‘é‡å­˜å‚¨å†™å…¥
                try:
                    self._index.insert_ref_docs(documents, show_progress=show_progress)
                    insert_elapsed = time.time() - insert_start_time
                    print(f"âœ… æ–‡æ¡£å·²æ‰¹é‡æ·»åŠ åˆ°ç°æœ‰ç´¢å¼• (è€—æ—¶: {insert_elapsed:.2f}s)")
                    logger.info(
                        f"æ‰¹é‡å¢é‡æ·»åŠ å®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, "
                        f"è€—æ—¶{insert_elapsed:.2f}s, "
                        f"å¹³å‡{insert_elapsed/len(documents):.3f}s/æ–‡æ¡£"
                    )
                except AttributeError:
                    # å¦‚æœinsert_ref_docsä¸å­˜åœ¨ï¼Œå›é€€åˆ°æ‰¹é‡æ’å…¥èŠ‚ç‚¹çš„æ–¹å¼
                    logger.warning("insert_ref_docsä¸å¯ç”¨ï¼Œä½¿ç”¨èŠ‚ç‚¹æ‰¹é‡æ’å…¥æ–¹å¼")
                    from llama_index.core.node_parser import SentenceSplitter
                    node_parser = SentenceSplitter(
                        chunk_size=self.chunk_size,
                        chunk_overlap=self.chunk_overlap
                    )
                    # æ‰¹é‡åˆ†å—å¹¶æ’å…¥èŠ‚ç‚¹
                    batch_size = config.EMBED_BATCH_SIZE * 2
                    for i in range(0, len(documents), batch_size):
                        batch_docs = documents[i:i+batch_size]
                        nodes = node_parser.get_nodes_from_documents(batch_docs)
                        # æ‰¹é‡æ’å…¥èŠ‚ç‚¹
                        for node in nodes:
                            self._index.insert(node)
                        if show_progress:
                            progress = min(i + batch_size, len(documents))
                            print(f"   è¿›åº¦: {progress}/{len(documents)} ({progress/len(documents)*100:.1f}%)")
                    insert_elapsed = time.time() - insert_start_time
                    print(f"âœ… æ–‡æ¡£å·²æ‰¹é‡æ·»åŠ åˆ°ç°æœ‰ç´¢å¼• (è€—æ—¶: {insert_elapsed:.2f}s)")
                    logger.info(
                        f"æ‰¹é‡å¢é‡æ·»åŠ å®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, "
                        f"è€—æ—¶{insert_elapsed:.2f}s, "
                        f"å¹³å‡{insert_elapsed/len(documents):.3f}s/æ–‡æ¡£"
                    )
            
            # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
            stats = self.get_stats()
            total_elapsed = time.time() - start_time
            
            print(f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡: {stats}")
            logger.info(
                f"ç´¢å¼•æ„å»ºå®Œæˆ: "
                f"æ–‡æ¡£æ•°={len(documents)}, "
                f"å‘é‡æ•°={stats.get('document_count', 0)}, "
                f"æ€»è€—æ—¶={total_elapsed:.2f}s, "
                f"å¹³å‡={total_elapsed/len(documents):.3f}s/æ–‡æ¡£"
            )
            
            # æ„å»ºå‘é‡IDæ˜ å°„ï¼ˆä¼˜åŒ–ï¼šæ‰¹é‡æŸ¥è¯¢ï¼‰
            vector_ids_map_start = time.time()
            vector_ids_map = self._get_vector_ids_batch(
                [doc.metadata.get("file_path", "") for doc in documents 
                 if doc.metadata.get("file_path")]
            )
            vector_ids_elapsed = time.time() - vector_ids_map_start
            print(f"ğŸ“‹ å·²è®°å½• {len(vector_ids_map)} ä¸ªæ–‡ä»¶çš„å‘é‡IDæ˜ å°„ (è€—æ—¶: {vector_ids_elapsed:.2f}s)")
            logger.debug(f"å‘é‡IDæ˜ å°„æ„å»ºè€—æ—¶: {vector_ids_elapsed:.2f}s")
            
            return self._index, vector_ids_map
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise
    
    def get_index(self) -> VectorStoreIndex:
        """è·å–ç°æœ‰ç´¢å¼•
        
        Returns:
            VectorStoreIndexå¯¹è±¡
        """
        if self._index is None:
            # å°è¯•ä»å·²æœ‰çš„å‘é‡å­˜å‚¨åŠ è½½
            try:
                self._index = VectorStoreIndex.from_vector_store(
                    vector_store=self.vector_store,
                    storage_context=self.storage_context,
                )
                print("âœ… ä»å‘é‡å­˜å‚¨åŠ è½½ç´¢å¼•æˆåŠŸ")
            except Exception as e:
                print(f"â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œå°†åœ¨æ·»åŠ æ–‡æ¡£ååˆ›å»º")
                # åˆ›å»ºä¸€ä¸ªç©ºç´¢å¼•
                self._index = VectorStoreIndex.from_documents(
                    [],
                    storage_context=self.storage_context,
                )
        
        return self._index
    
    def clear_index(self):
        """æ¸…ç©ºç´¢å¼•"""
        try:
            # åˆ é™¤é›†åˆ
            self.chroma_client.delete_collection(name=self.collection_name)
            print(f"âœ… å·²åˆ é™¤é›†åˆ: {self.collection_name}")
            
            # é‡æ–°åˆ›å»ºé›†åˆ
            self.chroma_collection = self.chroma_client.get_or_create_collection(
                name=self.collection_name
            )
            self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
            self.storage_context = StorageContext.from_defaults(
                vector_store=self.vector_store
            )
            
            # é‡ç½®ç´¢å¼•
            self._index = None
            print("âœ… ç´¢å¼•å·²æ¸…ç©º")
            
        except Exception as e:
            print(f"âŒ æ¸…ç©ºç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def get_stats(self) -> dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            count = self.chroma_collection.count()
            return {
                "collection_name": self.collection_name,
                "document_count": count,
                "embedding_model": self.embedding_model_name,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
            }
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def search(self, query: str, top_k: int = 5) -> List[dict]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if self._index is None:
            self.get_index()
        
        retriever = self._index.as_retriever(similarity_top_k=top_k)
        nodes = retriever.retrieve(query)
        
        results = []
        for node in nodes:
            results.append({
                "text": node.node.text,
                "score": node.score,
                "metadata": node.node.metadata,
            })
        
        return results
    
    def incremental_update(
        self,
        added_docs: List[LlamaDocument],
        modified_docs: List[LlamaDocument],
        deleted_file_paths: List[str],
        metadata_manager=None
    ) -> dict:
        """æ‰§è¡Œå¢é‡æ›´æ–°
        
        Args:
            added_docs: æ–°å¢çš„æ–‡æ¡£åˆ—è¡¨
            modified_docs: ä¿®æ”¹çš„æ–‡æ¡£åˆ—è¡¨
            deleted_file_paths: åˆ é™¤çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨å®ä¾‹ï¼ˆç”¨äºæŸ¥è¯¢å‘é‡IDï¼‰
            
        Returns:
            æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "added": 0,
            "modified": 0,
            "deleted": 0,
            "errors": []
        }
        
        # ç¡®ä¿ç´¢å¼•å­˜åœ¨
        if self._index is None:
            self.get_index()
        
        # 1. å¤„ç†æ–°å¢
        if added_docs:
            try:
                added_count, added_vector_ids = self._add_documents(added_docs)
                stats["added"] = added_count
                print(f"âœ… æ–°å¢ {added_count} ä¸ªæ–‡æ¡£")
                
                # æ›´æ–°å…ƒæ•°æ®çš„å‘é‡ID
                if metadata_manager and added_docs:
                    for doc in added_docs:
                        file_path = doc.metadata.get("file_path", "")
                        if file_path and file_path in added_vector_ids:
                            owner = doc.metadata.get("repository", "").split("/")[0] if "/" in doc.metadata.get("repository", "") else ""
                            repo = doc.metadata.get("repository", "").split("/")[1] if "/" in doc.metadata.get("repository", "") else ""
                            branch = doc.metadata.get("branch", "main")
                            
                            if owner and repo:
                                metadata_manager.update_file_vector_ids(
                                    owner, repo, branch, file_path,
                                    added_vector_ids[file_path]
                                )
            except Exception as e:
                error_msg = f"æ–°å¢æ–‡æ¡£å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)
        
        # 2. å¤„ç†ä¿®æ”¹ï¼ˆå…ˆæ‰¹é‡åˆ é™¤æ—§çš„ï¼Œå†æ‰¹é‡æ·»åŠ æ–°çš„ï¼‰
        if modified_docs:
            try:
                # ä¼˜åŒ–ï¼šæ‰¹é‡æ”¶é›†æ‰€æœ‰éœ€è¦åˆ é™¤çš„å‘é‡ID
                all_vector_ids_to_delete = []
                for doc in modified_docs:
                    file_path = doc.metadata.get("file_path", "")
                    if file_path and metadata_manager:
                        # ä»å…ƒæ•°æ®ä¸­è·å–æ—§çš„å‘é‡ID
                        owner = doc.metadata.get("repository", "").split("/")[0] if "/" in doc.metadata.get("repository", "") else ""
                        repo = doc.metadata.get("repository", "").split("/")[1] if "/" in doc.metadata.get("repository", "") else ""
                        branch = doc.metadata.get("branch", "main")
                        
                        vector_ids = metadata_manager.get_file_vector_ids(owner, repo, branch, file_path)
                        if vector_ids:
                            all_vector_ids_to_delete.extend(vector_ids)
                
                # æ‰¹é‡åˆ é™¤æ‰€æœ‰æ—§å‘é‡ï¼ˆä¼˜åŒ–ï¼šä¸€æ¬¡æ€§åˆ é™¤ï¼‰
                deleted_vector_count = 0
                if all_vector_ids_to_delete:
                    # å»é‡
                    unique_vector_ids = list(set(all_vector_ids_to_delete))
                    # åˆ†æ‰¹åˆ é™¤ä»¥é¿å…å•æ¬¡åˆ é™¤è¿‡å¤šæ•°æ®
                    batch_delete_size = 100
                    for i in range(0, len(unique_vector_ids), batch_delete_size):
                        batch_ids = unique_vector_ids[i:i+batch_delete_size]
                        self._delete_vectors_by_ids(batch_ids)
                        deleted_vector_count += len(batch_ids)
                
                # æ‰¹é‡æ·»åŠ æ–°ç‰ˆæœ¬
                modified_count, modified_vector_ids = self._add_documents(modified_docs)
                stats["modified"] = modified_count
                print(f"âœ… æ›´æ–° {modified_count} ä¸ªæ–‡æ¡£ï¼ˆæ‰¹é‡åˆ é™¤ {deleted_vector_count} ä¸ªæ—§å‘é‡ï¼‰")
                
                # æ›´æ–°å…ƒæ•°æ®çš„å‘é‡ID
                if metadata_manager and modified_docs:
                    for doc in modified_docs:
                        file_path = doc.metadata.get("file_path", "")
                        if file_path and file_path in modified_vector_ids:
                            owner = doc.metadata.get("repository", "").split("/")[0] if "/" in doc.metadata.get("repository", "") else ""
                            repo = doc.metadata.get("repository", "").split("/")[1] if "/" in doc.metadata.get("repository", "") else ""
                            branch = doc.metadata.get("branch", "main")
                            
                            if owner and repo:
                                metadata_manager.update_file_vector_ids(
                                    owner, repo, branch, file_path,
                                    modified_vector_ids[file_path]
                                )
            except Exception as e:
                error_msg = f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)
        
        # 3. å¤„ç†åˆ é™¤
        if deleted_file_paths and metadata_manager:
            try:
                deleted_count = self._delete_documents(deleted_file_paths, metadata_manager)
                stats["deleted"] = deleted_count
                print(f"âœ… åˆ é™¤ {deleted_count} ä¸ªæ–‡æ¡£")
            except Exception as e:
                error_msg = f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)
        
        return stats
    
    def _add_documents(self, documents: List[LlamaDocument]) -> Tuple[int, Dict[str, List[str]]]:
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡æ’å…¥ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            (æˆåŠŸæ·»åŠ çš„æ–‡æ¡£æ•°é‡, æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDçš„æ˜ å°„)
        """
        if not documents:
            return 0, {}
        
        try:
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡æ’å…¥æ›¿ä»£é€ä¸ªæ’å…¥
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨insert_ref_docsæ‰¹é‡æ’å…¥
            try:
                self._index.insert_ref_docs(documents, show_progress=False)
                count = len(documents)
            except AttributeError:
                # å¦‚æœinsert_ref_docsä¸å¯ç”¨ï¼Œä½¿ç”¨èŠ‚ç‚¹æ‰¹é‡æ’å…¥
                from llama_index.core.node_parser import SentenceSplitter
                node_parser = SentenceSplitter(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap
                )
                # æ‰¹é‡åˆ†å—å¹¶æ’å…¥èŠ‚ç‚¹
                all_nodes = node_parser.get_nodes_from_documents(documents)
                for node in all_nodes:
                    self._index.insert(node)
                count = len(documents)
            except Exception as e:
                # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªæ’å…¥ï¼ˆä¿ç•™å®¹é”™èƒ½åŠ›ï¼‰
                logger.warning(f"æ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªæ’å…¥: {e}")
                count = 0
                for doc in documents:
                    try:
                        self._index.insert(doc)
                        count += 1
                    except Exception as insert_error:
                        print(f"âš ï¸  æ·»åŠ æ–‡æ¡£å¤±è´¥ [{doc.metadata.get('file_path', 'unknown')}]: {insert_error}")
                        logger.warning(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {insert_error}")
        except Exception as e:
            logger.error(f"æ‰¹é‡æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            print(f"âŒ æ‰¹é‡æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return 0, {}
        
        # ä¼˜åŒ–ï¼šæ‰¹é‡æŸ¥è¯¢å‘é‡IDæ˜ å°„
        file_paths = [doc.metadata.get("file_path", "") for doc in documents 
                     if doc.metadata.get("file_path")]
        vector_ids_map = self._get_vector_ids_batch(file_paths)
        
        return count, vector_ids_map
    
    def _delete_documents(self, file_paths: List[str], metadata_manager) -> int:
        """æ‰¹é‡åˆ é™¤æ–‡æ¡£
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨å®ä¾‹
            
        Returns:
            æˆåŠŸåˆ é™¤çš„æ–‡æ¡£æ•°é‡
        """
        deleted_count = 0
        
        for file_path in file_paths:
            # éœ€è¦ä»æ–‡æ¡£å…ƒæ•°æ®ä¸­æå–ä»“åº“ä¿¡æ¯
            # è¿™é‡Œå‡è®¾æ–‡ä»¶è·¯å¾„åŒ…å«ä»“åº“ä¿¡æ¯
            # å®é™…ä½¿ç”¨æ—¶éœ€è¦ä¼ é€’å®Œæ•´çš„ä»“åº“ä¿¡æ¯
            # æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
            pass
        
        return deleted_count
    
    def _get_vector_ids_by_metadata(self, file_path: str) -> List[str]:
        """é€šè¿‡æ–‡ä»¶è·¯å¾„æŸ¥è¯¢å¯¹åº”çš„å‘é‡IDåˆ—è¡¨
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å‘é‡IDåˆ—è¡¨
        """
        if not file_path:
            return []
        
        try:
            # æŸ¥è¯¢ Chroma collectionï¼ŒåŒ¹é… file_path
            results = self.chroma_collection.get(
                where={"file_path": file_path}
            )
            return results.get('ids', []) if results else []
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢å‘é‡IDå¤±è´¥ [{file_path}]: {e}")
            return []
    
    def _get_vector_ids_batch(self, file_paths: List[str]) -> Dict[str, List[str]]:
        """æ‰¹é‡æŸ¥è¯¢å‘é‡IDæ˜ å°„ï¼ˆä¼˜åŒ–ï¼šå‡å°‘æŸ¥è¯¢æ¬¡æ•°ï¼‰
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDåˆ—è¡¨çš„æ˜ å°„å­—å…¸
        """
        if not file_paths:
            return {}
        
        # å»é‡
        unique_paths = list(set(file_paths))
        vector_ids_map = {}
        
        try:
            # Chroma ä¸æ”¯æŒæ‰¹é‡ where æ¡ä»¶æŸ¥è¯¢ï¼Œä½†ä»å¯ä»¥ä¼˜åŒ–ï¼š
            # 1. å‡å°‘é‡å¤æŸ¥è¯¢ï¼ˆé€šè¿‡å»é‡ï¼‰
            # 2. æ‰¹é‡è·å–æ‰€æœ‰æ•°æ®ç„¶åè¿‡æ»¤ï¼ˆé€‚ç”¨äºæ•°æ®é‡ä¸å¤§çš„æƒ…å†µï¼‰
            # 3. æˆ–è€…ç»§ç»­é€ä¸ªæŸ¥è¯¢ä½†å»æ‰é‡å¤
            
            # æ–¹æ¡ˆï¼šåˆ†æ‰¹æŸ¥è¯¢ä»¥é¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šæ•°æ®
            batch_size = 50  # æ¯æ‰¹æŸ¥è¯¢50ä¸ªæ–‡ä»¶è·¯å¾„
            total_results = 0
            
            for i in range(0, len(unique_paths), batch_size):
                batch_paths = unique_paths[i:i+batch_size]
                for file_path in batch_paths:
                    vector_ids = self._get_vector_ids_by_metadata(file_path)
                    if vector_ids:
                        vector_ids_map[file_path] = vector_ids
                        total_results += len(vector_ids)
            
            logger.debug(
                f"æ‰¹é‡æŸ¥è¯¢å‘é‡ID: "
                f"è¾“å…¥{len(file_paths)}ä¸ªè·¯å¾„(å»é‡å{len(unique_paths)}ä¸ª), "
                f"æ‰¾åˆ°{len(vector_ids_map)}ä¸ªæ–‡ä»¶, "
                f"å…±{total_results}ä¸ªå‘é‡"
            )
        except Exception as e:
            logger.error(f"æ‰¹é‡æŸ¥è¯¢å‘é‡IDå¤±è´¥: {e}")
            # å›é€€åˆ°é€ä¸ªæŸ¥è¯¢
            for file_path in unique_paths:
                try:
                    vector_ids = self._get_vector_ids_by_metadata(file_path)
                    if vector_ids:
                        vector_ids_map[file_path] = vector_ids
                except Exception as query_error:
                    logger.warning(f"æŸ¥è¯¢å•ä¸ªå‘é‡IDå¤±è´¥ [{file_path}]: {query_error}")
        
        return vector_ids_map
    
    def _delete_vectors_by_ids(self, vector_ids: List[str]):
        """æ ¹æ®å‘é‡IDåˆ é™¤å‘é‡
        
        Args:
            vector_ids: å‘é‡IDåˆ—è¡¨
        """
        if not vector_ids:
            return
        
        try:
            self.chroma_collection.delete(ids=vector_ids)
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤å‘é‡å¤±è´¥: {e}")
            raise
    
    def get_node_ids_for_document(self, doc_id: str) -> List[str]:
        """è·å–æ–‡æ¡£å¯¹åº”çš„æ‰€æœ‰èŠ‚ç‚¹ID
        
        Args:
            doc_id: æ–‡æ¡£ID
            
        Returns:
            èŠ‚ç‚¹IDåˆ—è¡¨
        """
        # LlamaIndex ä½¿ç”¨èŠ‚ç‚¹ï¼ˆNodeï¼‰çš„æ¦‚å¿µï¼Œæ¯ä¸ªæ–‡æ¡£åˆ†å—åä¼šç”Ÿæˆå¤šä¸ªèŠ‚ç‚¹
        # éœ€è¦æŸ¥è¯¢ Chroma collection æ¥è·å–ä¸æ–‡æ¡£ç›¸å…³çš„æ‰€æœ‰èŠ‚ç‚¹
        try:
            # æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼Œç„¶åè¿‡æ»¤å‡ºåŒ¹é…çš„
            # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå¤§è§„æ¨¡æ•°æ®æ—¶éœ€è¦ä¼˜åŒ–
            result = self.chroma_collection.get()
            
            if not result or 'ids' not in result:
                return []
            
            # è¿”å›æ‰€æœ‰IDï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥æ ¹æ®å…ƒæ•°æ®è¿‡æ»¤ï¼‰
            return result['ids']
        except Exception as e:
            print(f"âš ï¸  æŸ¥è¯¢èŠ‚ç‚¹IDå¤±è´¥: {e}")
            return []
    
    def preload_wikipedia_concepts(
        self,
        concept_keywords: List[str],
        lang: str = "zh",
        show_progress: bool = True
    ) -> int:
        """é¢„åŠ è½½æ ¸å¿ƒæ¦‚å¿µçš„ç»´åŸºç™¾ç§‘å†…å®¹åˆ°ç´¢å¼•
        
        Args:
            concept_keywords: æ¦‚å¿µå…³é”®è¯åˆ—è¡¨ï¼ˆç»´åŸºç™¾ç§‘é¡µé¢æ ‡é¢˜ï¼‰
            lang: è¯­è¨€ä»£ç ï¼ˆzh=ä¸­æ–‡, en=è‹±æ–‡ï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            æˆåŠŸç´¢å¼•çš„é¡µé¢æ•°é‡
            
        Examples:
            >>> index_manager.preload_wikipedia_concepts(
            ...     ["ç³»ç»Ÿç§‘å­¦", "é’±å­¦æ£®", "æ§åˆ¶è®º"],
            ...     lang="zh"
            ... )
        """
        if not concept_keywords:
            print("âš ï¸  æ¦‚å¿µå…³é”®è¯åˆ—è¡¨ä¸ºç©º")
            return 0
        
        try:
            from src.data_loader import load_documents_from_wikipedia
            
            if show_progress:
                print(f"ğŸ“– é¢„åŠ è½½ {len(concept_keywords)} ä¸ªç»´åŸºç™¾ç§‘æ¦‚å¿µ...")
            
            logger.info(f"å¼€å§‹é¢„åŠ è½½ç»´åŸºç™¾ç§‘æ¦‚å¿µ: {concept_keywords}")
            
            # åŠ è½½ç»´åŸºç™¾ç§‘é¡µé¢
            wiki_docs = load_documents_from_wikipedia(
                pages=concept_keywords,
                lang=lang,
                auto_suggest=True,
                clean=True,
                show_progress=show_progress
            )
            
            if not wiki_docs:
                if show_progress:
                    print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ç»´åŸºç™¾ç§‘å†…å®¹")
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç»´åŸºç™¾ç§‘å†…å®¹")
                return 0
            
            # æ„å»ºç´¢å¼•
            self.build_index(wiki_docs, show_progress=show_progress)
            
            if show_progress:
                print(f"âœ… å·²ç´¢å¼• {len(wiki_docs)} ä¸ªç»´åŸºç™¾ç§‘é¡µé¢")
            
            logger.info(f"æˆåŠŸé¢„åŠ è½½ {len(wiki_docs)} ä¸ªç»´åŸºç™¾ç§‘é¡µé¢")
            
            return len(wiki_docs)
            
        except Exception as e:
            print(f"âŒ é¢„åŠ è½½ç»´åŸºç™¾ç§‘å¤±è´¥: {e}")
            logger.error(f"é¢„åŠ è½½ç»´åŸºç™¾ç§‘å¤±è´¥: {e}")
            return 0


def create_index_from_directory(
    directory_path: str | Path,
    collection_name: Optional[str] = None,
    recursive: bool = True
) -> IndexManager:
    """ä»ç›®å½•åˆ›å»ºç´¢å¼•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        directory_path: æ–‡æ¡£ç›®å½•è·¯å¾„
        collection_name: é›†åˆåç§°
        recursive: æ˜¯å¦é€’å½’åŠ è½½
        
    Returns:
        IndexManagerå¯¹è±¡
    """
    from src.data_loader import load_documents_from_directory
    
    # åŠ è½½æ–‡æ¡£
    print(f"ğŸ“‚ ä»ç›®å½•åŠ è½½æ–‡æ¡£: {directory_path}")
    documents = load_documents_from_directory(directory_path, recursive=recursive)
    
    if not documents:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return IndexManager(collection_name=collection_name)
    
    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index_manager = IndexManager(collection_name=collection_name)
    
    # æ„å»ºç´¢å¼•
    index_manager.build_index(documents)
    
    return index_manager


def create_index_from_urls(
    urls: List[str],
    collection_name: Optional[str] = None
) -> IndexManager:
    """ä»URLåˆ—è¡¨åˆ›å»ºç´¢å¼•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        urls: URLåˆ—è¡¨
        collection_name: é›†åˆåç§°
        
    Returns:
        IndexManagerå¯¹è±¡
    """
    from src.data_loader import load_documents_from_urls
    
    # åŠ è½½æ–‡æ¡£
    print(f"ğŸŒ ä» {len(urls)} ä¸ªURLåŠ è½½æ–‡æ¡£")
    documents = load_documents_from_urls(urls)
    
    if not documents:
        print("âš ï¸  æœªæˆåŠŸåŠ è½½ä»»ä½•ç½‘é¡µ")
        return IndexManager(collection_name=collection_name)
    
    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index_manager = IndexManager(collection_name=collection_name)
    
    # æ„å»ºç´¢å¼•
    index_manager.build_index(documents)
    
    return index_manager


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=== æµ‹è¯•ç´¢å¼•æ„å»º ===\n")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        LlamaDocument(
            text="ç³»ç»Ÿç§‘å­¦æ˜¯ç ”ç©¶ç³»ç»Ÿçš„ä¸€èˆ¬è§„å¾‹å’Œæ–¹æ³•çš„ç§‘å­¦ã€‚",
            metadata={"title": "ç³»ç»Ÿç§‘å­¦ç®€ä»‹", "source": "test"}
        ),
        LlamaDocument(
            text="é’±å­¦æ£®æ˜¯ä¸­å›½ç³»ç»Ÿç§‘å­¦çš„åˆ›å»ºè€…ä¹‹ä¸€ï¼Œä»–æå‡ºäº†ç³»ç»Ÿå·¥ç¨‹çš„ç†è®ºä½“ç³»ã€‚",
            metadata={"title": "é’±å­¦æ£®ä¸ç³»ç»Ÿç§‘å­¦", "source": "test"}
        ),
    ]
    
    # åˆ›å»ºç´¢å¼•
    index_manager = IndexManager(collection_name="test_collection")
    index_manager.build_index(test_docs)
    
    # æµ‹è¯•æœç´¢
    print("\n=== æµ‹è¯•æœç´¢ ===")
    results = index_manager.search("é’±å­¦æ£®", top_k=2)
    for i, result in enumerate(results, 1):
        print(f"\nç»“æœ {i}:")
        print(f"ç›¸ä¼¼åº¦: {result['score']:.4f}")
        print(f"å†…å®¹: {result['text']}")
    
    # æ¸…ç†æµ‹è¯•æ•°æ®
    index_manager.clear_index()
    print("\nâœ… æµ‹è¯•å®Œæˆ")

