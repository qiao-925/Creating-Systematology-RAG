"""
ç´¢å¼•æ„å»ºæ¨¡å—
è´Ÿè´£æ„å»ºå’Œç®¡ç†å‘é‡ç´¢å¼•ï¼Œé›†æˆChromaå‘é‡æ•°æ®åº“
"""

import os
from pathlib import Path
from typing import List, Optional, Tuple, Dict

import chromadb
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    Settings,
)
from llama_index.core.schema import Document as LlamaDocument
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore

from src.config import config, get_gpu_device, is_gpu_available, get_device_status
from src.logger import setup_logger

logger = setup_logger('indexer')

# å…¨å±€ embedding æ¨¡å‹ç¼“å­˜
_global_embed_model: Optional[HuggingFaceEmbedding] = None


def _setup_huggingface_env():
    """é…ç½® HuggingFace ç¯å¢ƒå˜é‡ï¼ˆé•œåƒå’Œç¦»çº¿æ¨¡å¼ï¼‰
    
    æ³¨æ„ï¼šç¯å¢ƒå˜é‡å·²åœ¨ src/__init__.py ä¸­é¢„è®¾ï¼Œè¿™é‡Œä»…ç”¨äºæ—¥å¿—è®°å½•å’Œç¡®è®¤
    """
    # è®¾ç½®é•œåƒåœ°å€
    if config.HF_ENDPOINT:
        os.environ['HF_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HUGGINGFACE_HUB_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HF_HUB_ENDPOINT'] = config.HF_ENDPOINT  # æ–°ç‰ˆæœ¬ä½¿ç”¨è¿™ä¸ª
        logger.info(f"ğŸŒ ä½¿ç”¨ HuggingFace é•œåƒ: {config.HF_ENDPOINT}")
    
    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    if config.HF_OFFLINE_MODE:
        os.environ['HF_HUB_OFFLINE'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '1'
        logger.info(f"ğŸ“´ å¯ç”¨ç¦»çº¿æ¨¡å¼ï¼ˆä»…ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")
    else:
        # ç¡®ä¿ç¦»çº¿æ¨¡å¼å…³é—­
        os.environ.pop('HF_HUB_OFFLINE', None)
        os.environ.pop('TRANSFORMERS_OFFLINE', None)


def load_embedding_model(model_name: Optional[str] = None, force_reload: bool = False) -> HuggingFaceEmbedding:
    """åŠ è½½ Embedding æ¨¡å‹ï¼ˆæ”¯æŒå…¨å±€å•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼ˆå³ä½¿å·²ç¼“å­˜ï¼‰
        
    Returns:
        HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    
    model_name = model_name or config.EMBEDDING_MODEL
    
    # å¦‚æœå·²ç»åŠ è½½è¿‡ä¸”æ¨¡å‹åç§°ç›¸åŒï¼Œç›´æ¥è¿”å›ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°åŠ è½½ï¼‰
    if _global_embed_model is not None and not force_reload:
        # æ£€æŸ¥ç¼“å­˜çš„æ¨¡å‹åç§°æ˜¯å¦ä¸æ–°é…ç½®ä¸€è‡´
        cached_model_name = getattr(_global_embed_model, 'model_name', None)
        if cached_model_name == model_name:
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„ Embedding æ¨¡å‹ï¼ˆå…¨å±€å˜é‡ï¼‰: {model_name}")
            logger.info(f"   æ¨¡å‹å¯¹è±¡ID: {id(_global_embed_model)}")
            return _global_embed_model
        else:
            # æ¨¡å‹åç§°ä¸ä¸€è‡´ï¼Œæ¸…é™¤ç¼“å­˜å¹¶é‡æ–°åŠ è½½
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹é…ç½®å˜æ›´: {cached_model_name} -> {model_name}")
            logger.info(f"   æ¸…é™¤æ—§æ¨¡å‹ç¼“å­˜ï¼Œé‡æ–°åŠ è½½æ–°æ¨¡å‹")
            _global_embed_model = None
    
    # å¦‚æœéœ€è¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œæ¸…é™¤ç¼“å­˜
    if force_reload:
        logger.info(f"ğŸ”„ å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å‹")
        _global_embed_model = None
    
    # é…ç½® HuggingFace ç¯å¢ƒå˜é‡
    _setup_huggingface_env()
    
    # åŠ è½½æ¨¡å‹
    logger.info(f"ğŸ“¦ å¼€å§‹åŠ è½½ Embedding æ¨¡å‹ï¼ˆå…¨æ–°åŠ è½½ï¼‰: {model_name}")
    
    try:
        # æ˜¾å¼æŒ‡å®šç¼“å­˜ç›®å½•ä»¥ç¡®ä¿ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        cache_folder = str(Path.home() / ".cache" / "huggingface")
        
        # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
        device = get_gpu_device()
        import torch
        
        # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
        if device.startswith("cuda") and is_gpu_available():
            device_name = torch.cuda.get_device_name()
            cuda_version = torch.version.cuda
            logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ:")
            logger.info(f"   è®¾å¤‡: {device}")
            logger.info(f"   GPUåç§°: {device_name}")
            logger.info(f"   CUDAç‰ˆæœ¬: {cuda_version}")
        else:
            logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
            logger.info("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
            logger.info("ğŸ’¡ å»ºè®®: å¦‚æœ‰GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        
        # æ„å»ºæ¨¡å‹å‚æ•°
        model_kwargs = {
            "trust_remote_code": True,
            "cache_folder": cache_folder,
        }
        
        _global_embed_model = HuggingFaceEmbedding(
            model_name=model_name,
            embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†ï¼Œæå‡æ€§èƒ½
            max_length=config.EMBED_MAX_LENGTH,  # è®¾ç½®æœ€å¤§é•¿åº¦
            **model_kwargs
        )
        
        # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPUï¼ˆå¦‚æœä¸æ”¯æŒé€šè¿‡å‚æ•°æŒ‡å®šï¼‰
        try:
            if device.startswith("cuda") and is_gpu_available():
                # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                if hasattr(_global_embed_model, '_model') and hasattr(_global_embed_model._model, 'to'):
                    _global_embed_model._model = _global_embed_model._model.to(device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                elif hasattr(_global_embed_model, 'model') and hasattr(_global_embed_model.model, 'to'):
                    _global_embed_model.model = _global_embed_model.model.to(device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
            else:
                logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
            logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
        
        logger.info(f"âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_folder}")
        if device.startswith("cuda"):
            logger.info(f"âš¡ GPUåŠ é€Ÿæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (æ¨è10-50)")
        else:
            logger.info(f"ğŸŒ CPUæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (å»ºè®®è°ƒæ•´ä¸º5-10)")
        logger.info(f"ğŸ“ æœ€å¤§é•¿åº¦: {config.EMBED_MAX_LENGTH}")
    except Exception as e:
        # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ä¸”ç¼ºå°‘ç¼“å­˜ï¼Œå°è¯•åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
        if config.HF_OFFLINE_MODE and "offline" in str(e).lower():
            logger.warning(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æœ¬åœ°æ— ç¼“å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼å°è¯•ä¸‹è½½")
            os.environ.pop('HF_HUB_OFFLINE', None)
            
            try:
                cache_folder = str(Path.home() / ".cache" / "huggingface")
                
                # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                device = get_gpu_device()
                import torch
                
                # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                if device.startswith("cuda") and is_gpu_available():
                    device_name = torch.cuda.get_device_name()
                    logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ: {device_name} ({device})")
                else:
                    logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                    logger.info("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                
                # æ„å»ºæ¨¡å‹å‚æ•°
                model_kwargs = {
                    "trust_remote_code": True,
                    "cache_folder": cache_folder,
                }
                
                _global_embed_model = HuggingFaceEmbedding(
                    model_name=model_name,
                    embed_batch_size=config.EMBED_BATCH_SIZE,
                    max_length=config.EMBED_MAX_LENGTH,
                    **model_kwargs
                )
                
                # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPU
                try:
                    if device.startswith("cuda") and is_gpu_available():
                        # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                        if hasattr(_global_embed_model, '_model') and hasattr(_global_embed_model._model, 'to'):
                            _global_embed_model._model = _global_embed_model._model.to(device)
                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                        elif hasattr(_global_embed_model, 'model') and hasattr(_global_embed_model.model, 'to'):
                            _global_embed_model.model = _global_embed_model.model.to(device)
                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                    else:
                        logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                except Exception as e:
                    logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                    logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                
                logger.info(f"âœ… Embedding æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ: {model_name}")
                if device.startswith("cuda"):
                    logger.info(f"âš¡ GPUåŠ é€Ÿæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (æ¨è10-50)")
                else:
                    logger.info(f"ğŸŒ CPUæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (å»ºè®®è°ƒæ•´ä¸º5-10)")
                logger.info(f"ğŸ“ æœ€å¤§é•¿åº¦: {config.EMBED_MAX_LENGTH}")
            except Exception as retry_error:
                logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {retry_error}")
                raise
        else:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    return _global_embed_model


def set_global_embed_model(model: HuggingFaceEmbedding):
    """è®¾ç½®å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Args:
        model: HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    _global_embed_model = model
    logger.debug("ğŸ”§ è®¾ç½®å…¨å±€ Embedding æ¨¡å‹")


def get_global_embed_model() -> Optional[HuggingFaceEmbedding]:
    """è·å–å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Returns:
        å·²åŠ è½½çš„æ¨¡å‹å®ä¾‹ï¼Œå¦‚æœæœªåŠ è½½åˆ™è¿”å› None
    """
    return _global_embed_model


def clear_embedding_model_cache():
    """æ¸…é™¤å…¨å±€ Embedding æ¨¡å‹ç¼“å­˜
    
    ç”¨äºæ¨¡å‹åˆ‡æ¢æˆ–å¼ºåˆ¶é‡æ–°åŠ è½½åœºæ™¯
    """
    global _global_embed_model
    if _global_embed_model is not None:
        logger.info(f"ğŸ§¹ æ¸…é™¤ Embedding æ¨¡å‹ç¼“å­˜")
        _global_embed_model = None


def get_embedding_model_status() -> dict:
    """è·å– Embedding æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    
    Returns:
        åŒ…å«æ¨¡å‹çŠ¶æ€çš„å­—å…¸ï¼š
        {
            "loaded": bool,              # æ˜¯å¦å·²åŠ è½½
            "model_name": str,           # æ¨¡å‹åç§°
            "cache_dir": str,            # ç¼“å­˜ç›®å½•
            "cache_exists": bool,        # æœ¬åœ°ç¼“å­˜æ˜¯å¦å­˜åœ¨
            "offline_mode": bool,        # æ˜¯å¦ç¦»çº¿æ¨¡å¼
            "mirror": str,               # é•œåƒåœ°å€
        }
    """
    import os
    from pathlib import Path
    
    model_name = config.EMBEDDING_MODEL
    
    # æ£€æŸ¥ç¼“å­˜ç›®å½•
    cache_root = Path.home() / ".cache" / "huggingface" / "hub"
    # HuggingFace ç¼“å­˜æ ¼å¼: models--{org}--{model}
    model_cache_name = model_name.replace("/", "--")
    cache_dir = cache_root / f"models--{model_cache_name}"
    cache_exists = cache_dir.exists()
    
    return {
        "loaded": _global_embed_model is not None,
        "model_name": model_name,
        "cache_dir": str(cache_dir),
        "cache_exists": cache_exists,
        "offline_mode": config.HF_OFFLINE_MODE,
        "mirror": config.HF_ENDPOINT if config.HF_ENDPOINT else "huggingface.co (å®˜æ–¹)",
    }


class IndexManager:
    """ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(
        self,
        collection_name: Optional[str] = None,
        persist_dir: Optional[Path] = None,
        embedding_model: Optional[str] = None,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        embed_model_instance: Optional[HuggingFaceEmbedding] = None,
    ):
        """åˆå§‹åŒ–ç´¢å¼•ç®¡ç†å™¨
        
        Args:
            collection_name: Chromaé›†åˆåç§°
            persist_dir: å‘é‡å­˜å‚¨æŒä¹…åŒ–ç›®å½•
            embedding_model: Embeddingæ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å—é‡å å¤§å°
            embed_model_instance: é¢„åŠ è½½çš„Embeddingæ¨¡å‹å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        # ä½¿ç”¨é…ç½®æˆ–é»˜è®¤å€¼
        self.collection_name = collection_name or config.CHROMA_COLLECTION_NAME
        self.persist_dir = persist_dir or config.VECTOR_STORE_PATH
        self.embedding_model_name = embedding_model or config.EMBEDDING_MODEL
        self.chunk_size = chunk_size or config.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or config.CHUNK_OVERLAP
        
        # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
        self.persist_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„å®ä¾‹ï¼‰
        if embed_model_instance is not None:
            print(f"âœ… ä½¿ç”¨é¢„åŠ è½½çš„Embeddingæ¨¡å‹: {self.embedding_model_name}")
            self.embed_model = embed_model_instance
        else:
            # æ£€æŸ¥å…¨å±€ç¼“å­˜ä¸­çš„æ¨¡å‹æ˜¯å¦åŒ¹é…å½“å‰é…ç½®
            global _global_embed_model
            cached_model_name = None
            if _global_embed_model is not None:
                cached_model_name = getattr(_global_embed_model, 'model_name', None)
            
            # å¦‚æœæ¨¡å‹åç§°ä¸åŒ¹é…ï¼Œæ¸…ç†ç¼“å­˜
            if cached_model_name and cached_model_name != self.embedding_model_name:
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹é…ç½®å˜æ›´: {cached_model_name} -> {self.embedding_model_name}")
                logger.info(f"   æ¸…ç†æ—§æ¨¡å‹ç¼“å­˜å¹¶é‡æ–°åŠ è½½")
                clear_embedding_model_cache()
            
            # å°è¯•ä½¿ç”¨å…¨å±€ç¼“å­˜çš„æ¨¡å‹ï¼ˆå¦‚æœåŒ¹é…ï¼‰
            if _global_embed_model is not None:
                try:
                    # éªŒè¯ç¼“å­˜çš„æ¨¡å‹æ˜¯å¦çœŸçš„åŒ¹é…ï¼ˆé€šè¿‡å®é™…è®¡ç®—ç»´åº¦ï¼‰
                    test_embedding = _global_embed_model.get_query_embedding("test")
                    cached_dim = len(test_embedding)
                    logger.info(f"âœ… ä½¿ç”¨å…¨å±€ç¼“å­˜çš„Embeddingæ¨¡å‹: {self.embedding_model_name} (ç»´åº¦: {cached_dim})")
                    self.embed_model = _global_embed_model
                except Exception as e:
                    logger.warning(f"âš ï¸  éªŒè¯ç¼“å­˜æ¨¡å‹å¤±è´¥ï¼Œé‡æ–°åŠ è½½: {e}")
                    clear_embedding_model_cache()
                    # ç»§ç»­ä¸‹é¢çš„åŠ è½½æµç¨‹
                    self.embed_model = None
            else:
                self.embed_model = None
            
            # å¦‚æœç¼“å­˜ä¸å¯ç”¨ï¼ŒåŠ è½½æ–°æ¨¡å‹
            if self.embed_model is None:
                # é…ç½® HuggingFace ç¯å¢ƒå˜é‡
                _setup_huggingface_env()
                
                print(f"ğŸ“¦ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {self.embedding_model_name}")
                
                # ä½¿ç”¨load_embedding_modelå‡½æ•°ä»¥ç¡®ä¿ç¼“å­˜ç®¡ç†æ­£ç¡®
                try:
                    self.embed_model = load_embedding_model(
                        model_name=self.embedding_model_name,
                        force_reload=False  # å¦‚æœç¼“å­˜åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨
                    )
                    logger.info(f"âœ… é€šè¿‡load_embedding_modelåŠ è½½æ¨¡å‹: {self.embedding_model_name}")
                    # å¦‚æœæˆåŠŸåŠ è½½ï¼Œè·³è¿‡ç›´æ¥åŠ è½½çš„ä»£ç å—
                except Exception as e:
                    logger.warning(f"âš ï¸  load_embedding_modelå¤±è´¥ï¼Œä½¿ç”¨ç›´æ¥åŠ è½½æ–¹å¼: {e}")
                    # å›é€€åˆ°ç›´æ¥åŠ è½½æ–¹å¼
                    try:
                        cache_folder = str(Path.home() / ".cache" / "huggingface")
                        
                        # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                        device = get_gpu_device()
                        import torch
                        
                        # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                        if device.startswith("cuda") and is_gpu_available():
                            device_name = torch.cuda.get_device_name()
                            cuda_version = torch.version.cuda
                            print(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ:")
                            print(f"   è®¾å¤‡: {device}")
                            print(f"   GPUåç§°: {device_name}")
                            print(f"   CUDAç‰ˆæœ¬: {cuda_version}")
                            logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPU: {device_name} ({device})")
                        else:
                            print("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                            print("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                            logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                            logger.info("ğŸ’¡ å»ºè®®: å¦‚æœ‰GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æœ€ä½³æ€§èƒ½")
                        
                        # æ„å»ºæ¨¡å‹å‚æ•°
                        model_kwargs = {
                            "trust_remote_code": True,
                            "cache_folder": cache_folder,
                        }
                        
                        self.embed_model = HuggingFaceEmbedding(
                            model_name=self.embedding_model_name,
                            embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†
                            max_length=config.EMBED_MAX_LENGTH,
                            **model_kwargs
                        )
                        
                        # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPUï¼ˆå¦‚æœä¸æ”¯æŒé€šè¿‡å‚æ•°æŒ‡å®šï¼‰
                        try:
                            if device.startswith("cuda") and is_gpu_available():
                                # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                                if hasattr(self.embed_model, '_model') and hasattr(self.embed_model._model, 'to'):
                                    self.embed_model._model = self.embed_model._model.to(device)
                                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                                elif hasattr(self.embed_model, 'model') and hasattr(self.embed_model.model, 'to'):
                                    self.embed_model.model = self.embed_model.model.to(device)
                                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                            else:
                                logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                        except Exception as e:
                            logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                            logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                        
                        if device.startswith("cuda"):
                            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (GPUåŠ é€Ÿ, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE})")
                        else:
                            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (CPUæ¨¡å¼, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE}, å»ºè®®è°ƒæ•´ä¸º5-10)")
                    except Exception as load_error:
                        # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ä¸”ç¼ºå°‘ç¼“å­˜ï¼Œå°è¯•åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
                        if config.HF_OFFLINE_MODE and "offline" in str(load_error).lower():
                            print(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æœ¬åœ°æ— ç¼“å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼å°è¯•ä¸‹è½½...")
                            os.environ.pop('HF_HUB_OFFLINE', None)
                            
                            try:
                                cache_folder = str(Path.home() / ".cache" / "huggingface")
                                
                                # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                                device = get_gpu_device()
                                import torch
                                
                                # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                                if device.startswith("cuda") and is_gpu_available():
                                    device_name = torch.cuda.get_device_name()
                                    print(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ: {device_name} ({device})")
                                    logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPU: {device_name} ({device})")
                                else:
                                    print("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                                    print("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                                    logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                                
                                # æ„å»ºæ¨¡å‹å‚æ•°
                                model_kwargs = {
                                    "trust_remote_code": True,
                                    "cache_folder": cache_folder,
                                }
                                
                                self.embed_model = HuggingFaceEmbedding(
                                    model_name=self.embedding_model_name,
                                    embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†
                                    max_length=config.EMBED_MAX_LENGTH,
                                    **model_kwargs
                                )
                                
                                # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPU
                                try:
                                    if device.startswith("cuda") and is_gpu_available():
                                        if hasattr(self.embed_model, '_model') and hasattr(self.embed_model._model, 'to'):
                                            self.embed_model._model = self.embed_model._model.to(device)
                                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                                        elif hasattr(self.embed_model, 'model') and hasattr(self.embed_model.model, 'to'):
                                            self.embed_model.model = self.embed_model.model.to(device)
                                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                                    else:
                                        logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                                except Exception as e:
                                    logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                                    logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                                
                                if device.startswith("cuda"):
                                    print(f"âœ… æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ (GPUåŠ é€Ÿ, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE})")
                                else:
                                    print(f"âœ… æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ (CPUæ¨¡å¼, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE}, å»ºè®®è°ƒæ•´ä¸º5-10)")
                            except Exception as retry_error:
                                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {retry_error}")
                                raise
                        else:
                            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {load_error}")
                            raise
        
        # é…ç½®å…¨å±€Settings
        Settings.embed_model = self.embed_model
        Settings.chunk_size = self.chunk_size
        Settings.chunk_overlap = self.chunk_overlap
        
        # åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
        print(f"ğŸ—„ï¸  åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“: {self.persist_dir}")
        self.chroma_client = chromadb.PersistentClient(path=str(self.persist_dir))
        
        # æ£€æµ‹å¹¶ä¿®å¤embeddingç»´åº¦ä¸åŒ¹é…é—®é¢˜
        self._ensure_collection_dimension_match()
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
        
        # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )
        
        # ç´¢å¼•å¯¹è±¡ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._index: Optional[VectorStoreIndex] = None
        
        print("âœ… ç´¢å¼•ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def build_index(
        self,
        documents: List[LlamaDocument],
        show_progress: bool = True,
        cache_manager=None,
        task_id: Optional[str] = None
    ) -> Tuple[VectorStoreIndex, Dict[str, List[str]]]:
        """æ„å»ºæˆ–æ›´æ–°ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            cache_manager: ç¼“å­˜ç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼Œç”¨äºç¼“å­˜ï¼‰
            
        Returns:
            (VectorStoreIndexå¯¹è±¡, æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDçš„æ˜ å°„)
        """
        import time
        import hashlib
        import json
        start_time = time.time()
        
        if not documents:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ç´¢å¼•")
            return self.get_index(), {}
        
        # æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥æ¯ä¸ªæ–‡æ¡£æ˜¯å¦å·²å‘é‡åŒ–ï¼Œåªå¤„ç†æœªå®Œæˆçš„æ–‡æ¡£
        # è¿™ä¸ªåŠŸèƒ½é»˜è®¤å¯ç”¨ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨éƒ½èƒ½æé«˜æ€§èƒ½
        documents_to_process, already_vectorized = self._filter_vectorized_documents(documents)
        
        if already_vectorized > 0:
            logger.info(f"âœ… æ£€æµ‹åˆ° {already_vectorized} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å¤„ç†")
            print(f"ğŸ“Š æ–­ç‚¹ç»­ä¼ : {already_vectorized}/{len(documents)} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œå‰©ä½™ {len(documents_to_process)} ä¸ªå¾…å¤„ç†")
        
        # å¦‚æœæ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡æ¡£ï¼Œç›´æ¥è¿”å›
        if not documents_to_process:
            logger.info(f"âœ… æ‰€æœ‰æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å‘é‡åŒ–æ­¥éª¤")
            index = self.get_index()
            vector_ids_map = self._get_vector_ids_batch(
                [doc.metadata.get("file_path", "") for doc in documents 
                 if doc.metadata.get("file_path")]
            )
            
            # å¦‚æœæä¾›äº†ç¼“å­˜ç®¡ç†å™¨ï¼Œæ›´æ–°ç¼“å­˜çŠ¶æ€
            if cache_manager and task_id:
                from src.config import config
                if config.ENABLE_CACHE:
                    docs_hash = self._compute_documents_hash(documents)
                    step_name = cache_manager.STEP_VECTORIZE
                    cache_manager.mark_step_completed(
                        task_id=task_id,
                        step_name=step_name,
                        input_hash=docs_hash,
                        vector_count=self.chroma_collection.count() if hasattr(self, 'chroma_collection') else 0,
                        collection_name=self.collection_name
                    )
            
            return index, vector_ids_map
        
        # æ›´æ–°å¾…å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
        documents = documents_to_process
        
        # è·å–å½“å‰è®¾å¤‡ä¿¡æ¯
        device = get_gpu_device()
        
        print(f"\nğŸ”¨ å¼€å§‹æ„å»ºç´¢å¼•ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
        print(f"   åˆ†å—å‚æ•°: size={self.chunk_size}, overlap={self.chunk_overlap}")
        
        # è¾“å‡ºè®¾å¤‡ä¿¡æ¯
        if device.startswith("cuda"):
            import torch
            device_name = torch.cuda.get_device_name()
            print(f"ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} âš¡ GPUåŠ é€Ÿæ¨¡å¼")
            print(f"   GPU: {device_name}")
            print(f"   æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (GPUæ¨è10-50)")
            logger.info(f"ğŸ“Š ç´¢å¼•æ„å»ºä½¿ç”¨GPU: {device_name} ({device})")
        else:
            print(f"ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} ğŸŒ CPUæ¨¡å¼")
            print(f"   æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (CPUå»ºè®®5-10)")
            print(f"ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œé¢„è®¡è€—æ—¶30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
            logger.warning(f"ğŸ“Š ç´¢å¼•æ„å»ºä½¿ç”¨CPUï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
            logger.info(f"ğŸ’¡ å»ºè®®è°ƒæ•´EMBED_BATCH_SIZEä¸º5-10ä»¥è·å¾—æœ€ä½³CPUæ€§èƒ½")
        
        try:
            # å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç´¢å¼•
            if self._index is None:
                index_start_time = time.time()
                self._index = VectorStoreIndex.from_documents(
                    documents,
                    storage_context=self.storage_context,
                    show_progress=show_progress,
                )
                index_elapsed = time.time() - index_start_time
                print(f"âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ (è€—æ—¶: {index_elapsed:.2f}s)")
                logger.info(f"ç´¢å¼•åˆ›å»ºå®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, è€—æ—¶{index_elapsed:.2f}s, å¹³å‡{index_elapsed/len(documents):.3f}s/æ–‡æ¡£")
            else:
                # å¦‚æœç´¢å¼•å·²å­˜åœ¨ï¼Œæ‰¹é‡å¢é‡æ·»åŠ æ–‡æ¡£ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨insert_ref_docsæ‰¹é‡æ’å…¥ï¼‰
                insert_start_time = time.time()
                
                # ä½¿ç”¨insert_ref_docsæ‰¹é‡æ’å…¥ï¼Œæ€§èƒ½è¿œä¼˜äºé€ä¸ªinsert
                # LlamaIndexä¼šè‡ªåŠ¨æ‰¹å¤„ç†embeddingè®¡ç®—å’Œå‘é‡å­˜å‚¨å†™å…¥
                try:
                    self._index.insert_ref_docs(documents, show_progress=show_progress)
                    insert_elapsed = time.time() - insert_start_time
                    print(f"âœ… æ–‡æ¡£å·²æ‰¹é‡æ·»åŠ åˆ°ç°æœ‰ç´¢å¼• (è€—æ—¶: {insert_elapsed:.2f}s)")
                    logger.info(
                        f"æ‰¹é‡å¢é‡æ·»åŠ å®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, "
                        f"è€—æ—¶{insert_elapsed:.2f}s, "
                        f"å¹³å‡{insert_elapsed/len(documents):.3f}s/æ–‡æ¡£"
                    )
                except AttributeError:
                    # å¦‚æœinsert_ref_docsä¸å­˜åœ¨ï¼Œå›é€€åˆ°æ‰¹é‡æ’å…¥èŠ‚ç‚¹çš„æ–¹å¼
                    logger.warning("insert_ref_docsä¸å¯ç”¨ï¼Œä½¿ç”¨èŠ‚ç‚¹æ‰¹é‡æ’å…¥æ–¹å¼")
                    from llama_index.core.node_parser import SentenceSplitter
                    node_parser = SentenceSplitter(
                        chunk_size=self.chunk_size,
                        chunk_overlap=self.chunk_overlap
                    )
                    # æ‰¹é‡åˆ†å—å¹¶æ’å…¥èŠ‚ç‚¹
                    batch_size = config.EMBED_BATCH_SIZE * 2
                    for i in range(0, len(documents), batch_size):
                        batch_docs = documents[i:i+batch_size]
                        nodes = node_parser.get_nodes_from_documents(batch_docs)
                        # æ‰¹é‡æ’å…¥èŠ‚ç‚¹
                        for node in nodes:
                            self._index.insert(node)
                        if show_progress:
                            progress = min(i + batch_size, len(documents))
                            print(f"   è¿›åº¦: {progress}/{len(documents)} ({progress/len(documents)*100:.1f}%)")
                    insert_elapsed = time.time() - insert_start_time
                    print(f"âœ… æ–‡æ¡£å·²æ‰¹é‡æ·»åŠ åˆ°ç°æœ‰ç´¢å¼• (è€—æ—¶: {insert_elapsed:.2f}s)")
                    logger.info(
                        f"æ‰¹é‡å¢é‡æ·»åŠ å®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, "
                        f"è€—æ—¶{insert_elapsed:.2f}s, "
                        f"å¹³å‡{insert_elapsed/len(documents):.3f}s/æ–‡æ¡£"
                    )
            
            # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
            stats = self.get_stats()
            total_elapsed = time.time() - start_time
            
            print(f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡: {stats}")
            device_info = f"{device} ({'GPUåŠ é€Ÿ' if device.startswith('cuda') else 'CPUæ¨¡å¼'})"
            logger.info(
                f"ç´¢å¼•æ„å»ºå®Œæˆ (è®¾å¤‡: {device_info}): "
                f"æ–‡æ¡£æ•°={len(documents)}, "
                f"å‘é‡æ•°={stats.get('document_count', 0)}, "
                f"æ€»è€—æ—¶={total_elapsed:.2f}s, "
                f"å¹³å‡={total_elapsed/len(documents):.3f}s/æ–‡æ¡£"
            )
            
            # æ„å»ºå‘é‡IDæ˜ å°„ï¼ˆä¼˜åŒ–ï¼šæ‰¹é‡æŸ¥è¯¢ï¼‰
            vector_ids_map_start = time.time()
            vector_ids_map = self._get_vector_ids_batch(
                [doc.metadata.get("file_path", "") for doc in documents 
                 if doc.metadata.get("file_path")]
            )
            vector_ids_elapsed = time.time() - vector_ids_map_start
            print(f"ğŸ“‹ å·²è®°å½• {len(vector_ids_map)} ä¸ªæ–‡ä»¶çš„å‘é‡IDæ˜ å°„ (è€—æ—¶: {vector_ids_elapsed:.2f}s)")
            logger.debug(f"å‘é‡IDæ˜ å°„æ„å»ºè€—æ—¶: {vector_ids_elapsed:.2f}s")
            
            # å¦‚æœæä¾›äº†ç¼“å­˜ç®¡ç†å™¨ï¼Œæ›´æ–°ç¼“å­˜çŠ¶æ€
            if cache_manager and task_id:
                from src.config import config
                if config.ENABLE_CACHE:
                    try:
                        docs_hash = self._compute_documents_hash(documents)
                        vector_count = stats.get('document_count', 0)
                        cache_manager.mark_step_completed(
                            task_id=task_id,
                            step_name=cache_manager.STEP_VECTORIZE,
                            input_hash=docs_hash,
                            vector_count=vector_count,
                            collection_name=self.collection_name
                        )
                    except Exception as e:
                        logger.warning(f"æ›´æ–°å‘é‡åŒ–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            
            return self._index, vector_ids_map
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            
            # å¦‚æœæä¾›äº†ç¼“å­˜ç®¡ç†å™¨ï¼Œæ ‡è®°æ­¥éª¤å¤±è´¥
            if cache_manager and task_id:
                try:
                    cache_manager.mark_step_failed(
                        task_id=task_id,
                        step_name=cache_manager.STEP_VECTORIZE,
                        error_message=str(e)
                    )
                except Exception:
                    pass
            
            raise
    
    def get_index(self) -> VectorStoreIndex:
        """è·å–ç°æœ‰ç´¢å¼•
        
        Returns:
            VectorStoreIndexå¯¹è±¡
        """
        if self._index is None:
            # å°è¯•ä»å·²æœ‰çš„å‘é‡å­˜å‚¨åŠ è½½
            try:
                self._index = VectorStoreIndex.from_vector_store(
                    vector_store=self.vector_store,
                    storage_context=self.storage_context,
                )
                print("âœ… ä»å‘é‡å­˜å‚¨åŠ è½½ç´¢å¼•æˆåŠŸ")
            except Exception as e:
                print(f"â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œå°†åœ¨æ·»åŠ æ–‡æ¡£ååˆ›å»º")
                # åˆ›å»ºä¸€ä¸ªç©ºç´¢å¼•
                self._index = VectorStoreIndex.from_documents(
                    [],
                    storage_context=self.storage_context,
                )
        
        return self._index
    
    def _ensure_collection_dimension_match(self):
        """ç¡®ä¿collectionçš„embeddingç»´åº¦ä¸å½“å‰æ¨¡å‹åŒ¹é…
        
        å¦‚æœcollectionå·²å­˜åœ¨ä½†ç»´åº¦ä¸åŒ¹é…ï¼Œä¼šè‡ªåŠ¨åˆ é™¤å¹¶é‡æ–°åˆ›å»º
        """
        try:
            # é¦–å…ˆç¡®ä¿èƒ½è·å–å½“å‰embeddingæ¨¡å‹çš„ç»´åº¦ï¼ˆå¿…é¡»æˆåŠŸï¼‰
            model_dim = None
            dim_detection_methods = []
            
            # æ–¹æ³•1: å°è¯•ä»æ¨¡å‹å±æ€§è·å–
            if hasattr(self.embed_model, 'embed_dim'):
                model_dim = self.embed_model.embed_dim
                dim_detection_methods.append("embed_dimå±æ€§")
            elif hasattr(self.embed_model, '_model') and hasattr(self.embed_model._model, 'config'):
                # å°è¯•ä»transformersæ¨¡å‹configè·å–
                try:
                    model_dim = getattr(self.embed_model._model.config, 'hidden_size', None)
                    if model_dim:
                        dim_detection_methods.append("æ¨¡å‹config.hidden_size")
                except Exception as e:
                    logger.debug(f"ä»æ¨¡å‹configè·å–ç»´åº¦å¤±è´¥: {e}")
            
            # æ–¹æ³•2: é€šè¿‡å®é™…è®¡ç®—ä¸€ä¸ªæµ‹è¯•å‘é‡è·å–ç»´åº¦ï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
            if model_dim is None:
                try:
                    test_embedding = self.embed_model.get_query_embedding("test")
                    model_dim = len(test_embedding)
                    dim_detection_methods.append("å®é™…è®¡ç®—æµ‹è¯•å‘é‡")
                except Exception as e:
                    logger.warning(f"é€šè¿‡æµ‹è¯•å‘é‡è·å–ç»´åº¦å¤±è´¥: {e}")
            
            # å¦‚æœä»ç„¶æ— æ³•è·å–æ¨¡å‹ç»´åº¦ï¼Œè¿™æ˜¯ä¸¥é‡é”™è¯¯
            if model_dim is None:
                error_msg = "æ— æ³•æ£€æµ‹embeddingæ¨¡å‹ç»´åº¦ï¼Œè¿™å¯èƒ½å¯¼è‡´ç»´åº¦ä¸åŒ¹é…é”™è¯¯"
                logger.error(error_msg)
                print(f"âŒ {error_msg}")
                print(f"   å°è¯•çš„æ–¹æ³•: {dim_detection_methods}")
                raise ValueError(error_msg)
            
            logger.info(f"âœ… æˆåŠŸæ£€æµ‹åˆ°embeddingæ¨¡å‹ç»´åº¦: {model_dim} (æ–¹æ³•: {', '.join(dim_detection_methods)})")
            print(f"ğŸ“ å½“å‰embeddingæ¨¡å‹ç»´åº¦: {model_dim}")
            
            # å°è¯•è·å–ç°æœ‰collection
            try:
                existing_collection = self.chroma_client.get_collection(name=self.collection_name)
                
                # è·å–collectionçš„ç»´åº¦ï¼ˆä»metadataæˆ–æŸ¥è¯¢å®é™…æ•°æ®ï¼‰
                collection_dim = None
                collection_count = existing_collection.count()
                
                try:
                    # å°è¯•ä»collectionçš„metadataè·å–
                    if existing_collection.metadata and 'embedding_dimension' in existing_collection.metadata:
                        collection_dim = existing_collection.metadata['embedding_dimension']
                        logger.info(f"ä»collection metadataè·å–ç»´åº¦: {collection_dim}")
                    elif collection_count > 0:
                        # å¦‚æœcollectionæœ‰æ•°æ®ï¼Œå°è¯•æŸ¥è¯¢ä¸€ä¸ªå‘é‡è·å–ç»´åº¦
                        sample = existing_collection.peek(limit=1)
                        if sample and 'embeddings' in sample and sample['embeddings']:
                            collection_dim = len(sample['embeddings'][0])
                            logger.info(f"ä»collectionå®é™…æ•°æ®è·å–ç»´åº¦: {collection_dim}")
                except Exception as e:
                    logger.warning(f"è·å–collectionç»´åº¦å¤±è´¥: {e}")
                
                # å¦‚æœcollectionä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨ï¼ˆæ— éœ€æ£€æŸ¥ç»´åº¦ï¼‰
                if collection_count == 0:
                    self.chroma_collection = existing_collection
                    print(f"âœ… Collectionä¸ºç©ºï¼Œå¯ä»¥ä½¿ç”¨: {self.collection_name}")
                    logger.info(f"Collectionä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨: {self.collection_name}")
                # å¦‚æœcollectionæœ‰æ•°æ®ä½†æ— æ³•è·å–ç»´åº¦ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥ï¼šåˆ é™¤å¹¶é‡å»º
                elif collection_dim is None:
                    print(f"âš ï¸  Collectionæœ‰æ•°æ®ä½†æ— æ³•æ£€æµ‹ç»´åº¦ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥åˆ é™¤å¹¶é‡å»º")
                    print(f"   å½“å‰æ¨¡å‹ç»´åº¦: {model_dim}")
                    print(f"ğŸ”„ è‡ªåŠ¨åˆ é™¤æ—§collectionå¹¶é‡æ–°åˆ›å»º...")
                    
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.warning(f"å› æ— æ³•æ£€æµ‹ç»´åº¦ï¼Œå·²åˆ é™¤collection: {self.collection_name} (æ¨¡å‹ç»´åº¦: {model_dim})")
                    
                    # é‡æ–°åˆ›å»ºcollection
                    self.chroma_collection = self.chroma_client.get_or_create_collection(
                        name=self.collection_name
                    )
                    print(f"âœ… å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name}")
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œåˆ é™¤å¹¶é‡å»º
                elif model_dim != collection_dim:
                    print(f"âš ï¸  æ£€æµ‹åˆ°embeddingç»´åº¦ä¸åŒ¹é…:")
                    print(f"   Collectionç»´åº¦: {collection_dim}")
                    print(f"   å½“å‰æ¨¡å‹ç»´åº¦: {model_dim}")
                    print(f"ğŸ”„ è‡ªåŠ¨åˆ é™¤æ—§collectionå¹¶é‡æ–°åˆ›å»º...")
                    
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.info(f"å·²åˆ é™¤ç»´åº¦ä¸åŒ¹é…çš„collection: {self.collection_name} (ç»´åº¦: {collection_dim} -> {model_dim})")
                    
                    # é‡æ–°åˆ›å»ºcollection
                    self.chroma_collection = self.chroma_client.get_or_create_collection(
                        name=self.collection_name
                    )
                    print(f"âœ… å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name} (ç»´åº¦: {model_dim})")
                else:
                    # ç»´åº¦åŒ¹é…ï¼Œä½¿ç”¨ç°æœ‰collection
                    self.chroma_collection = existing_collection
                    print(f"âœ… Collectionç»´åº¦æ£€æŸ¥é€šè¿‡: {model_dim}ç»´")
                    logger.info(f"Collectionç»´åº¦åŒ¹é…: {model_dim}ç»´")
                    
            except Exception as e:
                # Collectionä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                if "does not exist" in str(e) or "not found" in str(e).lower():
                    self.chroma_collection = self.chroma_client.get_or_create_collection(
                        name=self.collection_name
                    )
                    print(f"âœ… åˆ›å»ºæ–°collection: {self.collection_name} (ç»´åº¦: {model_dim})")
                    logger.info(f"åˆ›å»ºæ–°collection: {self.collection_name} (ç»´åº¦: {model_dim})")
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                    logger.error(f"è·å–collectionæ—¶å‡ºé”™: {e}")
                    raise
                    
        except Exception as e:
            # å¦‚æœæ£€æµ‹è¿‡ç¨‹å‡ºé”™ï¼Œå°è¯•åˆ é™¤æ—§collectionå¹¶é‡å»ºï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            logger.error(f"ç»´åº¦æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
            logger.info("é‡‡ç”¨ä¿å®ˆç­–ç•¥ï¼šåˆ é™¤æ—§collectionå¹¶é‡å»º")
            
            try:
                # å°è¯•åˆ é™¤æ—§collectionï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                try:
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.info(f"å·²åˆ é™¤å¯èƒ½ä¸å…¼å®¹çš„collection: {self.collection_name}")
                    print(f"ğŸ”„ å·²åˆ é™¤å¯èƒ½ä¸å…¼å®¹çš„collection: {self.collection_name}")
                except:
                    # å¦‚æœåˆ é™¤å¤±è´¥ï¼ˆcollectionä¸å­˜åœ¨ï¼‰ï¼Œç»§ç»­åˆ›å»ºæ–°collection
                    pass
                
                # åˆ›å»ºæ–°collection
                self.chroma_collection = self.chroma_client.get_or_create_collection(
                    name=self.collection_name
                )
                print(f"âœ… å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name}")
                logger.info(f"å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name}")
            except Exception as fallback_error:
                logger.error(f"å›é€€åˆ›å»ºcollectionä¹Ÿå¤±è´¥: {fallback_error}")
                raise
    
    def clear_index(self):
        """æ¸…ç©ºç´¢å¼•"""
        try:
            # åˆ é™¤é›†åˆ
            self.chroma_client.delete_collection(name=self.collection_name)
            print(f"âœ… å·²åˆ é™¤é›†åˆ: {self.collection_name}")
            
            # é‡æ–°åˆ›å»ºé›†åˆ
            self.chroma_collection = self.chroma_client.get_or_create_collection(
                name=self.collection_name
            )
            self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
            self.storage_context = StorageContext.from_defaults(
                vector_store=self.vector_store
            )
            
            # é‡ç½®ç´¢å¼•
            self._index = None
            print("âœ… ç´¢å¼•å·²æ¸…ç©º")
            
        except Exception as e:
            print(f"âŒ æ¸…ç©ºç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def get_stats(self) -> dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            count = self.chroma_collection.count()
            return {
                "collection_name": self.collection_name,
                "document_count": count,
                "embedding_model": self.embedding_model_name,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
            }
        except Exception as e:
            print(f"âŒ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def search(self, query: str, top_k: int = 5) -> List[dict]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if self._index is None:
            self.get_index()
        
        retriever = self._index.as_retriever(similarity_top_k=top_k)
        nodes = retriever.retrieve(query)
        
        results = []
        for node in nodes:
            results.append({
                "text": node.node.text,
                "score": node.score,
                "metadata": node.node.metadata,
            })
        
        return results
    
    def incremental_update(
        self,
        added_docs: List[LlamaDocument],
        modified_docs: List[LlamaDocument],
        deleted_file_paths: List[str],
        metadata_manager=None
    ) -> dict:
        """æ‰§è¡Œå¢é‡æ›´æ–°
        
        Args:
            added_docs: æ–°å¢çš„æ–‡æ¡£åˆ—è¡¨
            modified_docs: ä¿®æ”¹çš„æ–‡æ¡£åˆ—è¡¨
            deleted_file_paths: åˆ é™¤çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨å®ä¾‹ï¼ˆç”¨äºæŸ¥è¯¢å‘é‡IDï¼‰
            
        Returns:
            æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "added": 0,
            "modified": 0,
            "deleted": 0,
            "errors": []
        }
        
        # ç¡®ä¿ç´¢å¼•å­˜åœ¨
        if self._index is None:
            self.get_index()
        
        # 1. å¤„ç†æ–°å¢
        if added_docs:
            try:
                added_count, added_vector_ids = self._add_documents(added_docs)
                stats["added"] = added_count
                print(f"âœ… æ–°å¢ {added_count} ä¸ªæ–‡æ¡£")
                
                # æ›´æ–°å…ƒæ•°æ®çš„å‘é‡ID
                if metadata_manager and added_docs:
                    for doc in added_docs:
                        file_path = doc.metadata.get("file_path", "")
                        if file_path and file_path in added_vector_ids:
                            owner = doc.metadata.get("repository", "").split("/")[0] if "/" in doc.metadata.get("repository", "") else ""
                            repo = doc.metadata.get("repository", "").split("/")[1] if "/" in doc.metadata.get("repository", "") else ""
                            branch = doc.metadata.get("branch", "main")
                            
                            if owner and repo:
                                metadata_manager.update_file_vector_ids(
                                    owner, repo, branch, file_path,
                                    added_vector_ids[file_path]
                                )
            except Exception as e:
                error_msg = f"æ–°å¢æ–‡æ¡£å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)
        
        # 2. å¤„ç†ä¿®æ”¹ï¼ˆå…ˆæ‰¹é‡åˆ é™¤æ—§çš„ï¼Œå†æ‰¹é‡æ·»åŠ æ–°çš„ï¼‰
        if modified_docs:
            try:
                # ä¼˜åŒ–ï¼šæ‰¹é‡æ”¶é›†æ‰€æœ‰éœ€è¦åˆ é™¤çš„å‘é‡ID
                all_vector_ids_to_delete = []
                for doc in modified_docs:
                    file_path = doc.metadata.get("file_path", "")
                    if file_path and metadata_manager:
                        # ä»å…ƒæ•°æ®ä¸­è·å–æ—§çš„å‘é‡ID
                        owner = doc.metadata.get("repository", "").split("/")[0] if "/" in doc.metadata.get("repository", "") else ""
                        repo = doc.metadata.get("repository", "").split("/")[1] if "/" in doc.metadata.get("repository", "") else ""
                        branch = doc.metadata.get("branch", "main")
                        
                        vector_ids = metadata_manager.get_file_vector_ids(owner, repo, branch, file_path)
                        if vector_ids:
                            all_vector_ids_to_delete.extend(vector_ids)
                
                # æ‰¹é‡åˆ é™¤æ‰€æœ‰æ—§å‘é‡ï¼ˆä¼˜åŒ–ï¼šä¸€æ¬¡æ€§åˆ é™¤ï¼‰
                deleted_vector_count = 0
                if all_vector_ids_to_delete:
                    # å»é‡
                    unique_vector_ids = list(set(all_vector_ids_to_delete))
                    # åˆ†æ‰¹åˆ é™¤ä»¥é¿å…å•æ¬¡åˆ é™¤è¿‡å¤šæ•°æ®
                    batch_delete_size = 100
                    for i in range(0, len(unique_vector_ids), batch_delete_size):
                        batch_ids = unique_vector_ids[i:i+batch_delete_size]
                        self._delete_vectors_by_ids(batch_ids)
                        deleted_vector_count += len(batch_ids)
                
                # æ‰¹é‡æ·»åŠ æ–°ç‰ˆæœ¬
                modified_count, modified_vector_ids = self._add_documents(modified_docs)
                stats["modified"] = modified_count
                print(f"âœ… æ›´æ–° {modified_count} ä¸ªæ–‡æ¡£ï¼ˆæ‰¹é‡åˆ é™¤ {deleted_vector_count} ä¸ªæ—§å‘é‡ï¼‰")
                
                # æ›´æ–°å…ƒæ•°æ®çš„å‘é‡ID
                if metadata_manager and modified_docs:
                    for doc in modified_docs:
                        file_path = doc.metadata.get("file_path", "")
                        if file_path and file_path in modified_vector_ids:
                            owner = doc.metadata.get("repository", "").split("/")[0] if "/" in doc.metadata.get("repository", "") else ""
                            repo = doc.metadata.get("repository", "").split("/")[1] if "/" in doc.metadata.get("repository", "") else ""
                            branch = doc.metadata.get("branch", "main")
                            
                            if owner and repo:
                                metadata_manager.update_file_vector_ids(
                                    owner, repo, branch, file_path,
                                    modified_vector_ids[file_path]
                                )
            except Exception as e:
                error_msg = f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)
        
        # 3. å¤„ç†åˆ é™¤
        if deleted_file_paths and metadata_manager:
            try:
                deleted_count = self._delete_documents(deleted_file_paths, metadata_manager)
                stats["deleted"] = deleted_count
                print(f"âœ… åˆ é™¤ {deleted_count} ä¸ªæ–‡æ¡£")
            except Exception as e:
                error_msg = f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)
        
        return stats
    
    def _add_documents(self, documents: List[LlamaDocument]) -> Tuple[int, Dict[str, List[str]]]:
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡æ’å…¥ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            (æˆåŠŸæ·»åŠ çš„æ–‡æ¡£æ•°é‡, æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDçš„æ˜ å°„)
        """
        if not documents:
            return 0, {}
        
        try:
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡æ’å…¥æ›¿ä»£é€ä¸ªæ’å…¥
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨insert_ref_docsæ‰¹é‡æ’å…¥
            try:
                self._index.insert_ref_docs(documents, show_progress=False)
                count = len(documents)
            except AttributeError:
                # å¦‚æœinsert_ref_docsä¸å¯ç”¨ï¼Œä½¿ç”¨èŠ‚ç‚¹æ‰¹é‡æ’å…¥
                from llama_index.core.node_parser import SentenceSplitter
                node_parser = SentenceSplitter(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap
                )
                # æ‰¹é‡åˆ†å—å¹¶æ’å…¥èŠ‚ç‚¹
                all_nodes = node_parser.get_nodes_from_documents(documents)
                for node in all_nodes:
                    self._index.insert(node)
                count = len(documents)
            except Exception as e:
                # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªæ’å…¥ï¼ˆä¿ç•™å®¹é”™èƒ½åŠ›ï¼‰
                logger.warning(f"æ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªæ’å…¥: {e}")
                count = 0
                for doc in documents:
                    try:
                        self._index.insert(doc)
                        count += 1
                    except Exception as insert_error:
                        print(f"âš ï¸  æ·»åŠ æ–‡æ¡£å¤±è´¥ [{doc.metadata.get('file_path', 'unknown')}]: {insert_error}")
                        logger.warning(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {insert_error}")
        except Exception as e:
            logger.error(f"æ‰¹é‡æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            print(f"âŒ æ‰¹é‡æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return 0, {}
        
        # ä¼˜åŒ–ï¼šæ‰¹é‡æŸ¥è¯¢å‘é‡IDæ˜ å°„
        file_paths = [doc.metadata.get("file_path", "") for doc in documents 
                     if doc.metadata.get("file_path")]
        vector_ids_map = self._get_vector_ids_batch(file_paths)
        
        return count, vector_ids_map
    
    def _delete_documents(self, file_paths: List[str], metadata_manager) -> int:
        """æ‰¹é‡åˆ é™¤æ–‡æ¡£
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨å®ä¾‹
            
        Returns:
            æˆåŠŸåˆ é™¤çš„æ–‡æ¡£æ•°é‡
        """
        deleted_count = 0
        
        for file_path in file_paths:
            # éœ€è¦ä»æ–‡æ¡£å…ƒæ•°æ®ä¸­æå–ä»“åº“ä¿¡æ¯
            # è¿™é‡Œå‡è®¾æ–‡ä»¶è·¯å¾„åŒ…å«ä»“åº“ä¿¡æ¯
            # å®é™…ä½¿ç”¨æ—¶éœ€è¦ä¼ é€’å®Œæ•´çš„ä»“åº“ä¿¡æ¯
            # æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
            pass
        
        return deleted_count
    
    def _get_vector_ids_by_metadata(self, file_path: str) -> List[str]:
        """é€šè¿‡æ–‡ä»¶è·¯å¾„æŸ¥è¯¢å¯¹åº”çš„å‘é‡IDåˆ—è¡¨
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å‘é‡IDåˆ—è¡¨
        """
        if not file_path:
            return []
        
        try:
            # æŸ¥è¯¢ Chroma collectionï¼ŒåŒ¹é… file_path
            results = self.chroma_collection.get(
                where={"file_path": file_path}
            )
            return results.get('ids', []) if results else []
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢å‘é‡IDå¤±è´¥ [{file_path}]: {e}")
            return []
    
    def _get_vector_ids_batch(self, file_paths: List[str]) -> Dict[str, List[str]]:
        """æ‰¹é‡æŸ¥è¯¢å‘é‡IDæ˜ å°„ï¼ˆä¼˜åŒ–ï¼šå‡å°‘æŸ¥è¯¢æ¬¡æ•°ï¼‰
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDåˆ—è¡¨çš„æ˜ å°„å­—å…¸
        """
        if not file_paths:
            return {}
        
        # å»é‡
        unique_paths = list(set(file_paths))
        vector_ids_map = {}
        
        try:
            # Chroma ä¸æ”¯æŒæ‰¹é‡ where æ¡ä»¶æŸ¥è¯¢ï¼Œä½†ä»å¯ä»¥ä¼˜åŒ–ï¼š
            # 1. å‡å°‘é‡å¤æŸ¥è¯¢ï¼ˆé€šè¿‡å»é‡ï¼‰
            # 2. æ‰¹é‡è·å–æ‰€æœ‰æ•°æ®ç„¶åè¿‡æ»¤ï¼ˆé€‚ç”¨äºæ•°æ®é‡ä¸å¤§çš„æƒ…å†µï¼‰
            # 3. æˆ–è€…ç»§ç»­é€ä¸ªæŸ¥è¯¢ä½†å»æ‰é‡å¤
            
            # æ–¹æ¡ˆï¼šåˆ†æ‰¹æŸ¥è¯¢ä»¥é¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šæ•°æ®
            batch_size = 50  # æ¯æ‰¹æŸ¥è¯¢50ä¸ªæ–‡ä»¶è·¯å¾„
            total_results = 0
            
            for i in range(0, len(unique_paths), batch_size):
                batch_paths = unique_paths[i:i+batch_size]
                for file_path in batch_paths:
                    vector_ids = self._get_vector_ids_by_metadata(file_path)
                    if vector_ids:
                        vector_ids_map[file_path] = vector_ids
                        total_results += len(vector_ids)
            
            logger.debug(
                f"æ‰¹é‡æŸ¥è¯¢å‘é‡ID: "
                f"è¾“å…¥{len(file_paths)}ä¸ªè·¯å¾„(å»é‡å{len(unique_paths)}ä¸ª), "
                f"æ‰¾åˆ°{len(vector_ids_map)}ä¸ªæ–‡ä»¶, "
                f"å…±{total_results}ä¸ªå‘é‡"
            )
        except Exception as e:
            logger.error(f"æ‰¹é‡æŸ¥è¯¢å‘é‡IDå¤±è´¥: {e}")
            # å›é€€åˆ°é€ä¸ªæŸ¥è¯¢
            for file_path in unique_paths:
                try:
                    vector_ids = self._get_vector_ids_by_metadata(file_path)
                    if vector_ids:
                        vector_ids_map[file_path] = vector_ids
                except Exception as query_error:
                    logger.warning(f"æŸ¥è¯¢å•ä¸ªå‘é‡IDå¤±è´¥ [{file_path}]: {query_error}")
        
        return vector_ids_map
    
    def _delete_vectors_by_ids(self, vector_ids: List[str]):
        """æ ¹æ®å‘é‡IDåˆ é™¤å‘é‡
        
        Args:
            vector_ids: å‘é‡IDåˆ—è¡¨
        """
        if not vector_ids:
            return
        
        try:
            self.chroma_collection.delete(ids=vector_ids)
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤å‘é‡å¤±è´¥: {e}")
            raise
    
    def get_node_ids_for_document(self, doc_id: str) -> List[str]:
        """è·å–æ–‡æ¡£å¯¹åº”çš„æ‰€æœ‰èŠ‚ç‚¹ID
        
        Args:
            doc_id: æ–‡æ¡£ID
            
        Returns:
            èŠ‚ç‚¹IDåˆ—è¡¨
        """
        # LlamaIndex ä½¿ç”¨èŠ‚ç‚¹ï¼ˆNodeï¼‰çš„æ¦‚å¿µï¼Œæ¯ä¸ªæ–‡æ¡£åˆ†å—åä¼šç”Ÿæˆå¤šä¸ªèŠ‚ç‚¹
        # éœ€è¦æŸ¥è¯¢ Chroma collection æ¥è·å–ä¸æ–‡æ¡£ç›¸å…³çš„æ‰€æœ‰èŠ‚ç‚¹
        try:
            # æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼Œç„¶åè¿‡æ»¤å‡ºåŒ¹é…çš„
            # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå¤§è§„æ¨¡æ•°æ®æ—¶éœ€è¦ä¼˜åŒ–
            result = self.chroma_collection.get()
            
            if not result or 'ids' not in result:
                return []
            
            # è¿”å›æ‰€æœ‰IDï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥æ ¹æ®å…ƒæ•°æ®è¿‡æ»¤ï¼‰
            return result['ids']
        except Exception as e:
            print(f"âš ï¸  æŸ¥è¯¢èŠ‚ç‚¹IDå¤±è´¥: {e}")
            return []
    
    @staticmethod
    def _compute_documents_hash(documents: List[LlamaDocument]) -> str:
        """è®¡ç®—æ–‡æ¡£åˆ—è¡¨çš„å“ˆå¸Œå€¼
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            MD5å“ˆå¸Œå€¼
        """
        import hashlib
        import json
        
        # åŸºäºæ–‡æ¡£æ–‡æœ¬å†…å®¹å’Œå…ƒæ•°æ®è®¡ç®—å“ˆå¸Œ
        docs_data = []
        for doc in documents:
            docs_data.append({
                "text": doc.text[:1000],  # åªä½¿ç”¨å‰1000å­—ç¬¦ä»¥æé«˜æ€§èƒ½
                "file_path": doc.metadata.get("file_path", ""),
                "file_name": doc.metadata.get("file_name", "")
            })
        
        docs_str = json.dumps(docs_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(docs_str.encode('utf-8')).hexdigest()
    
    def _check_vectors_exist(self, documents: List[LlamaDocument]) -> bool:
        """æ£€æŸ¥æ–‡æ¡£å¯¹åº”çš„å‘é‡æ˜¯å¦å·²å­˜åœ¨äº Chroma ä¸­
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            å¦‚æœæ‰€æœ‰æ–‡æ¡£çš„å‘é‡éƒ½å­˜åœ¨è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            # æ£€æŸ¥ collection æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•°æ®
            if not hasattr(self, 'chroma_collection'):
                return False
            
            collection_count = self.chroma_collection.count()
            if collection_count == 0:
                return False
            
            # æ£€æŸ¥æ¯ä¸ªæ–‡æ¡£æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„å‘é‡
            # ç®€åŒ–ç‰ˆæœ¬ï¼šå¦‚æœ collection æœ‰æ•°æ®ä¸”æ–‡æ¡£æ•°é‡åˆç†ï¼Œè®¤ä¸ºå·²å­˜åœ¨
            # æ›´ä¸¥æ ¼çš„æ£€æŸ¥å¯ä»¥é€šè¿‡æŸ¥è¯¢æ¯ä¸ªæ–‡ä»¶çš„å‘é‡IDå®ç°
            file_paths = [doc.metadata.get("file_path", "") for doc in documents if doc.metadata.get("file_path")]
            if not file_paths:
                return False
            
            # æ£€æŸ¥è‡³å°‘ä¸€éƒ¨åˆ†æ–‡ä»¶çš„å‘é‡æ˜¯å¦å­˜åœ¨
            sample_paths = file_paths[:min(5, len(file_paths))]  # æ£€æŸ¥å‰5ä¸ªæ–‡ä»¶
            existing_count = 0
            for file_path in sample_paths:
                vector_ids = self._get_vector_ids_by_metadata(file_path)
                if vector_ids:
                    existing_count += 1
            
            # å¦‚æœæ‰€æœ‰æ ·æœ¬æ–‡ä»¶éƒ½æœ‰å‘é‡ï¼Œè®¤ä¸ºç¼“å­˜æœ‰æ•ˆ
            return existing_count == len(sample_paths)
            
        except Exception as e:
            logger.warning(f"æ£€æŸ¥å‘é‡å­˜åœ¨æ€§å¤±è´¥: {e}")
            return False
    
    def _filter_vectorized_documents(
        self, 
        documents: List[LlamaDocument]
    ) -> Tuple[List[LlamaDocument], int]:
        """è¿‡æ»¤å·²å‘é‡åŒ–çš„æ–‡æ¡£ï¼Œå®ç°æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ 
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            (å¾…å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨, å·²å‘é‡åŒ–çš„æ–‡æ¡£æ•°é‡)
        """
        if not documents:
            return [], 0
        
        # ç¡®ä¿ç´¢å¼•å·²åˆå§‹åŒ–
        if self._index is None:
            self.get_index()
        
        # æ£€æŸ¥ collection æ˜¯å¦å­˜åœ¨
        if not hasattr(self, 'chroma_collection'):
            return documents, 0
        
        try:
            collection_count = self.chroma_collection.count()
            if collection_count == 0:
                # å¦‚æœ collection ä¸ºç©ºï¼Œæ‰€æœ‰æ–‡æ¡£éƒ½éœ€è¦å¤„ç†
                return documents, 0
            
            # æ£€æŸ¥æ¯ä¸ªæ–‡æ¡£æ˜¯å¦å·²å‘é‡åŒ–
            documents_to_process = []
            already_vectorized_count = 0
            
            for doc in documents:
                file_path = doc.metadata.get("file_path", "")
                if not file_path:
                    # å¦‚æœæ²¡æœ‰ file_pathï¼Œæ— æ³•æ£€æŸ¥ï¼Œéœ€è¦å¤„ç†
                    documents_to_process.append(doc)
                    continue
                
                # æŸ¥è¯¢è¯¥æ–‡ä»¶æ˜¯å¦å·²æœ‰å‘é‡
                vector_ids = self._get_vector_ids_by_metadata(file_path)
                if vector_ids:
                    # è¯¥æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡
                    already_vectorized_count += 1
                    logger.debug(f"æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡: {file_path}")
                else:
                    # è¯¥æ–‡æ¡£æœªå‘é‡åŒ–ï¼Œéœ€è¦å¤„ç†
                    documents_to_process.append(doc)
            
            if already_vectorized_count > 0:
                logger.info(
                    f"æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ : "
                    f"æ€»æ–‡æ¡£æ•°={len(documents)}, "
                    f"å·²å‘é‡åŒ–={already_vectorized_count}, "
                    f"å¾…å¤„ç†={len(documents_to_process)}"
                )
            
            return documents_to_process, already_vectorized_count
            
        except Exception as e:
            logger.warning(f"è¿‡æ»¤å·²å‘é‡åŒ–æ–‡æ¡£å¤±è´¥: {e}ï¼Œå°†å¤„ç†æ‰€æœ‰æ–‡æ¡£")
            return documents, 0
    
    def preload_wikipedia_concepts(
        self,
        concept_keywords: List[str],
        lang: str = "zh",
        show_progress: bool = True
    ) -> int:
        """é¢„åŠ è½½æ ¸å¿ƒæ¦‚å¿µçš„ç»´åŸºç™¾ç§‘å†…å®¹åˆ°ç´¢å¼•
        
        Args:
            concept_keywords: æ¦‚å¿µå…³é”®è¯åˆ—è¡¨ï¼ˆç»´åŸºç™¾ç§‘é¡µé¢æ ‡é¢˜ï¼‰
            lang: è¯­è¨€ä»£ç ï¼ˆzh=ä¸­æ–‡, en=è‹±æ–‡ï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            æˆåŠŸç´¢å¼•çš„é¡µé¢æ•°é‡
            
        Examples:
            >>> index_manager.preload_wikipedia_concepts(
            ...     ["ç³»ç»Ÿç§‘å­¦", "é’±å­¦æ£®", "æ§åˆ¶è®º"],
            ...     lang="zh"
            ... )
        """
        if not concept_keywords:
            print("âš ï¸  æ¦‚å¿µå…³é”®è¯åˆ—è¡¨ä¸ºç©º")
            return 0
        
        try:
            from src.data_loader import load_documents_from_wikipedia
            
            if show_progress:
                print(f"ğŸ“– é¢„åŠ è½½ {len(concept_keywords)} ä¸ªç»´åŸºç™¾ç§‘æ¦‚å¿µ...")
            
            logger.info(f"å¼€å§‹é¢„åŠ è½½ç»´åŸºç™¾ç§‘æ¦‚å¿µ: {concept_keywords}")
            
            # åŠ è½½ç»´åŸºç™¾ç§‘é¡µé¢
            wiki_docs = load_documents_from_wikipedia(
                pages=concept_keywords,
                lang=lang,
                auto_suggest=True,
                clean=True,
                show_progress=show_progress
            )
            
            if not wiki_docs:
                if show_progress:
                    print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ç»´åŸºç™¾ç§‘å†…å®¹")
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç»´åŸºç™¾ç§‘å†…å®¹")
                return 0
            
            # æ„å»ºç´¢å¼•
            self.build_index(wiki_docs, show_progress=show_progress)
            
            if show_progress:
                print(f"âœ… å·²ç´¢å¼• {len(wiki_docs)} ä¸ªç»´åŸºç™¾ç§‘é¡µé¢")
            
            logger.info(f"æˆåŠŸé¢„åŠ è½½ {len(wiki_docs)} ä¸ªç»´åŸºç™¾ç§‘é¡µé¢")
            
            return len(wiki_docs)
            
        except Exception as e:
            print(f"âŒ é¢„åŠ è½½ç»´åŸºç™¾ç§‘å¤±è´¥: {e}")
            logger.error(f"é¢„åŠ è½½ç»´åŸºç™¾ç§‘å¤±è´¥: {e}")
            return 0
    
    def close(self):
        """å…³é—­ç´¢å¼•ç®¡ç†å™¨ï¼Œé‡Šæ”¾èµ„æº
        
        æ˜¾å¼å…³é—­ Chroma å®¢æˆ·ç«¯è¿æ¥ï¼Œåœæ­¢åå°çº¿ç¨‹
        åº”è¯¥åœ¨åº”ç”¨å…³é—­æ—¶è°ƒç”¨æ­¤æ–¹æ³•
        """
        try:
            logger.info("ğŸ”§ å¼€å§‹å…³é—­ç´¢å¼•ç®¡ç†å™¨...")
            
            # 1. æ¸…ç† Chroma å®¢æˆ·ç«¯
            if hasattr(self, 'chroma_client') and self.chroma_client is not None:
                try:
                    # å°è¯•å¤šç§æ–¹å¼å…³é—­å®¢æˆ·ç«¯
                    client = self.chroma_client
                    
                    # æ–¹æ³•1: å°è¯•è°ƒç”¨ close() æ–¹æ³•
                    if hasattr(client, 'close'):
                        client.close()
                        logger.info("âœ… Chromaå®¢æˆ·ç«¯å·²é€šè¿‡ close() æ–¹æ³•å…³é—­")
                    # æ–¹æ³•2: å°è¯•è°ƒç”¨ reset() æ–¹æ³•ï¼ˆæŸäº›ç‰ˆæœ¬æ”¯æŒï¼‰
                    elif hasattr(client, 'reset'):
                        client.reset()
                        logger.info("âœ… Chromaå®¢æˆ·ç«¯å·²é€šè¿‡ reset() æ–¹æ³•é‡ç½®")
                    # æ–¹æ³•3: å°è¯•è®¿é—®å†…éƒ¨å±æ€§å¹¶å…³é—­
                    elif hasattr(client, '_client'):
                        # æŸäº›ç‰ˆæœ¬çš„ Chroma å¯èƒ½æœ‰å†…éƒ¨å®¢æˆ·ç«¯
                        inner_client = getattr(client, '_client', None)
                        if inner_client and hasattr(inner_client, 'close'):
                            inner_client.close()
                            logger.info("âœ… Chromaå†…éƒ¨å®¢æˆ·ç«¯å·²å…³é—­")
                    
                    # æ¸…ç†å¼•ç”¨
                    self.chroma_client = None
                    logger.info("âœ… Chromaå®¢æˆ·ç«¯å¼•ç”¨å·²æ¸…ç†")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸  å…³é—­ Chroma å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
                    # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¦æ¸…ç†å¼•ç”¨
                    self.chroma_client = None
            
            # 2. æ¸…ç†å…¶ä»–å¼•ç”¨
            if hasattr(self, 'chroma_collection'):
                self.chroma_collection = None
            if hasattr(self, 'vector_store'):
                self.vector_store = None
            if hasattr(self, 'storage_context'):
                self.storage_context = None
            if hasattr(self, '_index'):
                self._index = None
            
            # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆå¯é€‰ï¼Œå¸®åŠ©æ¸…ç†çº¿ç¨‹ï¼‰
            try:
                import gc
                gc.collect()
                logger.debug("âœ… å·²æ‰§è¡Œåƒåœ¾å›æ”¶")
            except Exception as e:
                logger.debug(f"åƒåœ¾å›æ”¶æ—¶å‡ºé”™: {e}")
            
            logger.info("âœ… ç´¢å¼•ç®¡ç†å™¨èµ„æºå·²é‡Šæ”¾")
            
        except Exception as e:
            logger.warning(f"âš ï¸  å…³é—­ç´¢å¼•ç®¡ç†å™¨æ—¶å‡ºé”™: {e}")
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¦å°½å¯èƒ½æ¸…ç†å¼•ç”¨
            try:
                self.chroma_client = None
                self.chroma_collection = None
                self.vector_store = None
                self.storage_context = None
                self._index = None
            except:
                pass
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«é‡Šæ”¾"""
        try:
            self.close()
        except Exception:
            # ææ„å‡½æ•°ä¸­çš„å¼‚å¸¸åº”è¯¥è¢«å¿½ç•¥
            pass


def create_index_from_directory(
    directory_path: str | Path,
    collection_name: Optional[str] = None,
    recursive: bool = True
) -> IndexManager:
    """ä»ç›®å½•åˆ›å»ºç´¢å¼•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        directory_path: æ–‡æ¡£ç›®å½•è·¯å¾„
        collection_name: é›†åˆåç§°
        recursive: æ˜¯å¦é€’å½’åŠ è½½
        
    Returns:
        IndexManagerå¯¹è±¡
    """
    from src.data_loader import load_documents_from_directory
    
    # åŠ è½½æ–‡æ¡£
    print(f"ğŸ“‚ ä»ç›®å½•åŠ è½½æ–‡æ¡£: {directory_path}")
    documents = load_documents_from_directory(directory_path, recursive=recursive)
    
    if not documents:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return IndexManager(collection_name=collection_name)
    
    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index_manager = IndexManager(collection_name=collection_name)
    
    # æ„å»ºç´¢å¼•
    index_manager.build_index(documents)
    
    return index_manager


def create_index_from_urls(
    urls: List[str],
    collection_name: Optional[str] = None
) -> IndexManager:
    """ä»URLåˆ—è¡¨åˆ›å»ºç´¢å¼•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        urls: URLåˆ—è¡¨
        collection_name: é›†åˆåç§°
        
    Returns:
        IndexManagerå¯¹è±¡
    """
    from src.data_loader import load_documents_from_urls
    
    # åŠ è½½æ–‡æ¡£
    print(f"ğŸŒ ä» {len(urls)} ä¸ªURLåŠ è½½æ–‡æ¡£")
    documents = load_documents_from_urls(urls)
    
    if not documents:
        print("âš ï¸  æœªæˆåŠŸåŠ è½½ä»»ä½•ç½‘é¡µ")
        return IndexManager(collection_name=collection_name)
    
    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index_manager = IndexManager(collection_name=collection_name)
    
    # æ„å»ºç´¢å¼•
    index_manager.build_index(documents)
    
    return index_manager


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=== æµ‹è¯•ç´¢å¼•æ„å»º ===\n")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        LlamaDocument(
            text="ç³»ç»Ÿç§‘å­¦æ˜¯ç ”ç©¶ç³»ç»Ÿçš„ä¸€èˆ¬è§„å¾‹å’Œæ–¹æ³•çš„ç§‘å­¦ã€‚",
            metadata={"title": "ç³»ç»Ÿç§‘å­¦ç®€ä»‹", "source": "test"}
        ),
        LlamaDocument(
            text="é’±å­¦æ£®æ˜¯ä¸­å›½ç³»ç»Ÿç§‘å­¦çš„åˆ›å»ºè€…ä¹‹ä¸€ï¼Œä»–æå‡ºäº†ç³»ç»Ÿå·¥ç¨‹çš„ç†è®ºä½“ç³»ã€‚",
            metadata={"title": "é’±å­¦æ£®ä¸ç³»ç»Ÿç§‘å­¦", "source": "test"}
        ),
    ]
    
    # åˆ›å»ºç´¢å¼•
    index_manager = IndexManager(collection_name="test_collection")
    index_manager.build_index(test_docs)
    
    # æµ‹è¯•æœç´¢
    print("\n=== æµ‹è¯•æœç´¢ ===")
    results = index_manager.search("é’±å­¦æ£®", top_k=2)
    for i, result in enumerate(results, 1):
        print(f"\nç»“æœ {i}:")
        print(f"ç›¸ä¼¼åº¦: {result['score']:.4f}")
        print(f"å†…å®¹: {result['text']}")
    
    # æ¸…ç†æµ‹è¯•æ•°æ®
    index_manager.clear_index()
    print("\nâœ… æµ‹è¯•å®Œæˆ")

