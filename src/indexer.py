"""
ç´¢å¼•æ„å»ºæ¨¡å—
è´Ÿè´£æ„å»ºå’Œç®¡ç†å‘é‡ç´¢å¼•ï¼Œé›†æˆChromaå‘é‡æ•°æ®åº“
"""

import os
import time
from pathlib import Path
from typing import List, Optional, Tuple, Dict

import chromadb
from tqdm import tqdm
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    Settings,
)
from llama_index.core.schema import Document as LlamaDocument
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore

from src.config import config, get_gpu_device, is_gpu_available, get_device_status
from src.logger import setup_logger
from src.embeddings.base import BaseEmbedding
from src.embeddings.factory import create_embedding

logger = setup_logger('indexer')

# å…¨å±€ embedding æ¨¡å‹ç¼“å­˜
_global_embed_model: Optional[HuggingFaceEmbedding] = None


def _setup_huggingface_env():
    """é…ç½® HuggingFace ç¯å¢ƒå˜é‡ï¼ˆé•œåƒå’Œç¦»çº¿æ¨¡å¼ï¼‰
    
    æ³¨æ„ï¼šç¯å¢ƒå˜é‡å·²åœ¨ src/__init__.py ä¸­é¢„è®¾ï¼Œè¿™é‡Œä»…ç”¨äºæ—¥å¿—è®°å½•å’Œç¡®è®¤
    """
    # è®¾ç½®é•œåƒåœ°å€
    if config.HF_ENDPOINT:
        os.environ['HF_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HUGGINGFACE_HUB_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HF_HUB_ENDPOINT'] = config.HF_ENDPOINT  # æ–°ç‰ˆæœ¬ä½¿ç”¨è¿™ä¸ª
        logger.info(f"ğŸŒ ä½¿ç”¨ HuggingFace é•œåƒ: {config.HF_ENDPOINT}")
    
    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    if config.HF_OFFLINE_MODE:
        os.environ['HF_HUB_OFFLINE'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '1'
        logger.info(f"ğŸ“´ å¯ç”¨ç¦»çº¿æ¨¡å¼ï¼ˆä»…ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")
    else:
        # ç¡®ä¿ç¦»çº¿æ¨¡å¼å…³é—­
        os.environ.pop('HF_HUB_OFFLINE', None)
        os.environ.pop('TRANSFORMERS_OFFLINE', None)


def load_embedding_model(model_name: Optional[str] = None, force_reload: bool = False) -> HuggingFaceEmbedding:
    """åŠ è½½ Embedding æ¨¡å‹ï¼ˆæ”¯æŒå…¨å±€å•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼ˆå³ä½¿å·²ç¼“å­˜ï¼‰
        
    Returns:
        HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    
    model_name = model_name or config.EMBEDDING_MODEL
    
    # å¦‚æœå·²ç»åŠ è½½è¿‡ä¸”æ¨¡å‹åç§°ç›¸åŒï¼Œç›´æ¥è¿”å›ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°åŠ è½½ï¼‰
    if _global_embed_model is not None and not force_reload:
        # æ£€æŸ¥ç¼“å­˜çš„æ¨¡å‹åç§°æ˜¯å¦ä¸æ–°é…ç½®ä¸€è‡´
        cached_model_name = getattr(_global_embed_model, 'model_name', None)
        if cached_model_name == model_name:
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„ Embedding æ¨¡å‹ï¼ˆå…¨å±€å˜é‡ï¼‰: {model_name}")
            logger.info(f"   æ¨¡å‹å¯¹è±¡ID: {id(_global_embed_model)}")
            return _global_embed_model
        else:
            # æ¨¡å‹åç§°ä¸ä¸€è‡´ï¼Œæ¸…é™¤ç¼“å­˜å¹¶é‡æ–°åŠ è½½
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹é…ç½®å˜æ›´: {cached_model_name} -> {model_name}")
            logger.info(f"   æ¸…é™¤æ—§æ¨¡å‹ç¼“å­˜ï¼Œé‡æ–°åŠ è½½æ–°æ¨¡å‹")
            _global_embed_model = None
    
    # å¦‚æœéœ€è¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œæ¸…é™¤ç¼“å­˜
    if force_reload:
        logger.info(f"ğŸ”„ å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å‹")
        _global_embed_model = None
    
    # é…ç½® HuggingFace ç¯å¢ƒå˜é‡
    _setup_huggingface_env()
    
    # åŠ è½½æ¨¡å‹
    logger.info(f"ğŸ“¦ å¼€å§‹åŠ è½½ Embedding æ¨¡å‹ï¼ˆå…¨æ–°åŠ è½½ï¼‰: {model_name}")
    
    try:
        # æ˜¾å¼æŒ‡å®šç¼“å­˜ç›®å½•ä»¥ç¡®ä¿ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        cache_folder = str(Path.home() / ".cache" / "huggingface")
        
        # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
        device = get_gpu_device()
        import torch
        
        # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
        if device.startswith("cuda") and is_gpu_available():
            device_name = torch.cuda.get_device_name()
            cuda_version = torch.version.cuda
            logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ:")
            logger.info(f"   è®¾å¤‡: {device}")
            logger.info(f"   GPUåç§°: {device_name}")
            logger.info(f"   CUDAç‰ˆæœ¬: {cuda_version}")
        else:
            logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
            logger.info("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
            logger.info("ğŸ’¡ å»ºè®®: å¦‚æœ‰GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        
        # æ„å»ºæ¨¡å‹å‚æ•°
        model_kwargs = {
            "trust_remote_code": True,
            "cache_folder": cache_folder,
        }
        
        _global_embed_model = HuggingFaceEmbedding(
            model_name=model_name,
            embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†ï¼Œæå‡æ€§èƒ½
            max_length=config.EMBED_MAX_LENGTH,  # è®¾ç½®æœ€å¤§é•¿åº¦
            **model_kwargs
        )
        
        # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPUï¼ˆå¦‚æœä¸æ”¯æŒé€šè¿‡å‚æ•°æŒ‡å®šï¼‰
        try:
            if device.startswith("cuda") and is_gpu_available():
                # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                if hasattr(_global_embed_model, '_model') and hasattr(_global_embed_model._model, 'to'):
                    _global_embed_model._model = _global_embed_model._model.to(device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                elif hasattr(_global_embed_model, 'model') and hasattr(_global_embed_model.model, 'to'):
                    _global_embed_model.model = _global_embed_model.model.to(device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
            else:
                logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
            logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
        
        logger.info(f"âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_folder}")
        if device.startswith("cuda"):
            logger.info(f"âš¡ GPUåŠ é€Ÿæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (æ¨è10-50)")
        else:
            logger.info(f"ğŸŒ CPUæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (å»ºè®®è°ƒæ•´ä¸º5-10)")
        logger.info(f"ğŸ“ æœ€å¤§é•¿åº¦: {config.EMBED_MAX_LENGTH}")
    except Exception as e:
        # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ä¸”ç¼ºå°‘ç¼“å­˜ï¼Œå°è¯•åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
        if config.HF_OFFLINE_MODE and "offline" in str(e).lower():
            logger.warning(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æœ¬åœ°æ— ç¼“å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼å°è¯•ä¸‹è½½")
            os.environ.pop('HF_HUB_OFFLINE', None)
            
            try:
                cache_folder = str(Path.home() / ".cache" / "huggingface")
                
                # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                device = get_gpu_device()
                import torch
                
                # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                if device.startswith("cuda") and is_gpu_available():
                    device_name = torch.cuda.get_device_name()
                    logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ: {device_name} ({device})")
                else:
                    logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                    logger.info("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                
                # æ„å»ºæ¨¡å‹å‚æ•°
                model_kwargs = {
                    "trust_remote_code": True,
                    "cache_folder": cache_folder,
                }
                
                _global_embed_model = HuggingFaceEmbedding(
                    model_name=model_name,
                    embed_batch_size=config.EMBED_BATCH_SIZE,
                    max_length=config.EMBED_MAX_LENGTH,
                    **model_kwargs
                )
                
                # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPU
                try:
                    if device.startswith("cuda") and is_gpu_available():
                        # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                        if hasattr(_global_embed_model, '_model') and hasattr(_global_embed_model._model, 'to'):
                            _global_embed_model._model = _global_embed_model._model.to(device)
                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                        elif hasattr(_global_embed_model, 'model') and hasattr(_global_embed_model.model, 'to'):
                            _global_embed_model.model = _global_embed_model.model.to(device)
                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                    else:
                        logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                except Exception as e:
                    logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                    logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                
                logger.info(f"âœ… Embedding æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ: {model_name}")
                if device.startswith("cuda"):
                    logger.info(f"âš¡ GPUåŠ é€Ÿæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (æ¨è10-50)")
                else:
                    logger.info(f"ğŸŒ CPUæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (å»ºè®®è°ƒæ•´ä¸º5-10)")
                logger.info(f"ğŸ“ æœ€å¤§é•¿åº¦: {config.EMBED_MAX_LENGTH}")
            except Exception as retry_error:
                logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {retry_error}")
                raise
        else:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    return _global_embed_model


def set_global_embed_model(model: HuggingFaceEmbedding):
    """è®¾ç½®å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Args:
        model: HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    _global_embed_model = model
    logger.debug("ğŸ”§ è®¾ç½®å…¨å±€ Embedding æ¨¡å‹")


def get_global_embed_model() -> Optional[HuggingFaceEmbedding]:
    """è·å–å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Returns:
        å·²åŠ è½½çš„æ¨¡å‹å®ä¾‹ï¼Œå¦‚æœæœªåŠ è½½åˆ™è¿”å› None
    """
    return _global_embed_model


def clear_embedding_model_cache():
    """æ¸…é™¤å…¨å±€ Embedding æ¨¡å‹ç¼“å­˜
    
    ç”¨äºæ¨¡å‹åˆ‡æ¢æˆ–å¼ºåˆ¶é‡æ–°åŠ è½½åœºæ™¯
    """
    global _global_embed_model
    if _global_embed_model is not None:
        logger.info(f"ğŸ§¹ æ¸…é™¤ Embedding æ¨¡å‹ç¼“å­˜")
        _global_embed_model = None


def get_embedding_model_status() -> dict:
    """è·å– Embedding æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    
    Returns:
        åŒ…å«æ¨¡å‹çŠ¶æ€çš„å­—å…¸ï¼š
        {
            "loaded": bool,              # æ˜¯å¦å·²åŠ è½½
            "model_name": str,           # æ¨¡å‹åç§°
            "cache_dir": str,            # ç¼“å­˜ç›®å½•
            "cache_exists": bool,        # æœ¬åœ°ç¼“å­˜æ˜¯å¦å­˜åœ¨
            "offline_mode": bool,        # æ˜¯å¦ç¦»çº¿æ¨¡å¼
            "mirror": str,               # é•œåƒåœ°å€
        }
    """
    import os
    from pathlib import Path
    
    model_name = config.EMBEDDING_MODEL
    
    # æ£€æŸ¥ç¼“å­˜ç›®å½•
    cache_root = Path.home() / ".cache" / "huggingface" / "hub"
    # HuggingFace ç¼“å­˜æ ¼å¼: models--{org}--{model}
    model_cache_name = model_name.replace("/", "--")
    cache_dir = cache_root / f"models--{model_cache_name}"
    cache_exists = cache_dir.exists()
    
    return {
        "loaded": _global_embed_model is not None,
        "model_name": model_name,
        "cache_dir": str(cache_dir),
        "cache_exists": cache_exists,
        "offline_mode": config.HF_OFFLINE_MODE,
        "mirror": config.HF_ENDPOINT if config.HF_ENDPOINT else "huggingface.co (å®˜æ–¹)",
    }


class IndexManager:
    """ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(
        self,
        collection_name: Optional[str] = None,
        persist_dir: Optional[Path] = None,
        embedding_model: Optional[str] = None,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        embed_model_instance: Optional[HuggingFaceEmbedding] = None,
        embedding_instance: Optional[BaseEmbedding] = None,  # æ–°å¢ï¼šç»Ÿä¸€Embeddingæ¥å£
    ):
        """åˆå§‹åŒ–ç´¢å¼•ç®¡ç†å™¨
        
        Args:
            collection_name: Chromaé›†åˆåç§°
            persist_dir: å‘é‡å­˜å‚¨æŒä¹…åŒ–ç›®å½•
            embedding_model: Embeddingæ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å—é‡å å¤§å°
            embed_model_instance: é¢„åŠ è½½çš„Embeddingæ¨¡å‹å®ä¾‹ï¼ˆæ—§æ¥å£ï¼Œå…¼å®¹ä¿ç•™ï¼‰
            embedding_instance: ç»Ÿä¸€çš„Embeddingå®ä¾‹ï¼ˆæ–°æ¥å£ï¼Œæ¨èä½¿ç”¨ï¼‰
        """
        # ä½¿ç”¨é…ç½®æˆ–é»˜è®¤å€¼
        self.collection_name = collection_name or config.CHROMA_COLLECTION_NAME
        self.persist_dir = persist_dir or config.VECTOR_STORE_PATH
        self.embedding_model_name = embedding_model or config.EMBEDDING_MODEL
        self.chunk_size = chunk_size or config.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or config.CHUNK_OVERLAP
        
        # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
        self.persist_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ç»Ÿä¸€çš„Embeddingå®ä¾‹ï¼ˆå¦‚æœæä¾›ï¼‰
        self._embedding_instance = embedding_instance
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆä¼˜å…ˆçº§ï¼šembedding_instance > embed_model_instance > å·¥å‚åˆ›å»ºï¼‰
        if embedding_instance is not None:
            # ä¼˜å…ˆä½¿ç”¨æ–°çš„ç»Ÿä¸€æ¥å£
            logger.info(f"âœ… ä½¿ç”¨æä¾›çš„Embeddingå®ä¾‹: {embedding_instance}")
            # ä»BaseEmbeddingè·å–LlamaIndexå…¼å®¹çš„æ¨¡å‹
            if hasattr(embedding_instance, 'get_llama_index_embedding'):
                self.embed_model = embedding_instance.get_llama_index_embedding()
                logger.info(f"   è·å–LlamaIndexå…¼å®¹å®ä¾‹")
            else:
                # å¦‚æœä¸æ”¯æŒget_llama_index_embeddingï¼Œç›´æ¥ä½¿ç”¨ï¼ˆå‡è®¾å·²å…¼å®¹ï¼‰
                self.embed_model = embedding_instance
        elif embed_model_instance is not None:
            # å‘åå…¼å®¹ï¼šä½¿ç”¨æ—§æ¥å£
            logger.info(f"âœ… ä½¿ç”¨é¢„åŠ è½½çš„Embeddingæ¨¡å‹ï¼ˆæ—§æ¥å£ï¼‰: {self.embedding_model_name}")
            self.embed_model = embed_model_instance
            self._embedding_instance = None  # æ—§æ¥å£æ²¡æœ‰ç»Ÿä¸€å®ä¾‹
        else:
            # æ£€æŸ¥å…¨å±€ç¼“å­˜ä¸­çš„æ¨¡å‹æ˜¯å¦åŒ¹é…å½“å‰é…ç½®
            global _global_embed_model
            cached_model_name = None
            if _global_embed_model is not None:
                cached_model_name = getattr(_global_embed_model, 'model_name', None)
            
            # å¦‚æœæ¨¡å‹åç§°ä¸åŒ¹é…ï¼Œæ¸…ç†ç¼“å­˜
            if cached_model_name and cached_model_name != self.embedding_model_name:
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹é…ç½®å˜æ›´: {cached_model_name} -> {self.embedding_model_name}")
                logger.info(f"   æ¸…ç†æ—§æ¨¡å‹ç¼“å­˜å¹¶é‡æ–°åŠ è½½")
                clear_embedding_model_cache()
            
            # å°è¯•ä½¿ç”¨å…¨å±€ç¼“å­˜çš„æ¨¡å‹ï¼ˆå¦‚æœåŒ¹é…ï¼‰
            if _global_embed_model is not None:
                try:
                    # éªŒè¯ç¼“å­˜çš„æ¨¡å‹æ˜¯å¦çœŸçš„åŒ¹é…ï¼ˆé€šè¿‡å®é™…è®¡ç®—ç»´åº¦ï¼‰
                    test_embedding = _global_embed_model.get_query_embedding("test")
                    cached_dim = len(test_embedding)
                    logger.info(f"âœ… ä½¿ç”¨å…¨å±€ç¼“å­˜çš„Embeddingæ¨¡å‹: {self.embedding_model_name} (ç»´åº¦: {cached_dim})")
                    self.embed_model = _global_embed_model
                except Exception as e:
                    logger.warning(f"âš ï¸  éªŒè¯ç¼“å­˜æ¨¡å‹å¤±è´¥ï¼Œé‡æ–°åŠ è½½: {e}")
                    clear_embedding_model_cache()
                    # ç»§ç»­ä¸‹é¢çš„åŠ è½½æµç¨‹
                    self.embed_model = None
            else:
                self.embed_model = None
            
            # å¦‚æœç¼“å­˜ä¸å¯ç”¨ï¼ŒåŠ è½½æ–°æ¨¡å‹
            if self.embed_model is None:
                # é…ç½® HuggingFace ç¯å¢ƒå˜é‡
                _setup_huggingface_env()
                
                print(f"ğŸ“¦ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {self.embedding_model_name}")
                
                # ä½¿ç”¨load_embedding_modelå‡½æ•°ä»¥ç¡®ä¿ç¼“å­˜ç®¡ç†æ­£ç¡®
                try:
                    self.embed_model = load_embedding_model(
                        model_name=self.embedding_model_name,
                        force_reload=False  # å¦‚æœç¼“å­˜åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨
                    )
                    logger.info(f"âœ… é€šè¿‡load_embedding_modelåŠ è½½æ¨¡å‹: {self.embedding_model_name}")
                    # å¦‚æœæˆåŠŸåŠ è½½ï¼Œè·³è¿‡ç›´æ¥åŠ è½½çš„ä»£ç å—
                except Exception as e:
                    logger.warning(f"âš ï¸  load_embedding_modelå¤±è´¥ï¼Œä½¿ç”¨ç›´æ¥åŠ è½½æ–¹å¼: {e}")
                    # å›é€€åˆ°ç›´æ¥åŠ è½½æ–¹å¼
                    try:
                        cache_folder = str(Path.home() / ".cache" / "huggingface")
                        
                        # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                        device = get_gpu_device()
                        import torch
                        
                        # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                        if device.startswith("cuda") and is_gpu_available():
                            device_name = torch.cuda.get_device_name()
                            cuda_version = torch.version.cuda
                            print(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ:")
                            print(f"   è®¾å¤‡: {device}")
                            print(f"   GPUåç§°: {device_name}")
                            print(f"   CUDAç‰ˆæœ¬: {cuda_version}")
                            logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPU: {device_name} ({device})")
                        else:
                            print("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                            print("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                            logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                            logger.info("ğŸ’¡ å»ºè®®: å¦‚æœ‰GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æœ€ä½³æ€§èƒ½")
                        
                        # æ„å»ºæ¨¡å‹å‚æ•°
                        model_kwargs = {
                            "trust_remote_code": True,
                            "cache_folder": cache_folder,
                        }
                        
                        self.embed_model = HuggingFaceEmbedding(
                            model_name=self.embedding_model_name,
                            embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†
                            max_length=config.EMBED_MAX_LENGTH,
                            **model_kwargs
                        )
                        
                        # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPUï¼ˆå¦‚æœä¸æ”¯æŒé€šè¿‡å‚æ•°æŒ‡å®šï¼‰
                        try:
                            if device.startswith("cuda") and is_gpu_available():
                                # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                                if hasattr(self.embed_model, '_model') and hasattr(self.embed_model._model, 'to'):
                                    self.embed_model._model = self.embed_model._model.to(device)
                                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                                elif hasattr(self.embed_model, 'model') and hasattr(self.embed_model.model, 'to'):
                                    self.embed_model.model = self.embed_model.model.to(device)
                                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                            else:
                                logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                        except Exception as e:
                            logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                            logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                        
                        if device.startswith("cuda"):
                            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (GPUåŠ é€Ÿ, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE})")
                        else:
                            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (CPUæ¨¡å¼, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE}, å»ºè®®è°ƒæ•´ä¸º5-10)")
                    except Exception as load_error:
                        # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ä¸”ç¼ºå°‘ç¼“å­˜ï¼Œå°è¯•åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
                        if config.HF_OFFLINE_MODE and "offline" in str(load_error).lower():
                            print(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æœ¬åœ°æ— ç¼“å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼å°è¯•ä¸‹è½½...")
                            os.environ.pop('HF_HUB_OFFLINE', None)
                            
                            try:
                                cache_folder = str(Path.home() / ".cache" / "huggingface")
                                
                                # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                                device = get_gpu_device()
                                import torch
                                
                                # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                                if device.startswith("cuda") and is_gpu_available():
                                    device_name = torch.cuda.get_device_name()
                                    print(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ: {device_name} ({device})")
                                    logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPU: {device_name} ({device})")
                                else:
                                    print("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                                    print("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                                    logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                                
                                # æ„å»ºæ¨¡å‹å‚æ•°
                                model_kwargs = {
                                    "trust_remote_code": True,
                                    "cache_folder": cache_folder,
                                }
                                
                                self.embed_model = HuggingFaceEmbedding(
                                    model_name=self.embedding_model_name,
                                    embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†
                                    max_length=config.EMBED_MAX_LENGTH,
                                    **model_kwargs
                                )
                                
                                # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPU
                                try:
                                    if device.startswith("cuda") and is_gpu_available():
                                        if hasattr(self.embed_model, '_model') and hasattr(self.embed_model._model, 'to'):
                                            self.embed_model._model = self.embed_model._model.to(device)
                                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                                        elif hasattr(self.embed_model, 'model') and hasattr(self.embed_model.model, 'to'):
                                            self.embed_model.model = self.embed_model.model.to(device)
                                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                                    else:
                                        logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                                except Exception as e:
                                    logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                                    logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                                
                                if device.startswith("cuda"):
                                    print(f"âœ… æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ (GPUåŠ é€Ÿ, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE})")
                                else:
                                    print(f"âœ… æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ (CPUæ¨¡å¼, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE}, å»ºè®®è°ƒæ•´ä¸º5-10)")
                            except Exception as retry_error:
                                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {retry_error}")
                                raise
                        else:
                            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {load_error}")
                            raise
        
        # é…ç½®å…¨å±€Settings
        Settings.embed_model = self.embed_model
        Settings.chunk_size = self.chunk_size
        Settings.chunk_overlap = self.chunk_overlap
        
        # åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
        print(f"ğŸ—„ï¸  åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“: {self.persist_dir}")
        self.chroma_client = chromadb.PersistentClient(path=str(self.persist_dir))
        
        # æ‰“å°æ•°æ®åº“ä¿¡æ¯
        self._print_database_info()
        
        # æ£€æµ‹å¹¶ä¿®å¤embeddingç»´åº¦ä¸åŒ¹é…é—®é¢˜
        self._ensure_collection_dimension_match()
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
        
        # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )
        
        # ç´¢å¼•å¯¹è±¡ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._index: Optional[VectorStoreIndex] = None
        
        print("âœ… ç´¢å¼•ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    # ==================== æ‰¹å¤„ç†ï¼šæŒ‰ç›®å½•/å­æ¨¡å—åˆ†ç»„ ====================
    def _group_documents_by_directory(
        self,
        documents: List[LlamaDocument],
        depth: int,
        docs_per_batch: int
    ) -> List[List[LlamaDocument]]:
        """æŒ‰ç›¸å¯¹è·¯å¾„ç›®å½•å±‚çº§åˆ†ç»„å¹¶åˆ‡åˆ†ä¸ºæ‰¹æ¬¡
        
        åˆ†ç»„è§„åˆ™ï¼š
        - ä»¥æ–‡æ¡£ metadata['file_path'] ä¸ºä¾æ®ï¼ŒæŒ‰ depth å±‚ç›®å½•åˆ†ç»„
        - æ— æ³•è§£æè·¯å¾„æˆ–åœ¨æ ¹ç›®å½•çš„æ–‡ä»¶å½’ä¸º '_root'
        - æ¯ä¸ªç»„å†…æŒ‰ docs_per_batch è¿›è¡ŒäºŒæ¬¡åˆ‡åˆ†
        
        Returns:
            æ‰¹æ¬¡åˆ—è¡¨ï¼ˆæ¯æ‰¹ä¸ºæ–‡æ¡£åˆ—è¡¨ï¼‰
        """
        from collections import defaultdict
        from pathlib import PurePosixPath

        if not documents:
            return []

        # å½’ä¸€åŒ–åˆ†éš”ç¬¦ï¼Œé¿å… Windows è·¯å¾„å½±å“
        def normalize_rel_path(p: str) -> str:
            if not p:
                return ""
            # å°†åæ–œæ æ›¿æ¢ä¸ºæ–œæ ï¼Œä¾¿äºå±‚çº§åˆ‡åˆ†
            return p.replace('\\', '/').lstrip('/')

        # è®¡ç®—åˆ†ç»„é”®
        def group_key_for_path(rel_path: str) -> str:
            if not rel_path:
                return "_root"
            parts = [seg for seg in normalize_rel_path(rel_path).split('/') if seg]
            if not parts:
                return "_root"
            use_depth = max(1, depth)
            return '/'.join(parts[:use_depth])

        groups: defaultdict[str, List[LlamaDocument]] = defaultdict(list)
        for doc in documents:
            rel_path = doc.metadata.get('file_path', '') or ''
            key = group_key_for_path(rel_path)
            groups[key].append(doc)

        # äºŒæ¬¡åˆ‡åˆ†ï¼šå°†æ¯ä¸ªç»„æŒ‰ docs_per_batch åˆ‡åˆ†ä¸ºå¤šä¸ªæ‰¹æ¬¡
        batches: List[List[LlamaDocument]] = []
        per_batch = max(1, int(docs_per_batch) if docs_per_batch else 20)
        for key, docs in groups.items():
            # ä¿æŒç¨³å®šæ€§ï¼šæŒ‰æ–‡ä»¶åæ’åºï¼Œé¿å…ä¸ç¡®å®šé¡ºåº
            docs_sorted = sorted(
                docs,
                key=lambda d: (d.metadata.get('file_path', ''), d.metadata.get('file_name', ''))
            )
            for i in range(0, len(docs_sorted), per_batch):
                batch_docs = docs_sorted[i:i+per_batch]
                batches.append(batch_docs)

        # ç¨³å®šæ’åºï¼šæŒ‰ç»„åã€å†æŒ‰é¦–æ–‡æ¡£è·¯å¾„
        def batch_sort_key(batch: List[LlamaDocument]) -> tuple:
            if not batch:
                return ("", "")
            first_path = batch[0].metadata.get('file_path', '') or ''
            key = group_key_for_path(first_path)
            return (key, first_path)

        batches.sort(key=batch_sort_key)
        return batches

    # ==================== æ‰¹å¤„ç†ï¼šæ‰¹çº§æ–­ç‚¹ç»­ä¼ ä¸é‡è¯• ====================
    def _batch_ckpt_path(self) -> Path:
        """è¿”å›æ‰¹çº§checkpointæ–‡ä»¶è·¯å¾„ï¼ˆæ¯ä¸ªé›†åˆä¸€ä¸ªæ–‡ä»¶ï¼‰"""
        ckpt_dir = self.persist_dir / "batch_checkpoints"
        ckpt_dir.mkdir(parents=True, exist_ok=True)
        return ckpt_dir / f"{self.collection_name}.json"

    def _load_batch_ckpt(self) -> dict:
        import json
        ckpt_file = self._batch_ckpt_path()
        if not ckpt_file.exists():
            return {"completed": {}}
        try:
            with open(ckpt_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if not isinstance(data, dict) or 'completed' not in data:
                    return {"completed": {}}
                return data
        except Exception:
            return {"completed": {}}

    def _save_batch_ckpt(self, data: dict) -> None:
        import json
        ckpt_file = self._batch_ckpt_path()
        try:
            with open(ckpt_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"å†™å…¥æ‰¹æ¬¡checkpointå¤±è´¥: {e}")

    @staticmethod
    def _compute_batch_id(group_key: str, file_paths: List[str]) -> str:
        import hashlib, json
        payload = json.dumps({
            "group": group_key,
            "files": sorted(file_paths),
        }, ensure_ascii=False)
        return hashlib.md5(payload.encode('utf-8')).hexdigest()
    
    def build_index(
        self,
        documents: List[LlamaDocument],
        show_progress: bool = True,
        cache_manager=None,
        task_id: Optional[str] = None
    ) -> Tuple[VectorStoreIndex, Dict[str, List[str]]]:
        """æ„å»ºæˆ–æ›´æ–°ç´¢å¼•
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            cache_manager: ç¼“å­˜ç®¡ç†å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼‰
            task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼Œç”¨äºç¼“å­˜ï¼‰
            
        Returns:
            (VectorStoreIndexå¯¹è±¡, æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDçš„æ˜ å°„)
        """
        import hashlib
        import json
        start_time = time.time()
        
        if not documents:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ç´¢å¼•")
            return self.get_index(), {}
        
        # æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ ï¼šæ£€æŸ¥æ¯ä¸ªæ–‡æ¡£æ˜¯å¦å·²å‘é‡åŒ–ï¼Œåªå¤„ç†æœªå®Œæˆçš„æ–‡æ¡£
        # è¿™ä¸ªåŠŸèƒ½é»˜è®¤å¯ç”¨ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨ç¼“å­˜ç®¡ç†å™¨éƒ½èƒ½æé«˜æ€§èƒ½
        documents_to_process, already_vectorized = self._filter_vectorized_documents(documents)
        
        if already_vectorized > 0:
            logger.info(f"âœ… æ£€æµ‹åˆ° {already_vectorized} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å¤„ç†")
            print(f"ğŸ“Š æ–­ç‚¹ç»­ä¼ : {already_vectorized}/{len(documents)} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œå‰©ä½™ {len(documents_to_process)} ä¸ªå¾…å¤„ç†")
        
        # å¦‚æœæ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡æ¡£ï¼Œç›´æ¥è¿”å›
        if not documents_to_process:
            logger.info(f"âœ… æ‰€æœ‰æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å‘é‡åŒ–æ­¥éª¤")
            index = self.get_index()
            vector_ids_map = self._get_vector_ids_batch(
                [doc.metadata.get("file_path", "") for doc in documents 
                 if doc.metadata.get("file_path")]
            )
            
            # å¦‚æœæä¾›äº†ç¼“å­˜ç®¡ç†å™¨ï¼Œæ›´æ–°ç¼“å­˜çŠ¶æ€
            if cache_manager and task_id:
                if config.ENABLE_CACHE:
                    docs_hash = self._compute_documents_hash(documents)
                    step_name = cache_manager.STEP_VECTORIZE
                    cache_manager.mark_step_completed(
                        task_id=task_id,
                        step_name=step_name,
                        input_hash=docs_hash,
                        vector_count=self.chroma_collection.count() if hasattr(self, 'chroma_collection') else 0,
                        collection_name=self.collection_name
                    )
            
            return index, vector_ids_map
        
        # æ›´æ–°å¾…å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨
        documents = documents_to_process
        
        # è·å–å½“å‰è®¾å¤‡ä¿¡æ¯
        device = get_gpu_device()
        
        print(f"\nğŸ”¨ å¼€å§‹æ„å»ºç´¢å¼•ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
        print(f"   åˆ†å—å‚æ•°: size={self.chunk_size}, overlap={self.chunk_overlap}")
        
        # è¾“å‡ºè®¾å¤‡ä¿¡æ¯
        if device.startswith("cuda"):
            import torch
            device_name = torch.cuda.get_device_name()
            print(f"ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} âš¡ GPUåŠ é€Ÿæ¨¡å¼")
            print(f"   GPU: {device_name}")
            print(f"   æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (GPUæ¨è10-50)")
            logger.info(f"ğŸ“Š ç´¢å¼•æ„å»ºä½¿ç”¨GPU: {device_name} ({device})")
        else:
            print(f"ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} ğŸŒ CPUæ¨¡å¼")
            print(f"   æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (CPUå»ºè®®5-10)")
            print(f"ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œé¢„è®¡è€—æ—¶30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
            logger.warning(f"ğŸ“Š ç´¢å¼•æ„å»ºä½¿ç”¨CPUï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
            logger.info(f"ğŸ’¡ å»ºè®®è°ƒæ•´EMBED_BATCH_SIZEä¸º5-10ä»¥è·å¾—æœ€ä½³CPUæ€§èƒ½")
        
        try:
            # æ‰¹æ¨¡å¼ï¼šæŒ‰ç›®å½•/å­æ¨¡å—åˆ†æ‰¹æ‰§è¡Œï¼ˆç”±é…ç½®å¼€å…³æ§åˆ¶ï¼‰
            if config.INDEX_BATCH_MODE:
                total_docs = len(documents)
                group_depth = max(1, config.GROUP_DEPTH)
                docs_per_batch = max(1, config.DOCS_PER_BATCH)

                print("\nğŸ§­ æ‰¹å¤„ç†æ¨¡å¼å·²å¯ç”¨")
                print(f"   åˆ†ç»„æ–¹å¼: directory (depth={group_depth})")
                print(f"   ç›®æ ‡æ¯æ‰¹æ–‡æ¡£æ•°: {docs_per_batch}")
                print(f"   æ€»æ–‡æ¡£æ•°: {total_docs}")

                batches = self._group_documents_by_directory(
                    documents=documents,
                    depth=group_depth,
                    docs_per_batch=docs_per_batch,
                )
                # æµ‹è¯•/é™é€Ÿï¼šä»…å¤„ç†å‰ N æ‰¹
                if config.INDEX_MAX_BATCHES and config.INDEX_MAX_BATCHES > 0:
                    print(f"   æµ‹è¯•æ¨¡å¼: ä»…å¤„ç†å‰ {config.INDEX_MAX_BATCHES} æ‰¹")
                    batches = batches[:config.INDEX_MAX_BATCHES]
                total_batches = len(batches)
                print(f"   ç”Ÿæˆæ‰¹æ¬¡æ•°: {total_batches}")

                # ç¡®ä¿ç´¢å¼•å¯¹è±¡å­˜åœ¨
                index = self.get_index()

                from llama_index.core.node_parser import SentenceSplitter
                node_parser = SentenceSplitter(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap
                )

                grand_start = time.time()
                grand_docs = 0
                grand_nodes = 0
                grand_tokens_est = 0

                # åŠ è½½æ‰¹çº§checkpoint
                ckpt = self._load_batch_ckpt()
                completed = ckpt.get("completed", {})

                for b_idx, batch_docs in enumerate(batches, start=1):
                    if not batch_docs:
                        continue
                    # è®¡ç®—æ‰¹æ¬¡é”®ï¼ˆç›®å½•ç»„ï¼‰
                    first_path = batch_docs[0].metadata.get('file_path', '') or ''
                    key = (first_path.replace('\\', '/').lstrip('/') or '_root').split('/')
                    group_key = '/'.join(key[:group_depth]) if key else '_root'

                    batch_doc_count = len(batch_docs)
                    # ç²—ä¼° tokensï¼ˆåŸºäºå­—ç¬¦æ•°/4ï¼‰
                    tokens_est = sum(max(1, len(d.text) // 4) for d in batch_docs)

                    file_list = [d.metadata.get('file_path', '') or '' for d in batch_docs]
                    batch_id = self._compute_batch_id(group_key, file_list)
                    if completed.get(batch_id):
                        print(f"\nğŸ“¦ æ‰¹æ¬¡ {b_idx}/{total_batches} | ç»„: {group_key} å·²å®Œæˆï¼Œè·³è¿‡ (checkpoint)")
                        grand_docs += batch_doc_count
                        # èŠ‚ç‚¹/ä»¤ç‰ŒæœªçŸ¥ï¼Œé‡‡ç”¨0ç´¯åŠ ï¼Œä»…ä½œä¸ºè·³è¿‡æç¤º
                        continue

                    print(f"\nğŸ“¦ æ‰¹æ¬¡ {b_idx}/{total_batches} | ç»„: {group_key}")
                    print(f"   æ–‡æ¡£: {batch_doc_count} | ä¼°ç®—tokens: {tokens_est}")
                    if show_progress:
                        print("   é˜¶æ®µ: åˆ†å—ä¸­...")

                    # åˆ†å—ï¼šæŒ‰æ–‡æ¡£å¾ªç¯ï¼Œä¾¿äºå±•ç¤º doc çº§è¿›åº¦
                    nodes = []
                    if show_progress:
                        for d in tqdm(batch_docs, desc="åˆ†å—", disable=not show_progress, unit="doc"):
                            nodes.extend(node_parser.get_nodes_from_documents([d]))
                    else:
                        nodes = node_parser.get_nodes_from_documents(batch_docs)

                    node_count = len(nodes)
                    print(f"   èŠ‚ç‚¹: {node_count}")

                    # æ’å…¥ï¼šä¼˜å…ˆä½¿ç”¨ insert_nodesï¼ˆæ‰¹é‡ï¼‰ï¼Œå¦åˆ™æŒ‰æ‰¹æ¬¡å†…é€ä¸ª insert
                    if show_progress:
                        print("   é˜¶æ®µ: å‘é‡åŒ–+å†™å…¥ä¸­...")
                    insert_start = time.time()
                    try:
                        if hasattr(self._index, 'insert_nodes'):
                            self._index.insert_nodes(nodes)
                        else:
                            for node in nodes:
                                self._index.insert(node)
                    except Exception as e:
                        # ç®€å•é‡è¯•ï¼ˆæœ€å¤š2æ¬¡ï¼‰ï¼šå…ˆé€ä¸ªinsertå›é€€
                        logger.warning(f"æ‰¹æ¬¡æ’å…¥å¼‚å¸¸ï¼Œå›é€€é€ä¸ªæ’å…¥é‡è¯•: {e}")
                        retry_ok = False
                        for attempt in range(1, 3):
                            try:
                                for node in nodes:
                                    self._index.insert(node)
                                retry_ok = True
                                break
                            except Exception as e2:
                                logger.warning(f"é€ä¸ªæ’å…¥é‡è¯•å¤±è´¥({attempt}/2): {e2}")
                                continue
                        if not retry_ok:
                            logger.error("æ‰¹æ¬¡å†™å…¥å¤±è´¥ï¼Œè·³è¿‡è¯¥æ‰¹æ¬¡å¹¶ç»§ç»­")
                            continue

                    insert_elapsed = time.time() - insert_start
                    docs_per_s = batch_doc_count / insert_elapsed if insert_elapsed > 0 else 0
                    nodes_per_s = node_count / insert_elapsed if insert_elapsed > 0 else 0
                    tokens_per_s = tokens_est / insert_elapsed if insert_elapsed > 0 else 0
                    print(f"   â±ï¸ æ‰¹è€—æ—¶: {insert_elapsed:.2f}s | é€Ÿç‡: {nodes_per_s:.1f} nodes/s, {docs_per_s:.1f} docs/s, {tokens_per_s:.1f} tok/s | it/s={nodes_per_s:.1f}")

                    grand_docs += batch_doc_count
                    grand_nodes += node_count
                    grand_tokens_est += tokens_est

                    # æ ‡è®°æ‰¹æ¬¡å®Œæˆï¼ˆcheckpointï¼‰
                    completed[batch_id] = {
                        "group": group_key,
                        "files": file_list,
                        "docs": batch_doc_count,
                        "nodes": node_count,
                        "tokens_est": tokens_est,
                        "elapsed": insert_elapsed,
                    }
                    ckpt["completed"] = completed
                    self._save_batch_ckpt(ckpt)

                grand_elapsed = time.time() - grand_start
                grand_docs_s = grand_docs / grand_elapsed if grand_elapsed > 0 else 0
                grand_nodes_s = grand_nodes / grand_elapsed if grand_elapsed > 0 else 0
                grand_tokens_s = grand_tokens_est / grand_elapsed if grand_elapsed > 0 else 0
                print("\nâœ… æ‰¹å¤„ç†å®Œæˆ")
                print(f"   æ€»æ‰¹æ¬¡: {total_batches} | æ€»æ–‡æ¡£: {grand_docs} | æ€»èŠ‚ç‚¹: {grand_nodes} | æ€»tokens(ä¼°ç®—): {grand_tokens_est}")
                print(f"   æ€»è€—æ—¶: {grand_elapsed:.2f}s | å¹³å‡é€Ÿç‡: {grand_nodes_s:.1f} nodes/s, {grand_docs_s:.1f} docs/s, {grand_tokens_s:.1f} tok/s")

            # éæ‰¹æ¨¡å¼ï¼šä¿æŒç°æœ‰è·¯å¾„
            elif self._index is None:
                index_start_time = time.time()
                self._index = VectorStoreIndex.from_documents(
                    documents,
                    storage_context=self.storage_context,
                    show_progress=show_progress,
                )
                index_elapsed = time.time() - index_start_time
                print(f"âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ (è€—æ—¶: {index_elapsed:.2f}s)")
                logger.info(f"ç´¢å¼•åˆ›å»ºå®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, è€—æ—¶{index_elapsed:.2f}s, å¹³å‡{index_elapsed/len(documents):.3f}s/æ–‡æ¡£")
            else:
                # å¦‚æœç´¢å¼•å·²å­˜åœ¨ï¼Œæ‰¹é‡å¢é‡æ·»åŠ æ–‡æ¡£ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨insert_ref_docsæ‰¹é‡æ’å…¥ï¼‰
                insert_start_time = time.time()
                
                # ä½¿ç”¨insert_ref_docsæ‰¹é‡æ’å…¥ï¼Œæ€§èƒ½è¿œä¼˜äºé€ä¸ªinsert
                # LlamaIndexä¼šè‡ªåŠ¨æ‰¹å¤„ç†embeddingè®¡ç®—å’Œå‘é‡å­˜å‚¨å†™å…¥
                try:
                    self._index.insert_ref_docs(documents, show_progress=show_progress)
                    insert_elapsed = time.time() - insert_start_time
                    print(f"âœ… æ–‡æ¡£å·²æ‰¹é‡æ·»åŠ åˆ°ç°æœ‰ç´¢å¼• (è€—æ—¶: {insert_elapsed:.2f}s)")
                    logger.info(
                        f"æ‰¹é‡å¢é‡æ·»åŠ å®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, "
                        f"è€—æ—¶{insert_elapsed:.2f}s, "
                        f"å¹³å‡{insert_elapsed/len(documents):.3f}s/æ–‡æ¡£"
                    )
                except AttributeError:
                    # å¦‚æœinsert_ref_docsä¸å­˜åœ¨ï¼Œå›é€€åˆ°æ‰¹é‡æ’å…¥èŠ‚ç‚¹çš„æ–¹å¼
                    logger.warning("insert_ref_docsä¸å¯ç”¨ï¼Œä½¿ç”¨èŠ‚ç‚¹æ‰¹é‡æ’å…¥æ–¹å¼")
                    from llama_index.core.node_parser import SentenceSplitter
                    node_parser = SentenceSplitter(
                        chunk_size=self.chunk_size,
                        chunk_overlap=self.chunk_overlap
                    )
                    
                    # å…ˆæ‰¹é‡åˆ†å—æ‰€æœ‰æ–‡æ¡£ï¼Œè·å–æ€»èŠ‚ç‚¹æ•°
                    all_nodes = []
                    if show_progress:
                        print("   æ­£åœ¨åˆ†å—æ–‡æ¡£...")
                    for doc in tqdm(documents, desc="åˆ†å—", disable=not show_progress, unit="doc"):
                        nodes = node_parser.get_nodes_from_documents([doc])
                        all_nodes.extend(nodes)
                    
                    total_nodes = len(all_nodes)
                    logger.info(f"æ–‡æ¡£åˆ†å—å®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£ -> {total_nodes}ä¸ªèŠ‚ç‚¹")
                    
                    # æ‰¹é‡æ’å…¥èŠ‚ç‚¹ï¼ˆä½¿ç”¨insert_nodesæ‰¹é‡æ’å…¥ï¼Œæ€§èƒ½è¿œä¼˜äºé€ä¸ªinsertï¼‰
                    batch_size = config.EMBED_BATCH_SIZE  # ä½¿ç”¨embedæ‰¹å¤„ç†å¤§å°
                    inserted_count = 0
                    
                    if show_progress:
                        # ä½¿ç”¨tqdmæ˜¾ç¤ºçœŸå®è¿›åº¦æ¡ï¼ŒåŒ…å«é€Ÿç‡ä¿¡æ¯
                        pbar = tqdm(
                            total=total_nodes,
                            desc="å‘é‡åŒ–å¹¶æ’å…¥",
                            unit="node",
                            unit_scale=True,
                            ncols=100,
                            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'
                        )
                    
                    batch_start_time = time.time()
                    # å°è¯•ä½¿ç”¨æ‰¹é‡æ’å…¥æ–¹æ³•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                    try:
                        # æ£€æŸ¥æ˜¯å¦æœ‰ insert_nodes æ–¹æ³•
                        if hasattr(self._index, 'insert_nodes'):
                            # ä½¿ç”¨æ‰¹é‡æ’å…¥ï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰
                            for i in range(0, len(all_nodes), batch_size):
                                batch_nodes = all_nodes[i:i+batch_size]
                                self._index.insert_nodes(batch_nodes)
                                inserted_count += len(batch_nodes)
                                
                                if show_progress:
                                    pbar.update(len(batch_nodes))
                                    # æ¯å¤„ç†ä¸€å®šæ•°é‡èŠ‚ç‚¹åæ›´æ–°é€Ÿç‡
                                    if inserted_count % (batch_size * 10) == 0:
                                        elapsed = time.time() - batch_start_time
                                        rate = inserted_count / elapsed if elapsed > 0 else 0
                                        pbar.set_postfix({'é€Ÿç‡': f'{rate:.1f} nodes/s'})
                        else:
                            # å›é€€åˆ°é€ä¸ªæ’å…¥ï¼ˆä½†è‡³å°‘æ˜¯æ‰¹å¤„ç†embeddingï¼‰
                            raise AttributeError("insert_nodes not available")
                    except (AttributeError, TypeError):
                        # å¦‚æœæ‰¹é‡æ’å…¥ä¸å¯ç”¨ï¼Œä½¿ç”¨é€ä¸ªæ’å…¥ï¼ˆä½†ä»æ‰¹å¤„ç†embeddingï¼‰
                        logger.debug("insert_nodesä¸å¯ç”¨ï¼Œä½¿ç”¨é€ä¸ªinsertï¼ˆLlamaIndexä¼šè‡ªåŠ¨æ‰¹å¤„ç†embeddingï¼‰")
                        for i in range(0, len(all_nodes), batch_size):
                            batch_nodes = all_nodes[i:i+batch_size]
                            # é€ä¸ªæ’å…¥èŠ‚ç‚¹ï¼ˆLlamaIndexå†…éƒ¨ä¼šæ‰¹å¤„ç†embeddingè®¡ç®—ï¼‰
                            for node in batch_nodes:
                                self._index.insert(node)
                            inserted_count += len(batch_nodes)
                            
                            if show_progress:
                                pbar.update(len(batch_nodes))
                                # æ¯å¤„ç†ä¸€å®šæ•°é‡èŠ‚ç‚¹åæ›´æ–°é€Ÿç‡
                                if inserted_count % (batch_size * 10) == 0:
                                    elapsed = time.time() - batch_start_time
                                    rate = inserted_count / elapsed if elapsed > 0 else 0
                                    pbar.set_postfix({'é€Ÿç‡': f'{rate:.1f} nodes/s'})
                    
                    if show_progress:
                        pbar.close()
                    
                    insert_elapsed = time.time() - insert_start_time
                    avg_rate = total_nodes / insert_elapsed if insert_elapsed > 0 else 0
                    print(f"âœ… æ–‡æ¡£å·²æ‰¹é‡æ·»åŠ åˆ°ç°æœ‰ç´¢å¼• (è€—æ—¶: {insert_elapsed:.2f}s, å¹³å‡é€Ÿç‡: {avg_rate:.1f} nodes/s)")
                    logger.info(
                        f"æ‰¹é‡å¢é‡æ·»åŠ å®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, {total_nodes}ä¸ªèŠ‚ç‚¹, "
                        f"è€—æ—¶{insert_elapsed:.2f}s, "
                        f"å¹³å‡é€Ÿç‡={avg_rate:.1f} nodes/s"
                    )
            
            # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
            stats = self.get_stats()
            total_elapsed = time.time() - start_time
            
            print(f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡: {stats}")
            device_info = f"{device} ({'GPUåŠ é€Ÿ' if device.startswith('cuda') else 'CPUæ¨¡å¼'})"
            logger.info(
                f"ç´¢å¼•æ„å»ºå®Œæˆ (è®¾å¤‡: {device_info}): "
                f"æ–‡æ¡£æ•°={len(documents)}, "
                f"å‘é‡æ•°={stats.get('document_count', 0)}, "
                f"æ€»è€—æ—¶={total_elapsed:.2f}s, "
                f"å¹³å‡={total_elapsed/len(documents):.3f}s/æ–‡æ¡£"
            )
            
            # æ„å»ºå‘é‡IDæ˜ å°„ï¼ˆä¼˜åŒ–ï¼šæ‰¹é‡æŸ¥è¯¢ï¼‰
            vector_ids_map_start = time.time()
            vector_ids_map = self._get_vector_ids_batch(
                [doc.metadata.get("file_path", "") for doc in documents 
                 if doc.metadata.get("file_path")]
            )
            vector_ids_elapsed = time.time() - vector_ids_map_start
            print(f"ğŸ“‹ å·²è®°å½• {len(vector_ids_map)} ä¸ªæ–‡ä»¶çš„å‘é‡IDæ˜ å°„ (è€—æ—¶: {vector_ids_elapsed:.2f}s)")
            logger.debug(f"å‘é‡IDæ˜ å°„æ„å»ºè€—æ—¶: {vector_ids_elapsed:.2f}s")
            
            # å¦‚æœæä¾›äº†ç¼“å­˜ç®¡ç†å™¨ï¼Œæ›´æ–°ç¼“å­˜çŠ¶æ€
            if cache_manager and task_id:
                if config.ENABLE_CACHE:
                    try:
                        docs_hash = self._compute_documents_hash(documents)
                        vector_count = stats.get('document_count', 0)
                        cache_manager.mark_step_completed(
                            task_id=task_id,
                            step_name=cache_manager.STEP_VECTORIZE,
                            input_hash=docs_hash,
                            vector_count=vector_count,
                            collection_name=self.collection_name
                        )
                    except Exception as e:
                        logger.warning(f"æ›´æ–°å‘é‡åŒ–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
            
            return self._index, vector_ids_map
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            
            # å¦‚æœæä¾›äº†ç¼“å­˜ç®¡ç†å™¨ï¼Œæ ‡è®°æ­¥éª¤å¤±è´¥
            if cache_manager and task_id:
                try:
                    cache_manager.mark_step_failed(
                        task_id=task_id,
                        step_name=cache_manager.STEP_VECTORIZE,
                        error_message=str(e)
                    )
                except Exception:
                    pass
            
            raise
    
    def get_embedding_instance(self) -> Optional[BaseEmbedding]:
        """è·å–ç»Ÿä¸€çš„Embeddingå®ä¾‹
        
        Returns:
            BaseEmbeddingå®ä¾‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
        """
        return self._embedding_instance
    
    def get_index(self) -> VectorStoreIndex:
        """è·å–ç°æœ‰ç´¢å¼•
        
        Returns:
            VectorStoreIndexå¯¹è±¡
        """
        if self._index is None:
            # å°è¯•ä»å·²æœ‰çš„å‘é‡å­˜å‚¨åŠ è½½
            try:
                self._index = VectorStoreIndex.from_vector_store(
                    vector_store=self.vector_store,
                    storage_context=self.storage_context,
                )
                print("âœ… ä»å‘é‡å­˜å‚¨åŠ è½½ç´¢å¼•æˆåŠŸ")
            except Exception as e:
                print(f"â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œå°†åœ¨æ·»åŠ æ–‡æ¡£ååˆ›å»º")
                # åˆ›å»ºä¸€ä¸ªç©ºç´¢å¼•
                self._index = VectorStoreIndex.from_documents(
                    [],
                    storage_context=self.storage_context,
                )
        
        return self._index
    
    def _print_database_info(self):
        """æ‰“å°æ•°æ®åº“å’Œcollectionçš„è¯¦ç»†ä¿¡æ¯"""
        try:
            # 1. åˆ—å‡ºæ‰€æœ‰collections
            try:
                all_collections = self.chroma_client.list_collections()
                print(f"\nğŸ“‹ æ•°æ®åº“ä¸­çš„Collectionsåˆ—è¡¨:")
                if all_collections:
                    for idx, coll in enumerate(all_collections, 1):
                        try:
                            coll_count = coll.count() if hasattr(coll, 'count') else 0
                            coll_name = coll.name if hasattr(coll, 'name') else str(coll)
                            print(f"   {idx}. {coll_name} - {coll_count} ä¸ªå‘é‡")
                            logger.info(f"Collection: {coll_name}, å‘é‡æ•°: {coll_count}")
                        except Exception as e:
                            coll_name = coll.name if hasattr(coll, 'name') else str(coll)
                            print(f"   {idx}. {coll_name} - æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯: {e}")
                else:
                    print("   (æ— collections)")
                    logger.info("æ•°æ®åº“ä¸­æš‚æ— collections")
            except Exception as e:
                logger.warning(f"è·å–collectionsåˆ—è¡¨å¤±è´¥: {e}")
                print(f"   âš ï¸  æ— æ³•åˆ—å‡ºcollections: {e}")
            
            # 2. æ£€æŸ¥å½“å‰collectionæ˜¯å¦å­˜åœ¨
            print(f"\nğŸ” æ£€æŸ¥ç›®æ ‡Collection: {self.collection_name}")
            try:
                existing_collection = self.chroma_client.get_collection(name=self.collection_name)
                collection_count = existing_collection.count()
                
                print(f"   âœ… Collectionå­˜åœ¨")
                print(f"   ğŸ“Š å‘é‡æ€»æ•°: {collection_count}")
                logger.info(f"Collection '{self.collection_name}' å­˜åœ¨ï¼Œå‘é‡æ•°: {collection_count}")
                
                # 3. è·å–collectionçš„è¯¦ç»†ä¿¡æ¯
                sample_data = None  # åˆå§‹åŒ–å˜é‡
                if collection_count > 0:
                    # è·å–æ ·æœ¬æ•°æ®ï¼ˆæœ€å¤š10æ¡ï¼‰
                    sample_limit = min(10, collection_count)
                    try:
                        sample_data = existing_collection.peek(limit=sample_limit)
                        
                        # ç»Ÿè®¡metadataä¸­çš„ä¿¡æ¯
                        file_paths = set()
                        repositories = set()
                        file_types = {}
                        
                        if sample_data and 'metadatas' in sample_data:
                            for metadata in sample_data['metadatas']:
                                if metadata:
                                    # æ”¶é›†æ–‡ä»¶è·¯å¾„
                                    if 'file_path' in metadata:
                                        file_paths.add(metadata['file_path'])
                                    
                                    # æ”¶é›†ä»“åº“ä¿¡æ¯
                                    if 'repository' in metadata:
                                        repositories.add(metadata['repository'])
                                    
                                    # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
                                    if 'file_name' in metadata:
                                        file_name = metadata['file_name']
                                        file_ext = Path(file_name).suffix.lower() if file_name else ''
                                        file_types[file_ext] = file_types.get(file_ext, 0) + 1
                        
                        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                        print(f"\n   ğŸ“ˆ Collectionç»Ÿè®¡ä¿¡æ¯:")
                        print(f"      â€¢ å‘é‡æ•°é‡: {collection_count}")
                        
                        if file_paths:
                            print(f"      â€¢ å”¯ä¸€æ–‡ä»¶è·¯å¾„æ•°: {len(file_paths)}")
                            if len(file_paths) <= 20:
                                print(f"      â€¢ æ–‡ä»¶è·¯å¾„åˆ—è¡¨:")
                                for fp in sorted(list(file_paths))[:20]:
                                    print(f"        - {fp}")
                            else:
                                print(f"      â€¢ æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå‰20ä¸ªï¼‰:")
                                for fp in sorted(list(file_paths))[:20]:
                                    print(f"        - {fp}")
                                print(f"        ... è¿˜æœ‰ {len(file_paths) - 20} ä¸ªæ–‡ä»¶")
                        
                        if repositories:
                            print(f"      â€¢ ä»“åº“åˆ—è¡¨:")
                            for repo in sorted(list(repositories)):
                                print(f"        - {repo}")
                        
                        if file_types:
                            print(f"      â€¢ æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
                            for ext, count in sorted(file_types.items(), key=lambda x: x[1], reverse=True):
                                ext_display = ext if ext else "(æ— æ‰©å±•å)"
                                print(f"        {ext_display}: {count} ä¸ª")
                        
                        # æ‰“å°æ ·æœ¬metadataï¼ˆå‰5æ¡ï¼‰
                        if sample_data and 'metadatas' in sample_data and sample_data['metadatas']:
                            print(f"\n   ğŸ“„ æ ·æœ¬æ•°æ®ï¼ˆå‰5æ¡ï¼‰:")
                            for idx, metadata in enumerate(sample_data['metadatas'][:5], 1):
                                if metadata:
                                    print(f"      {idx}. Metadata:")
                                    for key, value in metadata.items():
                                        # æˆªæ–­è¿‡é•¿çš„å€¼
                                        value_str = str(value)
                                        if len(value_str) > 100:
                                            value_str = value_str[:100] + "..."
                                        print(f"         {key}: {value_str}")
                                    
                                    # å¦‚æœæœ‰å¯¹åº”çš„æ–‡æ¡£ID
                                    if 'ids' in sample_data and idx <= len(sample_data['ids']):
                                        doc_id = sample_data['ids'][idx - 1]
                                        print(f"         id: {doc_id}")
                        
                        logger.info(
                            f"Collectionè¯¦æƒ…: å‘é‡æ•°={collection_count}, "
                            f"æ–‡ä»¶æ•°={len(file_paths)}, "
                            f"ä»“åº“æ•°={len(repositories)}, "
                            f"æ–‡ä»¶ç±»å‹={len(file_types)}"
                        )
                        
                    except Exception as e:
                        logger.warning(f"è·å–collectionæ ·æœ¬æ•°æ®å¤±è´¥: {e}")
                        print(f"   âš ï¸  æ— æ³•è·å–æ ·æœ¬æ•°æ®: {e}")
                    
                    # è·å–ç»´åº¦ä¿¡æ¯
                    try:
                        if existing_collection.metadata and 'embedding_dimension' in existing_collection.metadata:
                            dim = existing_collection.metadata['embedding_dimension']
                            print(f"   ğŸ“ Embeddingç»´åº¦: {dim}")
                            logger.info(f"Collectionç»´åº¦: {dim}")
                        elif sample_data and 'embeddings' in sample_data and sample_data['embeddings']:
                            dim = len(sample_data['embeddings'][0])
                            print(f"   ğŸ“ Embeddingç»´åº¦: {dim} (ä»æ ·æœ¬æ•°æ®æ£€æµ‹)")
                            logger.info(f"Collectionç»´åº¦: {dim} (ä»æ ·æœ¬æ•°æ®æ£€æµ‹)")
                    except Exception as e:
                        logger.debug(f"è·å–ç»´åº¦ä¿¡æ¯å¤±è´¥: {e}")
                else:
                    print(f"   â„¹ï¸  Collectionä¸ºç©º")
                    logger.info(f"Collection '{self.collection_name}' ä¸ºç©º")
                
            except Exception as e:
                # Collectionä¸å­˜åœ¨
                if "does not exist" in str(e) or "not found" in str(e).lower():
                    print(f"   â„¹ï¸  Collectionä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°collection")
                    logger.info(f"Collection '{self.collection_name}' ä¸å­˜åœ¨ï¼Œå°†åˆ›å»º")
                else:
                    print(f"   âš ï¸  æ£€æŸ¥collectionæ—¶å‡ºé”™: {e}")
                    logger.warning(f"æ£€æŸ¥collectionå¤±è´¥: {e}")
            
            print()  # ç©ºè¡Œåˆ†éš”
            
        except Exception as e:
            logger.error(f"æ‰“å°æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
            print(f"âš ï¸  æ‰“å°æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
    
    def _ensure_collection_dimension_match(self):
        """ç¡®ä¿collectionçš„embeddingç»´åº¦ä¸å½“å‰æ¨¡å‹åŒ¹é…
        
        å¦‚æœcollectionå·²å­˜åœ¨ä½†ç»´åº¦ä¸åŒ¹é…ï¼Œä¼šè‡ªåŠ¨åˆ é™¤å¹¶é‡æ–°åˆ›å»º
        """
        try:
            # é¦–å…ˆç¡®ä¿èƒ½è·å–å½“å‰embeddingæ¨¡å‹çš„ç»´åº¦ï¼ˆå¿…é¡»æˆåŠŸï¼‰
            model_dim = None
            dim_detection_methods = []
            
            # æ–¹æ³•1: å°è¯•ä»æ¨¡å‹å±æ€§è·å–
            if hasattr(self.embed_model, 'embed_dim'):
                model_dim = self.embed_model.embed_dim
                dim_detection_methods.append("embed_dimå±æ€§")
            elif hasattr(self.embed_model, '_model') and hasattr(self.embed_model._model, 'config'):
                # å°è¯•ä»transformersæ¨¡å‹configè·å–
                try:
                    model_dim = getattr(self.embed_model._model.config, 'hidden_size', None)
                    if model_dim:
                        dim_detection_methods.append("æ¨¡å‹config.hidden_size")
                except Exception as e:
                    logger.debug(f"ä»æ¨¡å‹configè·å–ç»´åº¦å¤±è´¥: {e}")
            
            # æ–¹æ³•2: é€šè¿‡å®é™…è®¡ç®—ä¸€ä¸ªæµ‹è¯•å‘é‡è·å–ç»´åº¦ï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
            if model_dim is None:
                try:
                    test_embedding = self.embed_model.get_query_embedding("test")
                    # ç¡®ä¿è½¬æ¢ä¸ºPythonæ ‡é‡æ•´æ•°ï¼Œé¿å…numpyæ•°ç»„ç±»å‹é—®é¢˜
                    if hasattr(test_embedding, 'shape') and len(test_embedding.shape) > 0:
                        # numpyæ•°ç»„ï¼Œä½¿ç”¨shapeçš„ç¬¬ä¸€ä¸ªç»´åº¦
                        model_dim = int(test_embedding.shape[0])
                    elif hasattr(test_embedding, '__len__'):
                        # åˆ—è¡¨æˆ–å…¶ä»–æœ‰é•¿åº¦çš„å¯¹è±¡
                        model_dim = int(len(test_embedding))
                    else:
                        # æ ‡é‡å€¼
                        model_dim = int(test_embedding)
                    dim_detection_methods.append("å®é™…è®¡ç®—æµ‹è¯•å‘é‡")
                except Exception as e:
                    logger.warning(f"é€šè¿‡æµ‹è¯•å‘é‡è·å–ç»´åº¦å¤±è´¥: {e}")
            
            # ç¡®ä¿model_dimæ˜¯Pythonæ ‡é‡æ•´æ•°
            if model_dim is not None:
                model_dim = int(model_dim)
            
            # å¦‚æœä»ç„¶æ— æ³•è·å–æ¨¡å‹ç»´åº¦ï¼Œè¿™æ˜¯ä¸¥é‡é”™è¯¯
            if model_dim is None:
                error_msg = "æ— æ³•æ£€æµ‹embeddingæ¨¡å‹ç»´åº¦ï¼Œè¿™å¯èƒ½å¯¼è‡´ç»´åº¦ä¸åŒ¹é…é”™è¯¯"
                logger.error(error_msg)
                print(f"âŒ {error_msg}")
                print(f"   å°è¯•çš„æ–¹æ³•: {dim_detection_methods}")
                raise ValueError(error_msg)
            
            logger.info(f"âœ… æˆåŠŸæ£€æµ‹åˆ°embeddingæ¨¡å‹ç»´åº¦: {model_dim} (æ–¹æ³•: {', '.join(dim_detection_methods)})")
            print(f"ğŸ“ å½“å‰embeddingæ¨¡å‹ç»´åº¦: {model_dim}")
            
            # å°è¯•è·å–ç°æœ‰collection
            try:
                existing_collection = self.chroma_client.get_collection(name=self.collection_name)
                
                # è·å–collectionçš„ç»´åº¦ï¼ˆä»metadataæˆ–æŸ¥è¯¢å®é™…æ•°æ®ï¼‰
                collection_dim = None
                collection_count = existing_collection.count()
                
                try:
                    # å°è¯•ä»collectionçš„metadataè·å–
                    if existing_collection.metadata and 'embedding_dimension' in existing_collection.metadata:
                        collection_dim = existing_collection.metadata['embedding_dimension']
                        # ç¡®ä¿è½¬æ¢ä¸ºPythonæ ‡é‡æ•´æ•°
                        collection_dim = int(collection_dim) if collection_dim is not None else None
                        logger.info(f"ä»collection metadataè·å–ç»´åº¦: {collection_dim}")
                    elif collection_count > 0:
                        # å¦‚æœcollectionæœ‰æ•°æ®ï¼Œå°è¯•æŸ¥è¯¢ä¸€ä¸ªå‘é‡è·å–ç»´åº¦
                        sample = existing_collection.peek(limit=1)
                        if sample and 'embeddings' in sample and sample['embeddings']:
                            embeddings_data = sample['embeddings']
                            # å¤„ç†ä¸åŒçš„æ•°æ®ç»“æ„ï¼šå¯èƒ½æ˜¯åˆ—è¡¨æˆ–numpyæ•°ç»„
                            if isinstance(embeddings_data, list) and len(embeddings_data) > 0:
                                first_embedding = embeddings_data[0]
                            else:
                                # å¦‚æœæ˜¯numpyæ•°ç»„æˆ–å…¶ä»–ç±»å‹ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨
                                first_embedding = embeddings_data[0] if hasattr(embeddings_data, '__getitem__') else embeddings_data
                            
                            # è·å–ç»´åº¦ï¼šä¼˜å…ˆä½¿ç”¨shapeï¼Œå¦‚æœå¤±è´¥åˆ™å°è¯•len
                            try:
                                if hasattr(first_embedding, 'shape') and len(first_embedding.shape) > 0:
                                    # numpyæ•°ç»„ï¼Œä½¿ç”¨shapeçš„ç¬¬ä¸€ä¸ªç»´åº¦
                                    collection_dim = int(first_embedding.shape[0])
                                elif hasattr(first_embedding, '__len__'):
                                    # åˆ—è¡¨æˆ–å…¶ä»–æœ‰é•¿åº¦çš„å¯¹è±¡
                                    collection_dim = int(len(first_embedding))
                                else:
                                    # æ ‡é‡å€¼
                                    collection_dim = int(first_embedding)
                            except (TypeError, ValueError) as dim_error:
                                logger.warning(f"æ— æ³•ä»embeddingæ•°æ®è·å–ç»´åº¦: {dim_error}, æ•°æ®ç±»å‹: {type(first_embedding)}")
                                collection_dim = None
                            
                            if collection_dim is not None:
                                logger.info(f"ä»collectionå®é™…æ•°æ®è·å–ç»´åº¦: {collection_dim}")
                except Exception as e:
                    logger.warning(f"è·å–collectionç»´åº¦å¤±è´¥: {e}")
                
                # å¦‚æœcollectionä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨ï¼ˆæ— éœ€æ£€æŸ¥ç»´åº¦ï¼‰
                if collection_count == 0:
                    self.chroma_collection = existing_collection
                    print(f"âœ… Collectionä¸ºç©ºï¼Œå¯ä»¥ä½¿ç”¨: {self.collection_name}")
                    logger.info(f"Collectionä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨: {self.collection_name}")
                # å¦‚æœcollectionæœ‰æ•°æ®ä½†æ— æ³•è·å–ç»´åº¦ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥ï¼šåˆ é™¤å¹¶é‡å»º
                elif collection_dim is None:
                    print(f"âš ï¸  Collectionæœ‰æ•°æ®ä½†æ— æ³•æ£€æµ‹ç»´åº¦ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥åˆ é™¤å¹¶é‡å»º")
                    print(f"   å½“å‰æ¨¡å‹ç»´åº¦: {model_dim}")
                    print(f"ğŸ”„ è‡ªåŠ¨åˆ é™¤æ—§collectionå¹¶é‡æ–°åˆ›å»º...")
                    
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.warning(f"å› æ— æ³•æ£€æµ‹ç»´åº¦ï¼Œå·²åˆ é™¤collection: {self.collection_name} (æ¨¡å‹ç»´åº¦: {model_dim})")
                    
                    # é‡æ–°åˆ›å»ºcollection
                    self.chroma_collection = self.chroma_client.get_or_create_collection(
                        name=self.collection_name
                    )
                    print(f"âœ… å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name}")
                    print(f"âš ï¸  **é‡è¦**: Collectionå·²é‡æ–°åˆ›å»ºï¼ŒåŸæœ‰æ•°æ®å·²è¢«æ¸…é™¤")
                    print(f"   ğŸ’¡ è¯·é‡æ–°å¯¼å…¥æ•°æ®ä»¥æ¢å¤ç´¢å¼•åŠŸèƒ½")
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œåˆ é™¤å¹¶é‡å»ºï¼ˆç¡®ä¿éƒ½æ˜¯æ•´æ•°åå†æ¯”è¾ƒï¼Œé¿å…numpyæ•°ç»„æ¯”è¾ƒé—®é¢˜ï¼‰
                elif int(model_dim) != int(collection_dim):
                    print(f"âš ï¸  æ£€æµ‹åˆ°embeddingç»´åº¦ä¸åŒ¹é…:")
                    print(f"   Collectionç»´åº¦: {collection_dim}")
                    print(f"   å½“å‰æ¨¡å‹ç»´åº¦: {model_dim}")
                    print(f"ğŸ”„ è‡ªåŠ¨åˆ é™¤æ—§collectionå¹¶é‡æ–°åˆ›å»º...")
                    
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.info(f"å·²åˆ é™¤ç»´åº¦ä¸åŒ¹é…çš„collection: {self.collection_name} (ç»´åº¦: {collection_dim} -> {model_dim})")
                    
                    # é‡æ–°åˆ›å»ºcollection
                    self.chroma_collection = self.chroma_client.get_or_create_collection(
                        name=self.collection_name
                    )
                    print(f"âœ… å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name} (ç»´åº¦: {model_dim})")
                    print(f"âš ï¸  **é‡è¦**: Collectionå·²é‡æ–°åˆ›å»ºï¼ŒåŸæœ‰æ•°æ®å·²è¢«æ¸…é™¤")
                    print(f"   ğŸ’¡ è¯·é‡æ–°å¯¼å…¥æ•°æ®ä»¥æ¢å¤ç´¢å¼•åŠŸèƒ½")
                else:
                    # ç»´åº¦åŒ¹é…ï¼Œä½¿ç”¨ç°æœ‰collection
                    self.chroma_collection = existing_collection
                    print(f"âœ… Collectionç»´åº¦æ£€æŸ¥é€šè¿‡: {model_dim}ç»´")
                    logger.info(f"Collectionç»´åº¦åŒ¹é…: {model_dim}ç»´")
                    
            except Exception as e:
                # Collectionä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                if "does not exist" in str(e) or "not found" in str(e).lower():
                    self.chroma_collection = self.chroma_client.get_or_create_collection(
                        name=self.collection_name
                    )
                    print(f"âœ… åˆ›å»ºæ–°collection: {self.collection_name} (ç»´åº¦: {model_dim})")
                    logger.info(f"åˆ›å»ºæ–°collection: {self.collection_name} (ç»´åº¦: {model_dim})")
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                    logger.error(f"è·å–collectionæ—¶å‡ºé”™: {e}")
                    raise
                    
        except Exception as e:
            # å¦‚æœæ£€æµ‹è¿‡ç¨‹å‡ºé”™ï¼Œå°è¯•åˆ é™¤æ—§collectionå¹¶é‡å»ºï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            logger.error(f"ç»´åº¦æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
            logger.info("é‡‡ç”¨ä¿å®ˆç­–ç•¥ï¼šåˆ é™¤æ—§collectionå¹¶é‡å»º")
            
            try:
                # å°è¯•åˆ é™¤æ—§collectionï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                try:
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.info(f"å·²åˆ é™¤å¯èƒ½ä¸å…¼å®¹çš„collection: {self.collection_name}")
                    print(f"ğŸ”„ å·²åˆ é™¤å¯èƒ½ä¸å…¼å®¹çš„collection: {self.collection_name}")
                except:
                    # å¦‚æœåˆ é™¤å¤±è´¥ï¼ˆcollectionä¸å­˜åœ¨ï¼‰ï¼Œç»§ç»­åˆ›å»ºæ–°collection
                    pass
                
                # åˆ›å»ºæ–°collection
                self.chroma_collection = self.chroma_client.get_or_create_collection(
                    name=self.collection_name
                )
                print(f"âœ… å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name}")
                logger.info(f"å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name}")
            except Exception as fallback_error:
                logger.error(f"å›é€€åˆ›å»ºcollectionä¹Ÿå¤±è´¥: {fallback_error}")
                raise
    
    def clear_index(self):
        """æ¸…ç©ºç´¢å¼•"""
        try:
            # åˆ é™¤é›†åˆ
            self.chroma_client.delete_collection(name=self.collection_name)
            print(f"âœ… å·²åˆ é™¤é›†åˆ: {self.collection_name}")
            
            # é‡æ–°åˆ›å»ºé›†åˆ
            self.chroma_collection = self.chroma_client.get_or_create_collection(
                name=self.collection_name
            )
            self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
            self.storage_context = StorageContext.from_defaults(
                vector_store=self.vector_store
            )
            
            # é‡ç½®ç´¢å¼•
            self._index = None
            print("âœ… ç´¢å¼•å·²æ¸…ç©º")
            
        except Exception as e:
            print(f"âŒ æ¸…ç©ºç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def get_stats(self) -> dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # æ£€æŸ¥chroma_collectionæ˜¯å¦å·²åˆå§‹åŒ–
            if not hasattr(self, 'chroma_collection') or self.chroma_collection is None:
                logger.warning("âš ï¸  chroma_collectionæœªåˆå§‹åŒ–ï¼Œæ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯")
                print(f"âš ï¸  chroma_collectionæœªåˆå§‹åŒ–ï¼Œæ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯")
                return {
                    "collection_name": self.collection_name,
                    "document_count": 0,
                    "embedding_model": self.embedding_model_name,
                    "chunk_size": self.chunk_size,
                    "chunk_overlap": self.chunk_overlap,
                    "error": "chroma_collectionæœªåˆå§‹åŒ–"
                }
            
            count = self.chroma_collection.count()
            logger.debug(f"Collection '{self.collection_name}' å‘é‡æ•°é‡: {count}")
            
            return {
                "collection_name": self.collection_name,
                "document_count": count,
                "embedding_model": self.embedding_model_name,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
            }
        except AttributeError as e:
            error_msg = f"chroma_collectionå±æ€§è®¿é—®å¤±è´¥: {e}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            return {
                "collection_name": self.collection_name,
                "document_count": 0,
                "embedding_model": self.embedding_model_name,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "error": str(e)
            }
        except Exception as e:
            error_msg = f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}"
            logger.error(error_msg, exc_info=True)
            print(f"âŒ {error_msg}")
            return {
                "collection_name": self.collection_name,
                "document_count": 0,
                "embedding_model": self.embedding_model_name,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "error": str(e)
            }
    
    def search(self, query: str, top_k: int = 5) -> List[dict]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if self._index is None:
            self.get_index()
        
        retriever = self._index.as_retriever(similarity_top_k=top_k)
        nodes = retriever.retrieve(query)
        
        results = []
        for node in nodes:
            results.append({
                "text": node.node.text,
                "score": node.score,
                "metadata": node.node.metadata,
            })
        
        return results
    
    def incremental_update(
        self,
        added_docs: List[LlamaDocument],
        modified_docs: List[LlamaDocument],
        deleted_file_paths: List[str],
        metadata_manager=None
    ) -> dict:
        """æ‰§è¡Œå¢é‡æ›´æ–°
        
        Args:
            added_docs: æ–°å¢çš„æ–‡æ¡£åˆ—è¡¨
            modified_docs: ä¿®æ”¹çš„æ–‡æ¡£åˆ—è¡¨
            deleted_file_paths: åˆ é™¤çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨å®ä¾‹ï¼ˆç”¨äºæŸ¥è¯¢å‘é‡IDï¼‰
            
        Returns:
            æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {
            "added": 0,
            "modified": 0,
            "deleted": 0,
            "errors": []
        }
        
        # ç¡®ä¿ç´¢å¼•å­˜åœ¨
        if self._index is None:
            self.get_index()
        
        # 1. å¤„ç†æ–°å¢
        if added_docs:
            try:
                added_count, added_vector_ids = self._add_documents(added_docs)
                stats["added"] = added_count
                print(f"âœ… æ–°å¢ {added_count} ä¸ªæ–‡æ¡£")
                
                # æ›´æ–°å…ƒæ•°æ®çš„å‘é‡ID
                if metadata_manager and added_docs:
                    for doc in added_docs:
                        file_path = doc.metadata.get("file_path", "")
                        if file_path and file_path in added_vector_ids:
                            owner = doc.metadata.get("repository", "").split("/")[0] if "/" in doc.metadata.get("repository", "") else ""
                            repo = doc.metadata.get("repository", "").split("/")[1] if "/" in doc.metadata.get("repository", "") else ""
                            branch = doc.metadata.get("branch", "main")
                            
                            if owner and repo:
                                metadata_manager.update_file_vector_ids(
                                    owner, repo, branch, file_path,
                                    added_vector_ids[file_path]
                                )
            except Exception as e:
                error_msg = f"æ–°å¢æ–‡æ¡£å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)
        
        # 2. å¤„ç†ä¿®æ”¹ï¼ˆå…ˆæ‰¹é‡åˆ é™¤æ—§çš„ï¼Œå†æ‰¹é‡æ·»åŠ æ–°çš„ï¼‰
        if modified_docs:
            try:
                # ä¼˜åŒ–ï¼šæ‰¹é‡æ”¶é›†æ‰€æœ‰éœ€è¦åˆ é™¤çš„å‘é‡ID
                all_vector_ids_to_delete = []
                for doc in modified_docs:
                    file_path = doc.metadata.get("file_path", "")
                    if file_path and metadata_manager:
                        # ä»å…ƒæ•°æ®ä¸­è·å–æ—§çš„å‘é‡ID
                        owner = doc.metadata.get("repository", "").split("/")[0] if "/" in doc.metadata.get("repository", "") else ""
                        repo = doc.metadata.get("repository", "").split("/")[1] if "/" in doc.metadata.get("repository", "") else ""
                        branch = doc.metadata.get("branch", "main")
                        
                        vector_ids = metadata_manager.get_file_vector_ids(owner, repo, branch, file_path)
                        if vector_ids:
                            all_vector_ids_to_delete.extend(vector_ids)
                
                # æ‰¹é‡åˆ é™¤æ‰€æœ‰æ—§å‘é‡ï¼ˆä¼˜åŒ–ï¼šä¸€æ¬¡æ€§åˆ é™¤ï¼‰
                deleted_vector_count = 0
                if all_vector_ids_to_delete:
                    # å»é‡
                    unique_vector_ids = list(set(all_vector_ids_to_delete))
                    # åˆ†æ‰¹åˆ é™¤ä»¥é¿å…å•æ¬¡åˆ é™¤è¿‡å¤šæ•°æ®
                    batch_delete_size = 100
                    for i in range(0, len(unique_vector_ids), batch_delete_size):
                        batch_ids = unique_vector_ids[i:i+batch_delete_size]
                        self._delete_vectors_by_ids(batch_ids)
                        deleted_vector_count += len(batch_ids)
                
                # æ‰¹é‡æ·»åŠ æ–°ç‰ˆæœ¬
                modified_count, modified_vector_ids = self._add_documents(modified_docs)
                stats["modified"] = modified_count
                print(f"âœ… æ›´æ–° {modified_count} ä¸ªæ–‡æ¡£ï¼ˆæ‰¹é‡åˆ é™¤ {deleted_vector_count} ä¸ªæ—§å‘é‡ï¼‰")
                
                # æ›´æ–°å…ƒæ•°æ®çš„å‘é‡ID
                if metadata_manager and modified_docs:
                    for doc in modified_docs:
                        file_path = doc.metadata.get("file_path", "")
                        if file_path and file_path in modified_vector_ids:
                            owner = doc.metadata.get("repository", "").split("/")[0] if "/" in doc.metadata.get("repository", "") else ""
                            repo = doc.metadata.get("repository", "").split("/")[1] if "/" in doc.metadata.get("repository", "") else ""
                            branch = doc.metadata.get("branch", "main")
                            
                            if owner and repo:
                                metadata_manager.update_file_vector_ids(
                                    owner, repo, branch, file_path,
                                    modified_vector_ids[file_path]
                                )
            except Exception as e:
                error_msg = f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)
        
        # 3. å¤„ç†åˆ é™¤
        if deleted_file_paths and metadata_manager:
            try:
                deleted_count = self._delete_documents(deleted_file_paths, metadata_manager)
                stats["deleted"] = deleted_count
                print(f"âœ… åˆ é™¤ {deleted_count} ä¸ªæ–‡æ¡£")
            except Exception as e:
                error_msg = f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}"
                print(f"âŒ {error_msg}")
                stats["errors"].append(error_msg)
        
        return stats
    
    def _add_documents(self, documents: List[LlamaDocument]) -> Tuple[int, Dict[str, List[str]]]:
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡æ’å…¥ï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            (æˆåŠŸæ·»åŠ çš„æ–‡æ¡£æ•°é‡, æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDçš„æ˜ å°„)
        """
        if not documents:
            return 0, {}
        
        try:
            # ä¼˜åŒ–ï¼šä½¿ç”¨æ‰¹é‡æ’å…¥æ›¿ä»£é€ä¸ªæ’å…¥
            # ä¼˜å…ˆå°è¯•ä½¿ç”¨insert_ref_docsæ‰¹é‡æ’å…¥
            try:
                self._index.insert_ref_docs(documents, show_progress=False)
                count = len(documents)
            except AttributeError:
                # å¦‚æœinsert_ref_docsä¸å¯ç”¨ï¼Œä½¿ç”¨èŠ‚ç‚¹æ‰¹é‡æ’å…¥
                from llama_index.core.node_parser import SentenceSplitter
                node_parser = SentenceSplitter(
                    chunk_size=self.chunk_size,
                    chunk_overlap=self.chunk_overlap
                )
                # æ‰¹é‡åˆ†å—å¹¶æ’å…¥èŠ‚ç‚¹
                all_nodes = node_parser.get_nodes_from_documents(documents)
                for node in all_nodes:
                    self._index.insert(node)
                count = len(documents)
            except Exception as e:
                # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªæ’å…¥ï¼ˆä¿ç•™å®¹é”™èƒ½åŠ›ï¼‰
                logger.warning(f"æ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå›é€€åˆ°é€ä¸ªæ’å…¥: {e}")
                count = 0
                for doc in documents:
                    try:
                        self._index.insert(doc)
                        count += 1
                    except Exception as insert_error:
                        print(f"âš ï¸  æ·»åŠ æ–‡æ¡£å¤±è´¥ [{doc.metadata.get('file_path', 'unknown')}]: {insert_error}")
                        logger.warning(f"æ·»åŠ æ–‡æ¡£å¤±è´¥: {insert_error}")
        except Exception as e:
            logger.error(f"æ‰¹é‡æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            print(f"âŒ æ‰¹é‡æ·»åŠ æ–‡æ¡£å¤±è´¥: {e}")
            return 0, {}
        
        # ä¼˜åŒ–ï¼šæ‰¹é‡æŸ¥è¯¢å‘é‡IDæ˜ å°„
        file_paths = [doc.metadata.get("file_path", "") for doc in documents 
                     if doc.metadata.get("file_path")]
        vector_ids_map = self._get_vector_ids_batch(file_paths)
        
        return count, vector_ids_map
    
    def _delete_documents(self, file_paths: List[str], metadata_manager) -> int:
        """æ‰¹é‡åˆ é™¤æ–‡æ¡£
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            metadata_manager: å…ƒæ•°æ®ç®¡ç†å™¨å®ä¾‹
            
        Returns:
            æˆåŠŸåˆ é™¤çš„æ–‡æ¡£æ•°é‡
        """
        deleted_count = 0
        
        for file_path in file_paths:
            # éœ€è¦ä»æ–‡æ¡£å…ƒæ•°æ®ä¸­æå–ä»“åº“ä¿¡æ¯
            # è¿™é‡Œå‡è®¾æ–‡ä»¶è·¯å¾„åŒ…å«ä»“åº“ä¿¡æ¯
            # å®é™…ä½¿ç”¨æ—¶éœ€è¦ä¼ é€’å®Œæ•´çš„ä»“åº“ä¿¡æ¯
            # æš‚æ—¶è·³è¿‡ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯
            pass
        
        return deleted_count
    
    def _get_vector_ids_by_metadata(self, file_path: str) -> List[str]:
        """é€šè¿‡æ–‡ä»¶è·¯å¾„æŸ¥è¯¢å¯¹åº”çš„å‘é‡IDåˆ—è¡¨
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å‘é‡IDåˆ—è¡¨
        """
        if not file_path:
            return []
        
        try:
            # æŸ¥è¯¢ Chroma collectionï¼ŒåŒ¹é… file_path
            results = self.chroma_collection.get(
                where={"file_path": file_path}
            )
            return results.get('ids', []) if results else []
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢å‘é‡IDå¤±è´¥ [{file_path}]: {e}")
            return []
    
    def _get_vector_ids_batch(self, file_paths: List[str]) -> Dict[str, List[str]]:
        """æ‰¹é‡æŸ¥è¯¢å‘é‡IDæ˜ å°„ï¼ˆä¼˜åŒ–ï¼šå‡å°‘æŸ¥è¯¢æ¬¡æ•°ï¼‰
        
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ–‡ä»¶è·¯å¾„åˆ°å‘é‡IDåˆ—è¡¨çš„æ˜ å°„å­—å…¸
        """
        if not file_paths:
            return {}
        
        # å»é‡
        unique_paths = list(set(file_paths))
        vector_ids_map = {}
        
        try:
            # Chroma ä¸æ”¯æŒæ‰¹é‡ where æ¡ä»¶æŸ¥è¯¢ï¼Œä½†ä»å¯ä»¥ä¼˜åŒ–ï¼š
            # 1. å‡å°‘é‡å¤æŸ¥è¯¢ï¼ˆé€šè¿‡å»é‡ï¼‰
            # 2. æ‰¹é‡è·å–æ‰€æœ‰æ•°æ®ç„¶åè¿‡æ»¤ï¼ˆé€‚ç”¨äºæ•°æ®é‡ä¸å¤§çš„æƒ…å†µï¼‰
            # 3. æˆ–è€…ç»§ç»­é€ä¸ªæŸ¥è¯¢ä½†å»æ‰é‡å¤
            
            # æ–¹æ¡ˆï¼šåˆ†æ‰¹æŸ¥è¯¢ä»¥é¿å…ä¸€æ¬¡æ€§åŠ è½½è¿‡å¤šæ•°æ®
            batch_size = 50  # æ¯æ‰¹æŸ¥è¯¢50ä¸ªæ–‡ä»¶è·¯å¾„
            total_results = 0
            
            for i in range(0, len(unique_paths), batch_size):
                batch_paths = unique_paths[i:i+batch_size]
                for file_path in batch_paths:
                    vector_ids = self._get_vector_ids_by_metadata(file_path)
                    if vector_ids:
                        vector_ids_map[file_path] = vector_ids
                        total_results += len(vector_ids)
            
            logger.debug(
                f"æ‰¹é‡æŸ¥è¯¢å‘é‡ID: "
                f"è¾“å…¥{len(file_paths)}ä¸ªè·¯å¾„(å»é‡å{len(unique_paths)}ä¸ª), "
                f"æ‰¾åˆ°{len(vector_ids_map)}ä¸ªæ–‡ä»¶, "
                f"å…±{total_results}ä¸ªå‘é‡"
            )
        except Exception as e:
            logger.error(f"æ‰¹é‡æŸ¥è¯¢å‘é‡IDå¤±è´¥: {e}")
            # å›é€€åˆ°é€ä¸ªæŸ¥è¯¢
            for file_path in unique_paths:
                try:
                    vector_ids = self._get_vector_ids_by_metadata(file_path)
                    if vector_ids:
                        vector_ids_map[file_path] = vector_ids
                except Exception as query_error:
                    logger.warning(f"æŸ¥è¯¢å•ä¸ªå‘é‡IDå¤±è´¥ [{file_path}]: {query_error}")
        
        return vector_ids_map
    
    def _delete_vectors_by_ids(self, vector_ids: List[str]):
        """æ ¹æ®å‘é‡IDåˆ é™¤å‘é‡
        
        Args:
            vector_ids: å‘é‡IDåˆ—è¡¨
        """
        if not vector_ids:
            return
        
        try:
            self.chroma_collection.delete(ids=vector_ids)
        except Exception as e:
            print(f"âš ï¸  åˆ é™¤å‘é‡å¤±è´¥: {e}")
            raise
    
    def get_node_ids_for_document(self, doc_id: str) -> List[str]:
        """è·å–æ–‡æ¡£å¯¹åº”çš„æ‰€æœ‰èŠ‚ç‚¹ID
        
        Args:
            doc_id: æ–‡æ¡£ID
            
        Returns:
            èŠ‚ç‚¹IDåˆ—è¡¨
        """
        # LlamaIndex ä½¿ç”¨èŠ‚ç‚¹ï¼ˆNodeï¼‰çš„æ¦‚å¿µï¼Œæ¯ä¸ªæ–‡æ¡£åˆ†å—åä¼šç”Ÿæˆå¤šä¸ªèŠ‚ç‚¹
        # éœ€è¦æŸ¥è¯¢ Chroma collection æ¥è·å–ä¸æ–‡æ¡£ç›¸å…³çš„æ‰€æœ‰èŠ‚ç‚¹
        try:
            # æŸ¥è¯¢æ‰€æœ‰æ•°æ®ï¼Œç„¶åè¿‡æ»¤å‡ºåŒ¹é…çš„
            # æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ï¼Œå¤§è§„æ¨¡æ•°æ®æ—¶éœ€è¦ä¼˜åŒ–
            result = self.chroma_collection.get()
            
            if not result or 'ids' not in result:
                return []
            
            # è¿”å›æ‰€æœ‰IDï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥æ ¹æ®å…ƒæ•°æ®è¿‡æ»¤ï¼‰
            return result['ids']
        except Exception as e:
            print(f"âš ï¸  æŸ¥è¯¢èŠ‚ç‚¹IDå¤±è´¥: {e}")
            return []
    
    @staticmethod
    def _compute_documents_hash(documents: List[LlamaDocument]) -> str:
        """è®¡ç®—æ–‡æ¡£åˆ—è¡¨çš„å“ˆå¸Œå€¼
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            MD5å“ˆå¸Œå€¼
        """
        import hashlib
        import json
        
        # åŸºäºæ–‡æ¡£æ–‡æœ¬å†…å®¹å’Œå…ƒæ•°æ®è®¡ç®—å“ˆå¸Œ
        docs_data = []
        for doc in documents:
            docs_data.append({
                "text": doc.text[:1000],  # åªä½¿ç”¨å‰1000å­—ç¬¦ä»¥æé«˜æ€§èƒ½
                "file_path": doc.metadata.get("file_path", ""),
                "file_name": doc.metadata.get("file_name", "")
            })
        
        docs_str = json.dumps(docs_data, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(docs_str.encode('utf-8')).hexdigest()
    
    def _check_vectors_exist(self, documents: List[LlamaDocument]) -> bool:
        """æ£€æŸ¥æ–‡æ¡£å¯¹åº”çš„å‘é‡æ˜¯å¦å·²å­˜åœ¨äº Chroma ä¸­
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            å¦‚æœæ‰€æœ‰æ–‡æ¡£çš„å‘é‡éƒ½å­˜åœ¨è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            # æ£€æŸ¥ collection æ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•°æ®
            if not hasattr(self, 'chroma_collection'):
                return False
            
            collection_count = self.chroma_collection.count()
            if collection_count == 0:
                return False
            
            # æ£€æŸ¥æ¯ä¸ªæ–‡æ¡£æ˜¯å¦éƒ½æœ‰å¯¹åº”çš„å‘é‡
            # ç®€åŒ–ç‰ˆæœ¬ï¼šå¦‚æœ collection æœ‰æ•°æ®ä¸”æ–‡æ¡£æ•°é‡åˆç†ï¼Œè®¤ä¸ºå·²å­˜åœ¨
            # æ›´ä¸¥æ ¼çš„æ£€æŸ¥å¯ä»¥é€šè¿‡æŸ¥è¯¢æ¯ä¸ªæ–‡ä»¶çš„å‘é‡IDå®ç°
            file_paths = [doc.metadata.get("file_path", "") for doc in documents if doc.metadata.get("file_path")]
            if not file_paths:
                return False
            
            # æ£€æŸ¥è‡³å°‘ä¸€éƒ¨åˆ†æ–‡ä»¶çš„å‘é‡æ˜¯å¦å­˜åœ¨
            sample_paths = file_paths[:min(5, len(file_paths))]  # æ£€æŸ¥å‰5ä¸ªæ–‡ä»¶
            existing_count = 0
            for file_path in sample_paths:
                vector_ids = self._get_vector_ids_by_metadata(file_path)
                if vector_ids:
                    existing_count += 1
            
            # å¦‚æœæ‰€æœ‰æ ·æœ¬æ–‡ä»¶éƒ½æœ‰å‘é‡ï¼Œè®¤ä¸ºç¼“å­˜æœ‰æ•ˆ
            return existing_count == len(sample_paths)
            
        except Exception as e:
            logger.warning(f"æ£€æŸ¥å‘é‡å­˜åœ¨æ€§å¤±è´¥: {e}")
            return False
    
    def _filter_vectorized_documents(
        self, 
        documents: List[LlamaDocument]
    ) -> Tuple[List[LlamaDocument], int]:
        """è¿‡æ»¤å·²å‘é‡åŒ–çš„æ–‡æ¡£ï¼Œå®ç°æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ 
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            (å¾…å¤„ç†çš„æ–‡æ¡£åˆ—è¡¨, å·²å‘é‡åŒ–çš„æ–‡æ¡£æ•°é‡)
        """
        if not documents:
            return [], 0
        
        # ç¡®ä¿ç´¢å¼•å·²åˆå§‹åŒ–
        if self._index is None:
            self.get_index()
        
        # æ£€æŸ¥ collection æ˜¯å¦å­˜åœ¨
        if not hasattr(self, 'chroma_collection'):
            return documents, 0
        
        try:
            collection_count = self.chroma_collection.count()
            if collection_count == 0:
                # å¦‚æœ collection ä¸ºç©ºï¼Œæ‰€æœ‰æ–‡æ¡£éƒ½éœ€è¦å¤„ç†
                return documents, 0
            
            # æ£€æŸ¥æ¯ä¸ªæ–‡æ¡£æ˜¯å¦å·²å‘é‡åŒ–
            documents_to_process = []
            already_vectorized_count = 0
            
            for doc in documents:
                file_path = doc.metadata.get("file_path", "")
                if not file_path:
                    # å¦‚æœæ²¡æœ‰ file_pathï¼Œæ— æ³•æ£€æŸ¥ï¼Œéœ€è¦å¤„ç†
                    documents_to_process.append(doc)
                    continue
                
                # æŸ¥è¯¢è¯¥æ–‡ä»¶æ˜¯å¦å·²æœ‰å‘é‡
                vector_ids = self._get_vector_ids_by_metadata(file_path)
                if vector_ids:
                    # è¯¥æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡
                    already_vectorized_count += 1
                    logger.debug(f"æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡: {file_path}")
                else:
                    # è¯¥æ–‡æ¡£æœªå‘é‡åŒ–ï¼Œéœ€è¦å¤„ç†
                    documents_to_process.append(doc)
            
            if already_vectorized_count > 0:
                logger.info(
                    f"æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ : "
                    f"æ€»æ–‡æ¡£æ•°={len(documents)}, "
                    f"å·²å‘é‡åŒ–={already_vectorized_count}, "
                    f"å¾…å¤„ç†={len(documents_to_process)}"
                )
            
            return documents_to_process, already_vectorized_count
            
        except Exception as e:
            logger.warning(f"è¿‡æ»¤å·²å‘é‡åŒ–æ–‡æ¡£å¤±è´¥: {e}ï¼Œå°†å¤„ç†æ‰€æœ‰æ–‡æ¡£")
            return documents, 0
    
    def preload_wikipedia_concepts(
        self,
        concept_keywords: List[str],
        lang: str = "zh",
        show_progress: bool = True
    ) -> int:
        """é¢„åŠ è½½æ ¸å¿ƒæ¦‚å¿µçš„ç»´åŸºç™¾ç§‘å†…å®¹åˆ°ç´¢å¼•
        
        Args:
            concept_keywords: æ¦‚å¿µå…³é”®è¯åˆ—è¡¨ï¼ˆç»´åŸºç™¾ç§‘é¡µé¢æ ‡é¢˜ï¼‰
            lang: è¯­è¨€ä»£ç ï¼ˆzh=ä¸­æ–‡, en=è‹±æ–‡ï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            æˆåŠŸç´¢å¼•çš„é¡µé¢æ•°é‡
            
        Examples:
            >>> index_manager.preload_wikipedia_concepts(
            ...     ["ç³»ç»Ÿç§‘å­¦", "é’±å­¦æ£®", "æ§åˆ¶è®º"],
            ...     lang="zh"
            ... )
        """
        if not concept_keywords:
            print("âš ï¸  æ¦‚å¿µå…³é”®è¯åˆ—è¡¨ä¸ºç©º")
            return 0
        
        try:
            from src.data_loader import load_documents_from_wikipedia
            
            if show_progress:
                print(f"ğŸ“– é¢„åŠ è½½ {len(concept_keywords)} ä¸ªç»´åŸºç™¾ç§‘æ¦‚å¿µ...")
            
            logger.info(f"å¼€å§‹é¢„åŠ è½½ç»´åŸºç™¾ç§‘æ¦‚å¿µ: {concept_keywords}")
            
            # åŠ è½½ç»´åŸºç™¾ç§‘é¡µé¢
            wiki_docs = load_documents_from_wikipedia(
                pages=concept_keywords,
                lang=lang,
                auto_suggest=True,
                clean=True,
                show_progress=show_progress
            )
            
            if not wiki_docs:
                if show_progress:
                    print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•ç»´åŸºç™¾ç§‘å†…å®¹")
                logger.warning("æœªæ‰¾åˆ°ä»»ä½•ç»´åŸºç™¾ç§‘å†…å®¹")
                return 0
            
            # æ„å»ºç´¢å¼•
            self.build_index(wiki_docs, show_progress=show_progress)
            
            if show_progress:
                print(f"âœ… å·²ç´¢å¼• {len(wiki_docs)} ä¸ªç»´åŸºç™¾ç§‘é¡µé¢")
            
            logger.info(f"æˆåŠŸé¢„åŠ è½½ {len(wiki_docs)} ä¸ªç»´åŸºç™¾ç§‘é¡µé¢")
            
            return len(wiki_docs)
            
        except Exception as e:
            print(f"âŒ é¢„åŠ è½½ç»´åŸºç™¾ç§‘å¤±è´¥: {e}")
            logger.error(f"é¢„åŠ è½½ç»´åŸºç™¾ç§‘å¤±è´¥: {e}")
            return 0
    
    def close(self):
        """å…³é—­ç´¢å¼•ç®¡ç†å™¨ï¼Œé‡Šæ”¾èµ„æº
        
        æ˜¾å¼å…³é—­ Chroma å®¢æˆ·ç«¯è¿æ¥ï¼Œåœæ­¢åå°çº¿ç¨‹
        åº”è¯¥åœ¨åº”ç”¨å…³é—­æ—¶è°ƒç”¨æ­¤æ–¹æ³•
        """
        try:
            logger.info("ğŸ”§ å¼€å§‹å…³é—­ç´¢å¼•ç®¡ç†å™¨...")
            
            # 1. æ¸…ç† Chroma å®¢æˆ·ç«¯
            if hasattr(self, 'chroma_client') and self.chroma_client is not None:
                try:
                    # å°è¯•å¤šç§æ–¹å¼å…³é—­å®¢æˆ·ç«¯
                    client = self.chroma_client
                    
                    # æ–¹æ³•1: å°è¯•è°ƒç”¨ close() æ–¹æ³•
                    if hasattr(client, 'close'):
                        client.close()
                        logger.info("âœ… Chromaå®¢æˆ·ç«¯å·²é€šè¿‡ close() æ–¹æ³•å…³é—­")
                    # æ–¹æ³•2: å°è¯•è°ƒç”¨ reset() æ–¹æ³•ï¼ˆæŸäº›ç‰ˆæœ¬æ”¯æŒï¼‰
                    elif hasattr(client, 'reset'):
                        client.reset()
                        logger.info("âœ… Chromaå®¢æˆ·ç«¯å·²é€šè¿‡ reset() æ–¹æ³•é‡ç½®")
                    # æ–¹æ³•3: å°è¯•è®¿é—®å†…éƒ¨å±æ€§å¹¶å…³é—­
                    elif hasattr(client, '_client'):
                        # æŸäº›ç‰ˆæœ¬çš„ Chroma å¯èƒ½æœ‰å†…éƒ¨å®¢æˆ·ç«¯
                        inner_client = getattr(client, '_client', None)
                        if inner_client and hasattr(inner_client, 'close'):
                            inner_client.close()
                            logger.info("âœ… Chromaå†…éƒ¨å®¢æˆ·ç«¯å·²å…³é—­")
                    
                    # æ¸…ç†å¼•ç”¨
                    self.chroma_client = None
                    logger.info("âœ… Chromaå®¢æˆ·ç«¯å¼•ç”¨å·²æ¸…ç†")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸  å…³é—­ Chroma å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
                    # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¦æ¸…ç†å¼•ç”¨
                    self.chroma_client = None
            
            # 2. æ¸…ç†å…¶ä»–å¼•ç”¨
            if hasattr(self, 'chroma_collection'):
                self.chroma_collection = None
            if hasattr(self, 'vector_store'):
                self.vector_store = None
            if hasattr(self, 'storage_context'):
                self.storage_context = None
            if hasattr(self, '_index'):
                self._index = None
            
            # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆå¯é€‰ï¼Œå¸®åŠ©æ¸…ç†çº¿ç¨‹ï¼‰
            try:
                import gc
                gc.collect()
                logger.debug("âœ… å·²æ‰§è¡Œåƒåœ¾å›æ”¶")
            except Exception as e:
                logger.debug(f"åƒåœ¾å›æ”¶æ—¶å‡ºé”™: {e}")
            
            logger.info("âœ… ç´¢å¼•ç®¡ç†å™¨èµ„æºå·²é‡Šæ”¾")
            
        except Exception as e:
            logger.warning(f"âš ï¸  å…³é—­ç´¢å¼•ç®¡ç†å™¨æ—¶å‡ºé”™: {e}")
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¦å°½å¯èƒ½æ¸…ç†å¼•ç”¨
            try:
                self.chroma_client = None
                self.chroma_collection = None
                self.vector_store = None
                self.storage_context = None
                self._index = None
            except:
                pass
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«é‡Šæ”¾"""
        try:
            self.close()
        except Exception:
            # ææ„å‡½æ•°ä¸­çš„å¼‚å¸¸åº”è¯¥è¢«å¿½ç•¥
            pass


def create_index_from_directory(
    directory_path: str | Path,
    collection_name: Optional[str] = None,
    recursive: bool = True
) -> IndexManager:
    """ä»ç›®å½•åˆ›å»ºç´¢å¼•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        directory_path: æ–‡æ¡£ç›®å½•è·¯å¾„
        collection_name: é›†åˆåç§°
        recursive: æ˜¯å¦é€’å½’åŠ è½½
        
    Returns:
        IndexManagerå¯¹è±¡
    """
    from src.data_loader import load_documents_from_directory
    
    # åŠ è½½æ–‡æ¡£
    print(f"ğŸ“‚ ä»ç›®å½•åŠ è½½æ–‡æ¡£: {directory_path}")
    documents = load_documents_from_directory(directory_path, recursive=recursive)
    
    if not documents:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return IndexManager(collection_name=collection_name)
    
    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index_manager = IndexManager(collection_name=collection_name)
    
    # æ„å»ºç´¢å¼•
    index_manager.build_index(documents)
    
    return index_manager


def create_index_from_urls(
    urls: List[str],
    collection_name: Optional[str] = None
) -> IndexManager:
    """ä»URLåˆ—è¡¨åˆ›å»ºç´¢å¼•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        urls: URLåˆ—è¡¨
        collection_name: é›†åˆåç§°
        
    Returns:
        IndexManagerå¯¹è±¡
    """
    from src.data_loader import load_documents_from_urls
    
    # åŠ è½½æ–‡æ¡£
    print(f"ğŸŒ ä» {len(urls)} ä¸ªURLåŠ è½½æ–‡æ¡£")
    documents = load_documents_from_urls(urls)
    
    if not documents:
        print("âš ï¸  æœªæˆåŠŸåŠ è½½ä»»ä½•ç½‘é¡µ")
        return IndexManager(collection_name=collection_name)
    
    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index_manager = IndexManager(collection_name=collection_name)
    
    # æ„å»ºç´¢å¼•
    index_manager.build_index(documents)
    
    return index_manager


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=== æµ‹è¯•ç´¢å¼•æ„å»º ===\n")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        LlamaDocument(
            text="ç³»ç»Ÿç§‘å­¦æ˜¯ç ”ç©¶ç³»ç»Ÿçš„ä¸€èˆ¬è§„å¾‹å’Œæ–¹æ³•çš„ç§‘å­¦ã€‚",
            metadata={"title": "ç³»ç»Ÿç§‘å­¦ç®€ä»‹", "source": "test"}
        ),
        LlamaDocument(
            text="é’±å­¦æ£®æ˜¯ä¸­å›½ç³»ç»Ÿç§‘å­¦çš„åˆ›å»ºè€…ä¹‹ä¸€ï¼Œä»–æå‡ºäº†ç³»ç»Ÿå·¥ç¨‹çš„ç†è®ºä½“ç³»ã€‚",
            metadata={"title": "é’±å­¦æ£®ä¸ç³»ç»Ÿç§‘å­¦", "source": "test"}
        ),
    ]
    
    # åˆ›å»ºç´¢å¼•
    index_manager = IndexManager(collection_name="test_collection")
    index_manager.build_index(test_docs)
    
    # æµ‹è¯•æœç´¢
    print("\n=== æµ‹è¯•æœç´¢ ===")
    results = index_manager.search("é’±å­¦æ£®", top_k=2)
    for i, result in enumerate(results, 1):
        print(f"\nç»“æœ {i}:")
        print(f"ç›¸ä¼¼åº¦: {result['score']:.4f}")
        print(f"å†…å®¹: {result['text']}")
    
    # æ¸…ç†æµ‹è¯•æ•°æ®
    index_manager.clear_index()
    print("\nâœ… æµ‹è¯•å®Œæˆ")

