"""
ç´¢å¼•æ„å»ºæ¨¡å—
è´Ÿè´£æ„å»ºå’Œç®¡ç†å‘é‡ç´¢å¼•ï¼Œé›†æˆChromaå‘é‡æ•°æ®åº“
"""

import os
import time
from pathlib import Path
from typing import List, Optional, Tuple, Dict

import chromadb
from tqdm import tqdm
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    Settings,
)
from llama_index.core.schema import Document as LlamaDocument
from llama_index.core.node_parser import SentenceSplitter
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore

from src.config import config, get_gpu_device, is_gpu_available, get_device_status
from src.logger import setup_logger

logger = setup_logger('indexer')

# å…¨å±€ embedding æ¨¡å‹ç¼“å­˜
_global_embed_model: Optional[HuggingFaceEmbedding] = None


def _setup_huggingface_env():
    """é…ç½® HuggingFace ç¯å¢ƒå˜é‡ï¼ˆé•œåƒå’Œç¦»çº¿æ¨¡å¼ï¼‰
    
    æ³¨æ„ï¼šç¯å¢ƒå˜é‡å·²åœ¨ src/__init__.py ä¸­é¢„è®¾ï¼Œè¿™é‡Œä»…ç”¨äºæ—¥å¿—è®°å½•å’Œç¡®è®¤
    """
    # è®¾ç½®é•œåƒåœ°å€
    if config.HF_ENDPOINT:
        os.environ['HF_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HUGGINGFACE_HUB_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HF_HUB_ENDPOINT'] = config.HF_ENDPOINT  # æ–°ç‰ˆæœ¬ä½¿ç”¨è¿™ä¸ª
        logger.info(f"ğŸŒ ä½¿ç”¨ HuggingFace é•œåƒ: {config.HF_ENDPOINT}")
    
    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    if config.HF_OFFLINE_MODE:
        os.environ['HF_HUB_OFFLINE'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '1'
        logger.info(f"ğŸ“´ å¯ç”¨ç¦»çº¿æ¨¡å¼ï¼ˆä»…ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")
    else:
        # ç¡®ä¿ç¦»çº¿æ¨¡å¼å…³é—­
        os.environ.pop('HF_HUB_OFFLINE', None)
        os.environ.pop('TRANSFORMERS_OFFLINE', None)


def load_embedding_model(model_name: Optional[str] = None, force_reload: bool = False) -> HuggingFaceEmbedding:
    """åŠ è½½ Embedding æ¨¡å‹ï¼ˆæ”¯æŒå…¨å±€å•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼ˆå³ä½¿å·²ç¼“å­˜ï¼‰
        
    Returns:
        HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    
    model_name = model_name or config.EMBEDDING_MODEL
    
    # å¦‚æœå·²ç»åŠ è½½è¿‡ä¸”æ¨¡å‹åç§°ç›¸åŒï¼Œç›´æ¥è¿”å›ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°åŠ è½½ï¼‰
    if _global_embed_model is not None and not force_reload:
        # æ£€æŸ¥ç¼“å­˜çš„æ¨¡å‹åç§°æ˜¯å¦ä¸æ–°é…ç½®ä¸€è‡´
        cached_model_name = getattr(_global_embed_model, 'model_name', None)
        if cached_model_name == model_name:
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„ Embedding æ¨¡å‹ï¼ˆå…¨å±€å˜é‡ï¼‰: {model_name}")
            logger.info(f"   æ¨¡å‹å¯¹è±¡ID: {id(_global_embed_model)}")
            return _global_embed_model
        else:
            # æ¨¡å‹åç§°ä¸ä¸€è‡´ï¼Œæ¸…é™¤ç¼“å­˜å¹¶é‡æ–°åŠ è½½
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹é…ç½®å˜æ›´: {cached_model_name} -> {model_name}")
            logger.info(f"   æ¸…é™¤æ—§æ¨¡å‹ç¼“å­˜ï¼Œé‡æ–°åŠ è½½æ–°æ¨¡å‹")
            _global_embed_model = None
    
    # å¦‚æœéœ€è¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œæ¸…é™¤ç¼“å­˜
    if force_reload:
        logger.info(f"ğŸ”„ å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å‹")
        _global_embed_model = None
    
    # é…ç½® HuggingFace ç¯å¢ƒå˜é‡
    _setup_huggingface_env()
    
    # åŠ è½½æ¨¡å‹
    logger.info(f"ğŸ“¦ å¼€å§‹åŠ è½½ Embedding æ¨¡å‹ï¼ˆå…¨æ–°åŠ è½½ï¼‰: {model_name}")
    
    try:
        # æ˜¾å¼æŒ‡å®šç¼“å­˜ç›®å½•ä»¥ç¡®ä¿ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        cache_folder = str(Path.home() / ".cache" / "huggingface")
        
        # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
        device = get_gpu_device()
        import torch
        
        # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
        if device.startswith("cuda") and is_gpu_available():
            device_name = torch.cuda.get_device_name()
            cuda_version = torch.version.cuda
            logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ:")
            logger.info(f"   è®¾å¤‡: {device}")
            logger.info(f"   GPUåç§°: {device_name}")
            logger.info(f"   CUDAç‰ˆæœ¬: {cuda_version}")
        else:
            logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
            logger.info("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
            logger.info("ğŸ’¡ å»ºè®®: å¦‚æœ‰GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        
        # æ„å»ºæ¨¡å‹å‚æ•°
        model_kwargs = {
            "trust_remote_code": True,
            "cache_folder": cache_folder,
        }
        
        _global_embed_model = HuggingFaceEmbedding(
            model_name=model_name,
            embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†ï¼Œæå‡æ€§èƒ½
            max_length=config.EMBED_MAX_LENGTH,  # è®¾ç½®æœ€å¤§é•¿åº¦
            **model_kwargs
        )
        
        # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPUï¼ˆå¦‚æœä¸æ”¯æŒé€šè¿‡å‚æ•°æŒ‡å®šï¼‰
        try:
            if device.startswith("cuda") and is_gpu_available():
                # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                if hasattr(_global_embed_model, '_model') and hasattr(_global_embed_model._model, 'to'):
                    _global_embed_model._model = _global_embed_model._model.to(device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                elif hasattr(_global_embed_model, 'model') and hasattr(_global_embed_model.model, 'to'):
                    _global_embed_model.model = _global_embed_model.model.to(device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
            else:
                logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
            logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
        
        logger.info(f"âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_folder}")
        if device.startswith("cuda"):
            logger.info(f"âš¡ GPUåŠ é€Ÿæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (æ¨è10-50)")
        else:
            logger.info(f"ğŸŒ CPUæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (å»ºè®®è°ƒæ•´ä¸º5-10)")
        logger.info(f"ğŸ“ æœ€å¤§é•¿åº¦: {config.EMBED_MAX_LENGTH}")
    except Exception as e:
        # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ä¸”ç¼ºå°‘ç¼“å­˜ï¼Œå°è¯•åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
        if config.HF_OFFLINE_MODE and "offline" in str(e).lower():
            logger.warning(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æœ¬åœ°æ— ç¼“å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼å°è¯•ä¸‹è½½")
            os.environ.pop('HF_HUB_OFFLINE', None)
            
            try:
                cache_folder = str(Path.home() / ".cache" / "huggingface")
                
                # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                device = get_gpu_device()
                import torch
                
                # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                if device.startswith("cuda") and is_gpu_available():
                    device_name = torch.cuda.get_device_name()
                    logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ: {device_name} ({device})")
                else:
                    logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                    logger.info("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                
                # æ„å»ºæ¨¡å‹å‚æ•°
                model_kwargs = {
                    "trust_remote_code": True,
                    "cache_folder": cache_folder,
                }
                
                _global_embed_model = HuggingFaceEmbedding(
                    model_name=model_name,
                    embed_batch_size=config.EMBED_BATCH_SIZE,
                    max_length=config.EMBED_MAX_LENGTH,
                    **model_kwargs
                )
                
                # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPU
                try:
                    if device.startswith("cuda") and is_gpu_available():
                        # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                        if hasattr(_global_embed_model, '_model') and hasattr(_global_embed_model._model, 'to'):
                            _global_embed_model._model = _global_embed_model._model.to(device)
                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                        elif hasattr(_global_embed_model, 'model') and hasattr(_global_embed_model.model, 'to'):
                            _global_embed_model.model = _global_embed_model.model.to(device)
                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                    else:
                        logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                except Exception as e:
                    logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                    logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                
                logger.info(f"âœ… Embedding æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ: {model_name}")
                if device.startswith("cuda"):
                    logger.info(f"âš¡ GPUåŠ é€Ÿæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (æ¨è10-50)")
                else:
                    logger.info(f"ğŸŒ CPUæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (å»ºè®®è°ƒæ•´ä¸º5-10)")
                logger.info(f"ğŸ“ æœ€å¤§é•¿åº¦: {config.EMBED_MAX_LENGTH}")
            except Exception as retry_error:
                logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {retry_error}")
                raise
        else:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    return _global_embed_model


def set_global_embed_model(model: HuggingFaceEmbedding):
    """è®¾ç½®å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Args:
        model: HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    _global_embed_model = model
    logger.debug("ğŸ”§ è®¾ç½®å…¨å±€ Embedding æ¨¡å‹")


def get_global_embed_model() -> Optional[HuggingFaceEmbedding]:
    """è·å–å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Returns:
        å·²åŠ è½½çš„æ¨¡å‹å®ä¾‹ï¼Œå¦‚æœæœªåŠ è½½åˆ™è¿”å› None
    """
    return _global_embed_model


def clear_embedding_model_cache():
    """æ¸…é™¤å…¨å±€ Embedding æ¨¡å‹ç¼“å­˜
    
    ç”¨äºæ¨¡å‹åˆ‡æ¢æˆ–å¼ºåˆ¶é‡æ–°åŠ è½½åœºæ™¯
    """
    global _global_embed_model
    if _global_embed_model is not None:
        logger.info(f"ğŸ§¹ æ¸…é™¤ Embedding æ¨¡å‹ç¼“å­˜")
        _global_embed_model = None


def get_embedding_model_status() -> dict:
    """è·å– Embedding æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    
    Returns:
        åŒ…å«æ¨¡å‹çŠ¶æ€çš„å­—å…¸ï¼š
        {
            "loaded": bool,              # æ˜¯å¦å·²åŠ è½½
            "model_name": str,           # æ¨¡å‹åç§°
            "cache_dir": str,            # ç¼“å­˜ç›®å½•
            "cache_exists": bool,        # æœ¬åœ°ç¼“å­˜æ˜¯å¦å­˜åœ¨
            "offline_mode": bool,        # æ˜¯å¦ç¦»çº¿æ¨¡å¼
            "mirror": str,               # é•œåƒåœ°å€
        }
    """
    import os
    from pathlib import Path
    
    model_name = config.EMBEDDING_MODEL
    
    # æ£€æŸ¥ç¼“å­˜ç›®å½•
    cache_root = Path.home() / ".cache" / "huggingface" / "hub"
    # HuggingFace ç¼“å­˜æ ¼å¼: models--{org}--{model}
    model_cache_name = model_name.replace("/", "--")
    cache_dir = cache_root / f"models--{model_cache_name}"
    cache_exists = cache_dir.exists()
    
    return {
        "loaded": _global_embed_model is not None,
        "model_name": model_name,
        "cache_dir": str(cache_dir),
        "cache_exists": cache_exists,
        "offline_mode": config.HF_OFFLINE_MODE,
        "mirror": config.HF_ENDPOINT if config.HF_ENDPOINT else "huggingface.co (å®˜æ–¹)",
    }


class IndexManager:
    """ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(
        self,
        collection_name: Optional[str] = None,
        persist_dir: Optional[Path] = None,
        embedding_model: Optional[str] = None,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        embed_model_instance: Optional[HuggingFaceEmbedding] = None,
    ):
        """åˆå§‹åŒ–ç´¢å¼•ç®¡ç†å™¨
        
        Args:
            collection_name: Chromaé›†åˆåç§°
            persist_dir: å‘é‡å­˜å‚¨æŒä¹…åŒ–ç›®å½•
            embedding_model: Embeddingæ¨¡å‹åç§°
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å—é‡å å¤§å°
            embed_model_instance: é¢„åŠ è½½çš„Embeddingæ¨¡å‹å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        # ä½¿ç”¨é…ç½®æˆ–é»˜è®¤å€¼
        self.collection_name = collection_name or config.CHROMA_COLLECTION_NAME
        self.persist_dir = persist_dir or config.VECTOR_STORE_PATH
        self.embedding_model_name = embedding_model or config.EMBEDDING_MODEL
        self.chunk_size = chunk_size or config.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or config.CHUNK_OVERLAP
        
        # ç¡®ä¿æŒä¹…åŒ–ç›®å½•å­˜åœ¨
        self.persist_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–embeddingæ¨¡å‹ï¼ˆä¼˜å…ˆä½¿ç”¨é¢„åŠ è½½çš„å®ä¾‹ï¼‰
        if embed_model_instance is not None:
            print(f"âœ… ä½¿ç”¨é¢„åŠ è½½çš„Embeddingæ¨¡å‹: {self.embedding_model_name}")
            self.embed_model = embed_model_instance
        else:
            # æ£€æŸ¥å…¨å±€ç¼“å­˜ä¸­çš„æ¨¡å‹æ˜¯å¦åŒ¹é…å½“å‰é…ç½®
            global _global_embed_model
            cached_model_name = None
            if _global_embed_model is not None:
                cached_model_name = getattr(_global_embed_model, 'model_name', None)
            
            # å¦‚æœæ¨¡å‹åç§°ä¸åŒ¹é…ï¼Œæ¸…ç†ç¼“å­˜
            if cached_model_name and cached_model_name != self.embedding_model_name:
                logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹é…ç½®å˜æ›´: {cached_model_name} -> {self.embedding_model_name}")
                logger.info(f"   æ¸…ç†æ—§æ¨¡å‹ç¼“å­˜å¹¶é‡æ–°åŠ è½½")
                clear_embedding_model_cache()
            
            # å°è¯•ä½¿ç”¨å…¨å±€ç¼“å­˜çš„æ¨¡å‹ï¼ˆå¦‚æœåŒ¹é…ï¼‰
            if _global_embed_model is not None:
                try:
                    # éªŒè¯ç¼“å­˜çš„æ¨¡å‹æ˜¯å¦çœŸçš„åŒ¹é…ï¼ˆé€šè¿‡å®é™…è®¡ç®—ç»´åº¦ï¼‰
                    test_embedding = _global_embed_model.get_query_embedding("test")
                    cached_dim = len(test_embedding)
                    logger.info(f"âœ… ä½¿ç”¨å…¨å±€ç¼“å­˜çš„Embeddingæ¨¡å‹: {self.embedding_model_name} (ç»´åº¦: {cached_dim})")
                    self.embed_model = _global_embed_model
                except Exception as e:
                    logger.warning(f"âš ï¸  éªŒè¯ç¼“å­˜æ¨¡å‹å¤±è´¥ï¼Œé‡æ–°åŠ è½½: {e}")
                    clear_embedding_model_cache()
                    # ç»§ç»­ä¸‹é¢çš„åŠ è½½æµç¨‹
                    self.embed_model = None
            else:
                self.embed_model = None
            
            # å¦‚æœç¼“å­˜ä¸å¯ç”¨ï¼ŒåŠ è½½æ–°æ¨¡å‹
            if self.embed_model is None:
                # é…ç½® HuggingFace ç¯å¢ƒå˜é‡
                _setup_huggingface_env()
                
                print(f"ğŸ“¦ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {self.embedding_model_name}")
                
                # ä½¿ç”¨load_embedding_modelå‡½æ•°ä»¥ç¡®ä¿ç¼“å­˜ç®¡ç†æ­£ç¡®
                try:
                    self.embed_model = load_embedding_model(
                        model_name=self.embedding_model_name,
                        force_reload=False  # å¦‚æœç¼“å­˜åŒ¹é…ï¼Œç›´æ¥ä½¿ç”¨
                    )
                    logger.info(f"âœ… é€šè¿‡load_embedding_modelåŠ è½½æ¨¡å‹: {self.embedding_model_name}")
                    # å¦‚æœæˆåŠŸåŠ è½½ï¼Œè·³è¿‡ç›´æ¥åŠ è½½çš„ä»£ç å—
                except Exception as e:
                    logger.warning(f"âš ï¸  load_embedding_modelå¤±è´¥ï¼Œä½¿ç”¨ç›´æ¥åŠ è½½æ–¹å¼: {e}")
                    # å›é€€åˆ°ç›´æ¥åŠ è½½æ–¹å¼
                    try:
                        cache_folder = str(Path.home() / ".cache" / "huggingface")
                        
                        # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                        device = get_gpu_device()
                        import torch
                        
                        # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                        if device.startswith("cuda") and is_gpu_available():
                            device_name = torch.cuda.get_device_name()
                            cuda_version = torch.version.cuda
                            print(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ:")
                            print(f"   è®¾å¤‡: {device}")
                            print(f"   GPUåç§°: {device_name}")
                            print(f"   CUDAç‰ˆæœ¬: {cuda_version}")
                            logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPU: {device_name} ({device})")
                        else:
                            print("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                            print("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                            logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                            logger.info("ğŸ’¡ å»ºè®®: å¦‚æœ‰GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æœ€ä½³æ€§èƒ½")
                        
                        # æ„å»ºæ¨¡å‹å‚æ•°
                        model_kwargs = {
                            "trust_remote_code": True,
                            "cache_folder": cache_folder,
                        }
                        
                        self.embed_model = HuggingFaceEmbedding(
                            model_name=self.embedding_model_name,
                            embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†
                            max_length=config.EMBED_MAX_LENGTH,
                            **model_kwargs
                        )
                        
                        # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPUï¼ˆå¦‚æœä¸æ”¯æŒé€šè¿‡å‚æ•°æŒ‡å®šï¼‰
                        try:
                            if device.startswith("cuda") and is_gpu_available():
                                # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                                if hasattr(self.embed_model, '_model') and hasattr(self.embed_model._model, 'to'):
                                    self.embed_model._model = self.embed_model._model.to(device)
                                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                                elif hasattr(self.embed_model, 'model') and hasattr(self.embed_model.model, 'to'):
                                    self.embed_model.model = self.embed_model.model.to(device)
                                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                            else:
                                logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                        except Exception as e:
                            logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                            logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                        
                        if device.startswith("cuda"):
                            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (GPUåŠ é€Ÿ, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE})")
                        else:
                            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (CPUæ¨¡å¼, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE}, å»ºè®®è°ƒæ•´ä¸º5-10)")
                    except Exception as load_error:
                        # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ä¸”ç¼ºå°‘ç¼“å­˜ï¼Œå°è¯•åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
                        if config.HF_OFFLINE_MODE and "offline" in str(load_error).lower():
                            print(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æœ¬åœ°æ— ç¼“å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼å°è¯•ä¸‹è½½...")
                            os.environ.pop('HF_HUB_OFFLINE', None)
                            
                            try:
                                cache_folder = str(Path.home() / ".cache" / "huggingface")
                                
                                # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                                device = get_gpu_device()
                                import torch
                                
                                # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                                if device.startswith("cuda") and is_gpu_available():
                                    device_name = torch.cuda.get_device_name()
                                    print(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ: {device_name} ({device})")
                                    logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPU: {device_name} ({device})")
                                else:
                                    print("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                                    print("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                                    logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                                
                                # æ„å»ºæ¨¡å‹å‚æ•°
                                model_kwargs = {
                                    "trust_remote_code": True,
                                    "cache_folder": cache_folder,
                                }
                                
                                self.embed_model = HuggingFaceEmbedding(
                                    model_name=self.embedding_model_name,
                                    embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†
                                    max_length=config.EMBED_MAX_LENGTH,
                                    **model_kwargs
                                )
                                
                                # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPU
                                try:
                                    if device.startswith("cuda") and is_gpu_available():
                                        if hasattr(self.embed_model, '_model') and hasattr(self.embed_model._model, 'to'):
                                            self.embed_model._model = self.embed_model._model.to(device)
                                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                                        elif hasattr(self.embed_model, 'model') and hasattr(self.embed_model.model, 'to'):
                                            self.embed_model.model = self.embed_model.model.to(device)
                                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                                    else:
                                        logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                                except Exception as e:
                                    logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                                    logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                                
                                if device.startswith("cuda"):
                                    print(f"âœ… æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ (GPUåŠ é€Ÿ, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE})")
                                else:
                                    print(f"âœ… æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ (CPUæ¨¡å¼, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE}, å»ºè®®è°ƒæ•´ä¸º5-10)")
                            except Exception as retry_error:
                                print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {retry_error}")
                                raise
                        else:
                            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {load_error}")
                            raise
        
        # é…ç½®å…¨å±€Settings
        Settings.embed_model = self.embed_model
        Settings.chunk_size = self.chunk_size
        Settings.chunk_overlap = self.chunk_overlap
        
        # åˆå§‹åŒ–Chromaå®¢æˆ·ç«¯
        print(f"ğŸ—„ï¸  åˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“: {self.persist_dir}")
        self.chroma_client = chromadb.PersistentClient(path=str(self.persist_dir))
        
        # æ‰“å°æ•°æ®åº“ä¿¡æ¯
        self._print_database_info()
        
        # æ£€æµ‹å¹¶ä¿®å¤embeddingç»´åº¦ä¸åŒ¹é…é—®é¢˜
        self._ensure_collection_dimension_match()
        
        # åˆ›å»ºå‘é‡å­˜å‚¨
        self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
        
        # åˆ›å»ºå­˜å‚¨ä¸Šä¸‹æ–‡
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )
        
        # ç´¢å¼•å¯¹è±¡ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._index: Optional[VectorStoreIndex] = None
        
        print("âœ… ç´¢å¼•ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def build_index(
        self,
        documents: List[LlamaDocument],
        show_progress: bool = True
    ) -> VectorStoreIndex:
        """æ„å»ºæˆ–æ›´æ–°ç´¢å¼•ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦
            
        Returns:
            VectorStoreIndexå¯¹è±¡
        """
        start_time = time.time()
        
        if not documents:
            print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ç´¢å¼•")
            return self.get_index()
        
        # è·å–å½“å‰è®¾å¤‡ä¿¡æ¯
        device = get_gpu_device()
        
        print(f"\nğŸ”¨ å¼€å§‹æ„å»ºç´¢å¼•ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
        print(f"   è®¾å¤‡: {device}")
        logger.info(f"å¼€å§‹ç´¢å¼•æ„å»º: {len(documents)}ä¸ªæ–‡æ¡£, è®¾å¤‡: {device}")
        
        try:
            # åˆ›å»ºæ–°ç´¢å¼•
            if self._index is None:
                self._index = VectorStoreIndex.from_documents(
                    documents,
                    storage_context=self.storage_context,
                    show_progress=show_progress,
                )
            else:
                # å¢é‡æ·»åŠ åˆ°ç°æœ‰ç´¢å¼•
                self._index.insert_ref_docs(documents, show_progress=show_progress)
            
            # ç»Ÿè®¡ä¿¡æ¯
            elapsed = time.time() - start_time
            stats = self.get_stats()
            
            print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆ (è€—æ—¶: {elapsed:.2f}s, æ–‡æ¡£æ•°: {stats['document_count']})")
            logger.info(f"ç´¢å¼•æ„å»ºå®Œæˆ: {len(documents)}ä¸ªæ–‡æ¡£, è€—æ—¶{elapsed:.2f}s")
            
            return self._index
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            logger.error(f"ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise
    
    def get_index(self) -> VectorStoreIndex:
        """è·å–ç°æœ‰ç´¢å¼•
        
        Returns:
            VectorStoreIndexå¯¹è±¡
        """
        if self._index is None:
            # å°è¯•ä»å·²æœ‰çš„å‘é‡å­˜å‚¨åŠ è½½
            try:
                self._index = VectorStoreIndex.from_vector_store(
                    vector_store=self.vector_store,
                    storage_context=self.storage_context,
                )
                print("âœ… ä»å‘é‡å­˜å‚¨åŠ è½½ç´¢å¼•æˆåŠŸ")
            except Exception as e:
                print(f"â„¹ï¸  æ²¡æœ‰æ‰¾åˆ°ç°æœ‰ç´¢å¼•ï¼Œå°†åœ¨æ·»åŠ æ–‡æ¡£ååˆ›å»º")
                # åˆ›å»ºä¸€ä¸ªç©ºç´¢å¼•
                self._index = VectorStoreIndex.from_documents(
                    [],
                    storage_context=self.storage_context,
                )
        
        return self._index
    
    def _print_database_info(self):
        """æ‰“å°æ•°æ®åº“å’Œcollectionçš„è¯¦ç»†ä¿¡æ¯"""
        try:
            # 1. åˆ—å‡ºæ‰€æœ‰collections
            try:
                all_collections = self.chroma_client.list_collections()
                print(f"\nğŸ“‹ æ•°æ®åº“ä¸­çš„Collectionsåˆ—è¡¨:")
                if all_collections:
                    for idx, coll in enumerate(all_collections, 1):
                        try:
                            coll_count = coll.count() if hasattr(coll, 'count') else 0
                            coll_name = coll.name if hasattr(coll, 'name') else str(coll)
                            print(f"   {idx}. {coll_name} - {coll_count} ä¸ªå‘é‡")
                            logger.info(f"Collection: {coll_name}, å‘é‡æ•°: {coll_count}")
                        except Exception as e:
                            coll_name = coll.name if hasattr(coll, 'name') else str(coll)
                            print(f"   {idx}. {coll_name} - æ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯: {e}")
                else:
                    print("   (æ— collections)")
                    logger.info("æ•°æ®åº“ä¸­æš‚æ— collections")
            except Exception as e:
                logger.warning(f"è·å–collectionsåˆ—è¡¨å¤±è´¥: {e}")
                print(f"   âš ï¸  æ— æ³•åˆ—å‡ºcollections: {e}")
            
            # 2. æ£€æŸ¥å½“å‰collectionæ˜¯å¦å­˜åœ¨
            print(f"\nğŸ” æ£€æŸ¥ç›®æ ‡Collection: {self.collection_name}")
            try:
                existing_collection = self.chroma_client.get_collection(name=self.collection_name)
                collection_count = existing_collection.count()
                
                print(f"   âœ… Collectionå­˜åœ¨")
                print(f"   ğŸ“Š å‘é‡æ€»æ•°: {collection_count}")
                logger.info(f"Collection '{self.collection_name}' å­˜åœ¨ï¼Œå‘é‡æ•°: {collection_count}")
                
                # 3. è·å–collectionçš„è¯¦ç»†ä¿¡æ¯
                sample_data = None  # åˆå§‹åŒ–å˜é‡
                if collection_count > 0:
                    # è·å–æ ·æœ¬æ•°æ®ï¼ˆæœ€å¤š10æ¡ï¼‰
                    sample_limit = min(10, collection_count)
                    try:
                        sample_data = existing_collection.peek(limit=sample_limit)
                        
                        # ç»Ÿè®¡metadataä¸­çš„ä¿¡æ¯
                        file_paths = set()
                        repositories = set()
                        file_types = {}
                        
                        if sample_data and 'metadatas' in sample_data:
                            for metadata in sample_data['metadatas']:
                                if metadata:
                                    # æ”¶é›†æ–‡ä»¶è·¯å¾„
                                    if 'file_path' in metadata:
                                        file_paths.add(metadata['file_path'])
                                    
                                    # æ”¶é›†ä»“åº“ä¿¡æ¯
                                    if 'repository' in metadata:
                                        repositories.add(metadata['repository'])
                                    
                                    # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
                                    if 'file_name' in metadata:
                                        file_name = metadata['file_name']
                                        file_ext = Path(file_name).suffix.lower() if file_name else ''
                                        file_types[file_ext] = file_types.get(file_ext, 0) + 1
                        
                        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                        print(f"\n   ğŸ“ˆ Collectionç»Ÿè®¡ä¿¡æ¯:")
                        print(f"      â€¢ å‘é‡æ•°é‡: {collection_count}")
                        
                        if file_paths:
                            print(f"      â€¢ å”¯ä¸€æ–‡ä»¶è·¯å¾„æ•°: {len(file_paths)}")
                            if len(file_paths) <= 20:
                                print(f"      â€¢ æ–‡ä»¶è·¯å¾„åˆ—è¡¨:")
                                for fp in sorted(list(file_paths))[:20]:
                                    print(f"        - {fp}")
                            else:
                                print(f"      â€¢ æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆå‰20ä¸ªï¼‰:")
                                for fp in sorted(list(file_paths))[:20]:
                                    print(f"        - {fp}")
                                print(f"        ... è¿˜æœ‰ {len(file_paths) - 20} ä¸ªæ–‡ä»¶")
                        
                        if repositories:
                            print(f"      â€¢ ä»“åº“åˆ—è¡¨:")
                            for repo in sorted(list(repositories)):
                                print(f"        - {repo}")
                        
                        if file_types:
                            print(f"      â€¢ æ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
                            for ext, count in sorted(file_types.items(), key=lambda x: x[1], reverse=True):
                                ext_display = ext if ext else "(æ— æ‰©å±•å)"
                                print(f"        {ext_display}: {count} ä¸ª")
                        
                        # æ‰“å°æ ·æœ¬metadataï¼ˆå‰5æ¡ï¼‰
                        if sample_data and 'metadatas' in sample_data and sample_data['metadatas']:
                            print(f"\n   ğŸ“„ æ ·æœ¬æ•°æ®ï¼ˆå‰5æ¡ï¼‰:")
                            for idx, metadata in enumerate(sample_data['metadatas'][:5], 1):
                                if metadata:
                                    print(f"      {idx}. Metadata:")
                                    for key, value in metadata.items():
                                        # æˆªæ–­è¿‡é•¿çš„å€¼
                                        value_str = str(value)
                                        if len(value_str) > 100:
                                            value_str = value_str[:100] + "..."
                                        print(f"         {key}: {value_str}")
                                    
                                    # å¦‚æœæœ‰å¯¹åº”çš„æ–‡æ¡£ID
                                    if 'ids' in sample_data and idx <= len(sample_data['ids']):
                                        doc_id = sample_data['ids'][idx - 1]
                                        print(f"         id: {doc_id}")
                        
                        logger.info(
                            f"Collectionè¯¦æƒ…: å‘é‡æ•°={collection_count}, "
                            f"æ–‡ä»¶æ•°={len(file_paths)}, "
                            f"ä»“åº“æ•°={len(repositories)}, "
                            f"æ–‡ä»¶ç±»å‹={len(file_types)}"
                        )
                        
                    except Exception as e:
                        logger.warning(f"è·å–collectionæ ·æœ¬æ•°æ®å¤±è´¥: {e}")
                        print(f"   âš ï¸  æ— æ³•è·å–æ ·æœ¬æ•°æ®: {e}")
                    
                    # è·å–ç»´åº¦ä¿¡æ¯
                    try:
                        if existing_collection.metadata and 'embedding_dimension' in existing_collection.metadata:
                            dim = existing_collection.metadata['embedding_dimension']
                            print(f"   ğŸ“ Embeddingç»´åº¦: {dim}")
                            logger.info(f"Collectionç»´åº¦: {dim}")
                        elif sample_data and 'embeddings' in sample_data and sample_data['embeddings']:
                            dim = len(sample_data['embeddings'][0])
                            print(f"   ğŸ“ Embeddingç»´åº¦: {dim} (ä»æ ·æœ¬æ•°æ®æ£€æµ‹)")
                            logger.info(f"Collectionç»´åº¦: {dim} (ä»æ ·æœ¬æ•°æ®æ£€æµ‹)")
                    except Exception as e:
                        logger.debug(f"è·å–ç»´åº¦ä¿¡æ¯å¤±è´¥: {e}")
                else:
                    print(f"   â„¹ï¸  Collectionä¸ºç©º")
                    logger.info(f"Collection '{self.collection_name}' ä¸ºç©º")
                
            except Exception as e:
                # Collectionä¸å­˜åœ¨
                if "does not exist" in str(e) or "not found" in str(e).lower():
                    print(f"   â„¹ï¸  Collectionä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°collection")
                    logger.info(f"Collection '{self.collection_name}' ä¸å­˜åœ¨ï¼Œå°†åˆ›å»º")
                else:
                    print(f"   âš ï¸  æ£€æŸ¥collectionæ—¶å‡ºé”™: {e}")
                    logger.warning(f"æ£€æŸ¥collectionå¤±è´¥: {e}")
            
            print()  # ç©ºè¡Œåˆ†éš”
            
        except Exception as e:
            logger.error(f"æ‰“å°æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
            print(f"âš ï¸  æ‰“å°æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}")
    
    def _ensure_collection_dimension_match(self):
        """ç¡®ä¿collectionçš„embeddingç»´åº¦ä¸å½“å‰æ¨¡å‹åŒ¹é…
        
        å¦‚æœcollectionå·²å­˜åœ¨ä½†ç»´åº¦ä¸åŒ¹é…ï¼Œä¼šè‡ªåŠ¨åˆ é™¤å¹¶é‡æ–°åˆ›å»º
        """
        try:
            # é¦–å…ˆç¡®ä¿èƒ½è·å–å½“å‰embeddingæ¨¡å‹çš„ç»´åº¦ï¼ˆå¿…é¡»æˆåŠŸï¼‰
            model_dim = None
            dim_detection_methods = []
            
            # æ–¹æ³•1: å°è¯•ä»æ¨¡å‹å±æ€§è·å–
            if hasattr(self.embed_model, 'embed_dim'):
                model_dim = self.embed_model.embed_dim
                dim_detection_methods.append("embed_dimå±æ€§")
            elif hasattr(self.embed_model, '_model') and hasattr(self.embed_model._model, 'config'):
                # å°è¯•ä»transformersæ¨¡å‹configè·å–
                try:
                    model_dim = getattr(self.embed_model._model.config, 'hidden_size', None)
                    if model_dim:
                        dim_detection_methods.append("æ¨¡å‹config.hidden_size")
                except Exception as e:
                    logger.debug(f"ä»æ¨¡å‹configè·å–ç»´åº¦å¤±è´¥: {e}")
            
            # æ–¹æ³•2: é€šè¿‡å®é™…è®¡ç®—ä¸€ä¸ªæµ‹è¯•å‘é‡è·å–ç»´åº¦ï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
            if model_dim is None:
                try:
                    test_embedding = self.embed_model.get_query_embedding("test")
                    model_dim = len(test_embedding)
                    dim_detection_methods.append("å®é™…è®¡ç®—æµ‹è¯•å‘é‡")
                except Exception as e:
                    logger.warning(f"é€šè¿‡æµ‹è¯•å‘é‡è·å–ç»´åº¦å¤±è´¥: {e}")
            
            # å¦‚æœä»ç„¶æ— æ³•è·å–æ¨¡å‹ç»´åº¦ï¼Œè¿™æ˜¯ä¸¥é‡é”™è¯¯
            if model_dim is None:
                error_msg = "æ— æ³•æ£€æµ‹embeddingæ¨¡å‹ç»´åº¦ï¼Œè¿™å¯èƒ½å¯¼è‡´ç»´åº¦ä¸åŒ¹é…é”™è¯¯"
                logger.error(error_msg)
                print(f"âŒ {error_msg}")
                print(f"   å°è¯•çš„æ–¹æ³•: {dim_detection_methods}")
                raise ValueError(error_msg)
            
            logger.info(f"âœ… æˆåŠŸæ£€æµ‹åˆ°embeddingæ¨¡å‹ç»´åº¦: {model_dim} (æ–¹æ³•: {', '.join(dim_detection_methods)})")
            print(f"ğŸ“ å½“å‰embeddingæ¨¡å‹ç»´åº¦: {model_dim}")
            
            # å°è¯•è·å–ç°æœ‰collection
            try:
                existing_collection = self.chroma_client.get_collection(name=self.collection_name)
                
                # è·å–collectionçš„ç»´åº¦ï¼ˆä»metadataæˆ–æŸ¥è¯¢å®é™…æ•°æ®ï¼‰
                collection_dim = None
                collection_count = existing_collection.count()
                
                try:
                    # å°è¯•ä»collectionçš„metadataè·å–
                    if existing_collection.metadata and 'embedding_dimension' in existing_collection.metadata:
                        collection_dim = existing_collection.metadata['embedding_dimension']
                        logger.info(f"ä»collection metadataè·å–ç»´åº¦: {collection_dim}")
                    elif collection_count > 0:
                        # å¦‚æœcollectionæœ‰æ•°æ®ï¼Œå°è¯•æŸ¥è¯¢ä¸€ä¸ªå‘é‡è·å–ç»´åº¦
                        sample = existing_collection.peek(limit=1)
                        if sample and 'embeddings' in sample and sample['embeddings']:
                            collection_dim = len(sample['embeddings'][0])
                            logger.info(f"ä»collectionå®é™…æ•°æ®è·å–ç»´åº¦: {collection_dim}")
                except Exception as e:
                    logger.warning(f"è·å–collectionç»´åº¦å¤±è´¥: {e}")
                
                # å¦‚æœcollectionä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨ï¼ˆæ— éœ€æ£€æŸ¥ç»´åº¦ï¼‰
                if collection_count == 0:
                    self.chroma_collection = existing_collection
                    print(f"âœ… Collectionä¸ºç©ºï¼Œå¯ä»¥ä½¿ç”¨: {self.collection_name}")
                    logger.info(f"Collectionä¸ºç©ºï¼Œç›´æ¥ä½¿ç”¨: {self.collection_name}")
                # å¦‚æœcollectionæœ‰æ•°æ®ä½†æ— æ³•è·å–ç»´åº¦ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥ï¼šåˆ é™¤å¹¶é‡å»º
                elif collection_dim is None:
                    print(f"âš ï¸  Collectionæœ‰æ•°æ®ä½†æ— æ³•æ£€æµ‹ç»´åº¦ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥åˆ é™¤å¹¶é‡å»º")
                    print(f"   å½“å‰æ¨¡å‹ç»´åº¦: {model_dim}")
                    print(f"ğŸ”„ è‡ªåŠ¨åˆ é™¤æ—§collectionå¹¶é‡æ–°åˆ›å»º...")
                    
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.warning(f"å› æ— æ³•æ£€æµ‹ç»´åº¦ï¼Œå·²åˆ é™¤collection: {self.collection_name} (æ¨¡å‹ç»´åº¦: {model_dim})")
                    
                    # é‡æ–°åˆ›å»ºcollection
                    self.chroma_collection = self.chroma_client.get_or_create_collection(
                        name=self.collection_name
                    )
                    print(f"âœ… å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name}")
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œåˆ é™¤å¹¶é‡å»º
                elif model_dim != collection_dim:
                    print(f"âš ï¸  æ£€æµ‹åˆ°embeddingç»´åº¦ä¸åŒ¹é…:")
                    print(f"   Collectionç»´åº¦: {collection_dim}")
                    print(f"   å½“å‰æ¨¡å‹ç»´åº¦: {model_dim}")
                    print(f"ğŸ”„ è‡ªåŠ¨åˆ é™¤æ—§collectionå¹¶é‡æ–°åˆ›å»º...")
                    
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.info(f"å·²åˆ é™¤ç»´åº¦ä¸åŒ¹é…çš„collection: {self.collection_name} (ç»´åº¦: {collection_dim} -> {model_dim})")
                    
                    # é‡æ–°åˆ›å»ºcollection
                    self.chroma_collection = self.chroma_client.get_or_create_collection(
                        name=self.collection_name
                    )
                    print(f"âœ… å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name} (ç»´åº¦: {model_dim})")
                else:
                    # ç»´åº¦åŒ¹é…ï¼Œä½¿ç”¨ç°æœ‰collection
                    self.chroma_collection = existing_collection
                    print(f"âœ… Collectionç»´åº¦æ£€æŸ¥é€šè¿‡: {model_dim}ç»´")
                    logger.info(f"Collectionç»´åº¦åŒ¹é…: {model_dim}ç»´")
                    
            except Exception as e:
                # Collectionä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                if "does not exist" in str(e) or "not found" in str(e).lower():
                    self.chroma_collection = self.chroma_client.get_or_create_collection(
                        name=self.collection_name
                    )
                    print(f"âœ… åˆ›å»ºæ–°collection: {self.collection_name} (ç»´åº¦: {model_dim})")
                    logger.info(f"åˆ›å»ºæ–°collection: {self.collection_name} (ç»´åº¦: {model_dim})")
                else:
                    # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
                    logger.error(f"è·å–collectionæ—¶å‡ºé”™: {e}")
                    raise
                    
        except Exception as e:
            # å¦‚æœæ£€æµ‹è¿‡ç¨‹å‡ºé”™ï¼Œå°è¯•åˆ é™¤æ—§collectionå¹¶é‡å»ºï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            logger.error(f"ç»´åº¦æ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}")
            logger.info("é‡‡ç”¨ä¿å®ˆç­–ç•¥ï¼šåˆ é™¤æ—§collectionå¹¶é‡å»º")
            
            try:
                # å°è¯•åˆ é™¤æ—§collectionï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                try:
                    self.chroma_client.delete_collection(name=self.collection_name)
                    logger.info(f"å·²åˆ é™¤å¯èƒ½ä¸å…¼å®¹çš„collection: {self.collection_name}")
                    print(f"ğŸ”„ å·²åˆ é™¤å¯èƒ½ä¸å…¼å®¹çš„collection: {self.collection_name}")
                except:
                    # å¦‚æœåˆ é™¤å¤±è´¥ï¼ˆcollectionä¸å­˜åœ¨ï¼‰ï¼Œç»§ç»­åˆ›å»ºæ–°collection
                    pass
                
                # åˆ›å»ºæ–°collection
                self.chroma_collection = self.chroma_client.get_or_create_collection(
                    name=self.collection_name
                )
                print(f"âœ… å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name}")
                logger.info(f"å·²é‡æ–°åˆ›å»ºcollection: {self.collection_name}")
            except Exception as fallback_error:
                logger.error(f"å›é€€åˆ›å»ºcollectionä¹Ÿå¤±è´¥: {fallback_error}")
                raise
    
    def clear_index(self):
        """æ¸…ç©ºç´¢å¼•"""
        try:
            # åˆ é™¤é›†åˆ
            self.chroma_client.delete_collection(name=self.collection_name)
            print(f"âœ… å·²åˆ é™¤é›†åˆ: {self.collection_name}")
            
            # é‡æ–°åˆ›å»ºé›†åˆ
            self.chroma_collection = self.chroma_client.get_or_create_collection(
                name=self.collection_name
            )
            self.vector_store = ChromaVectorStore(chroma_collection=self.chroma_collection)
            self.storage_context = StorageContext.from_defaults(
                vector_store=self.vector_store
            )
            
            # é‡ç½®ç´¢å¼•
            self._index = None
            print("âœ… ç´¢å¼•å·²æ¸…ç©º")
            
        except Exception as e:
            print(f"âŒ æ¸…ç©ºç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def get_stats(self) -> dict:
        """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # æ£€æŸ¥chroma_collectionæ˜¯å¦å·²åˆå§‹åŒ–
            if not hasattr(self, 'chroma_collection') or self.chroma_collection is None:
                logger.warning("âš ï¸  chroma_collectionæœªåˆå§‹åŒ–ï¼Œæ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯")
                print(f"âš ï¸  chroma_collectionæœªåˆå§‹åŒ–ï¼Œæ— æ³•è·å–ç»Ÿè®¡ä¿¡æ¯")
                return {
                    "collection_name": self.collection_name,
                    "document_count": 0,
                    "embedding_model": self.embedding_model_name,
                    "chunk_size": self.chunk_size,
                    "chunk_overlap": self.chunk_overlap,
                    "error": "chroma_collectionæœªåˆå§‹åŒ–"
                }
            
            count = self.chroma_collection.count()
            logger.debug(f"Collection '{self.collection_name}' å‘é‡æ•°é‡: {count}")
            
            return {
                "collection_name": self.collection_name,
                "document_count": count,
                "embedding_model": self.embedding_model_name,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
            }
        except AttributeError as e:
            error_msg = f"chroma_collectionå±æ€§è®¿é—®å¤±è´¥: {e}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            return {
                "collection_name": self.collection_name,
                "document_count": 0,
                "embedding_model": self.embedding_model_name,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "error": str(e)
            }
        except Exception as e:
            error_msg = f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}"
            logger.error(error_msg, exc_info=True)
            print(f"âŒ {error_msg}")
            return {
                "collection_name": self.collection_name,
                "document_count": 0,
                "embedding_model": self.embedding_model_name,
                "chunk_size": self.chunk_size,
                "chunk_overlap": self.chunk_overlap,
                "error": str(e)
            }
    
    def search(self, query: str, top_k: int = 5) -> List[dict]:
        """æœç´¢ç›¸ä¼¼æ–‡æ¡£ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if self._index is None:
            self.get_index()
        
        retriever = self._index.as_retriever(similarity_top_k=top_k)
        nodes = retriever.retrieve(query)
        
        results = []
        for node in nodes:
            results.append({
                "text": node.node.text,
                "score": node.score,
                "metadata": node.node.metadata,
            })
        
        return results
    
    def close(self):
        """å…³é—­ç´¢å¼•ç®¡ç†å™¨ï¼Œé‡Šæ”¾èµ„æº
        
        æ˜¾å¼å…³é—­ Chroma å®¢æˆ·ç«¯è¿æ¥ï¼Œåœæ­¢åå°çº¿ç¨‹
        åº”è¯¥åœ¨åº”ç”¨å…³é—­æ—¶è°ƒç”¨æ­¤æ–¹æ³•
        """
        try:
            logger.info("ğŸ”§ å¼€å§‹å…³é—­ç´¢å¼•ç®¡ç†å™¨...")
            
            # 1. æ¸…ç† Chroma å®¢æˆ·ç«¯
            if hasattr(self, 'chroma_client') and self.chroma_client is not None:
                try:
                    # å°è¯•å¤šç§æ–¹å¼å…³é—­å®¢æˆ·ç«¯
                    client = self.chroma_client
                    
                    # æ–¹æ³•1: å°è¯•è°ƒç”¨ close() æ–¹æ³•
                    if hasattr(client, 'close'):
                        client.close()
                        logger.info("âœ… Chromaå®¢æˆ·ç«¯å·²é€šè¿‡ close() æ–¹æ³•å…³é—­")
                    # æ–¹æ³•2: å°è¯•è°ƒç”¨ reset() æ–¹æ³•ï¼ˆæŸäº›ç‰ˆæœ¬æ”¯æŒï¼‰
                    elif hasattr(client, 'reset'):
                        client.reset()
                        logger.info("âœ… Chromaå®¢æˆ·ç«¯å·²é€šè¿‡ reset() æ–¹æ³•é‡ç½®")
                    # æ–¹æ³•3: å°è¯•è®¿é—®å†…éƒ¨å±æ€§å¹¶å…³é—­
                    elif hasattr(client, '_client'):
                        # æŸäº›ç‰ˆæœ¬çš„ Chroma å¯èƒ½æœ‰å†…éƒ¨å®¢æˆ·ç«¯
                        inner_client = getattr(client, '_client', None)
                        if inner_client and hasattr(inner_client, 'close'):
                            inner_client.close()
                            logger.info("âœ… Chromaå†…éƒ¨å®¢æˆ·ç«¯å·²å…³é—­")
                    
                    # æ¸…ç†å¼•ç”¨
                    self.chroma_client = None
                    logger.info("âœ… Chromaå®¢æˆ·ç«¯å¼•ç”¨å·²æ¸…ç†")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸  å…³é—­ Chroma å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
                    # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¦æ¸…ç†å¼•ç”¨
                    self.chroma_client = None
            
            # 2. æ¸…ç†å…¶ä»–å¼•ç”¨
            if hasattr(self, 'chroma_collection'):
                self.chroma_collection = None
            if hasattr(self, 'vector_store'):
                self.vector_store = None
            if hasattr(self, 'storage_context'):
                self.storage_context = None
            if hasattr(self, '_index'):
                self._index = None
            
            # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼ˆå¯é€‰ï¼Œå¸®åŠ©æ¸…ç†çº¿ç¨‹ï¼‰
            try:
                import gc
                gc.collect()
                logger.debug("âœ… å·²æ‰§è¡Œåƒåœ¾å›æ”¶")
            except Exception as e:
                logger.debug(f"åƒåœ¾å›æ”¶æ—¶å‡ºé”™: {e}")
            
            logger.info("âœ… ç´¢å¼•ç®¡ç†å™¨èµ„æºå·²é‡Šæ”¾")
            
        except Exception as e:
            logger.warning(f"âš ï¸  å…³é—­ç´¢å¼•ç®¡ç†å™¨æ—¶å‡ºé”™: {e}")
            # å³ä½¿å‡ºé”™ï¼Œä¹Ÿè¦å°½å¯èƒ½æ¸…ç†å¼•ç”¨
            try:
                self.chroma_client = None
                self.chroma_collection = None
                self.vector_store = None
                self.storage_context = None
                self._index = None
            except:
                pass
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºè¢«é‡Šæ”¾"""
        try:
            self.close()
        except Exception:
            # ææ„å‡½æ•°ä¸­çš„å¼‚å¸¸åº”è¯¥è¢«å¿½ç•¥
            pass


def create_index_from_directory(
    directory_path: str | Path,
    collection_name: Optional[str] = None,
    recursive: bool = True
) -> IndexManager:
    """ä»ç›®å½•åˆ›å»ºç´¢å¼•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        directory_path: æ–‡æ¡£ç›®å½•è·¯å¾„
        collection_name: é›†åˆåç§°
        recursive: æ˜¯å¦é€’å½’åŠ è½½
        
    Returns:
        IndexManagerå¯¹è±¡
    """
    from src.data_loader import load_documents_from_directory
    
    # åŠ è½½æ–‡æ¡£
    print(f"ğŸ“‚ ä»ç›®å½•åŠ è½½æ–‡æ¡£: {directory_path}")
    documents = load_documents_from_directory(directory_path, recursive=recursive)
    
    if not documents:
        print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return IndexManager(collection_name=collection_name)
    
    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index_manager = IndexManager(collection_name=collection_name)
    
    # æ„å»ºç´¢å¼•
    index_manager.build_index(documents)
    
    return index_manager


def create_index_from_urls(
    urls: List[str],
    collection_name: Optional[str] = None
) -> IndexManager:
    """ä»URLåˆ—è¡¨åˆ›å»ºç´¢å¼•ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        urls: URLåˆ—è¡¨
        collection_name: é›†åˆåç§°
        
    Returns:
        IndexManagerå¯¹è±¡
    """
    from src.data_loader import load_documents_from_urls
    
    # åŠ è½½æ–‡æ¡£
    print(f"ğŸŒ ä» {len(urls)} ä¸ªURLåŠ è½½æ–‡æ¡£")
    documents = load_documents_from_urls(urls)
    
    if not documents:
        print("âš ï¸  æœªæˆåŠŸåŠ è½½ä»»ä½•ç½‘é¡µ")
        return IndexManager(collection_name=collection_name)
    
    # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
    index_manager = IndexManager(collection_name=collection_name)
    
    # æ„å»ºç´¢å¼•
    index_manager.build_index(documents)
    
    return index_manager


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=== æµ‹è¯•ç´¢å¼•æ„å»º ===\n")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        LlamaDocument(
            text="ç³»ç»Ÿç§‘å­¦æ˜¯ç ”ç©¶ç³»ç»Ÿçš„ä¸€èˆ¬è§„å¾‹å’Œæ–¹æ³•çš„ç§‘å­¦ã€‚",
            metadata={"title": "ç³»ç»Ÿç§‘å­¦ç®€ä»‹", "source": "test"}
        ),
        LlamaDocument(
            text="é’±å­¦æ£®æ˜¯ä¸­å›½ç³»ç»Ÿç§‘å­¦çš„åˆ›å»ºè€…ä¹‹ä¸€ï¼Œä»–æå‡ºäº†ç³»ç»Ÿå·¥ç¨‹çš„ç†è®ºä½“ç³»ã€‚",
            metadata={"title": "é’±å­¦æ£®ä¸ç³»ç»Ÿç§‘å­¦", "source": "test"}
        ),
    ]
    
    # åˆ›å»ºç´¢å¼•
    index_manager = IndexManager(collection_name="test_collection")
    index_manager.build_index(test_docs)
    
    # æµ‹è¯•æœç´¢
    print("\n=== æµ‹è¯•æœç´¢ ===")
    results = index_manager.search("é’±å­¦æ£®", top_k=2)
    for i, result in enumerate(results, 1):
        print(f"\nç»“æœ {i}:")
        print(f"ç›¸ä¼¼åº¦: {result['score']:.4f}")
        print(f"å†…å®¹: {result['text']}")
    
    # æ¸…ç†æµ‹è¯•æ•°æ®
    index_manager.clear_index()
    print("\nâœ… æµ‹è¯•å®Œæˆ")

