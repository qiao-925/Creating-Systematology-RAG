"""
ç½‘é¡µåŠ è½½å™¨æ¨¡å—
ä»URLåˆ—è¡¨åŠ è½½ç½‘é¡µæ–‡æ¡£
"""

from typing import List

from llama_index.core.schema import Document as LlamaDocument

try:
    from llama_index.readers.web import SimpleWebPageReader
except ImportError:
    SimpleWebPageReader = None

from src.data_source import WebSource
from src.data_loader.processor import DocumentProcessor, safe_print
from src.data_loader.source_loader import load_documents_from_source, NEW_ARCHITECTURE_AVAILABLE
from src.logger import setup_logger

logger = setup_logger('data_loader')


def load_documents_from_urls(urls: List[str], 
                            clean: bool = True) -> List[LlamaDocument]:
    """ä»URLåˆ—è¡¨åŠ è½½æ–‡æ¡£ï¼ˆä½¿ç”¨å®˜æ–¹ SimpleWebPageReaderï¼‰
    
    Args:
        urls: URLåˆ—è¡¨
        clean: æ˜¯å¦æ¸…ç†æ–‡æœ¬
        
    Returns:
        Documentå¯¹è±¡åˆ—è¡¨
    """
    if SimpleWebPageReader is None:
        safe_print("âŒ ç¼ºå°‘ä¾èµ–ï¼šllama-index-readers-web")
        safe_print("   å®‰è£…ï¼špip install llama-index-readers-web")
        logger.error("SimpleWebPageReader æœªå®‰è£…")
        return []
    
    # ä½¿ç”¨æ–°æ¶æ„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if NEW_ARCHITECTURE_AVAILABLE:
        try:
            source = WebSource(urls=urls)
            documents = load_documents_from_source(source, clean=clean, show_progress=True)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            source.cleanup()
            
            return documents
        except Exception as e:
            logger.warning(f"æ–°æ¶æ„åŠ è½½å¤±è´¥ï¼Œå›é€€åˆ°æ—§å®ç°: {e}")
    
    # å›é€€åˆ°æ—§å®ç°
    if not urls:
        safe_print("âš ï¸  URL åˆ—è¡¨ä¸ºç©º")
        return []
    
    try:
        logger.info(f"å¼€å§‹åŠ è½½ {len(urls)} ä¸ªç½‘é¡µ")
        
        # ä½¿ç”¨ SimpleWebPageReader åŠ è½½ç½‘é¡µ
        reader = SimpleWebPageReader(html_to_text=True)
        documents = reader.load_data(urls)
        
        if not documents:
            safe_print("âš ï¸  æœªæˆåŠŸåŠ è½½ä»»ä½•ç½‘é¡µ")
            logger.warning("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•ç½‘é¡µ")
            return []
        
        # å¢å¼ºå…ƒæ•°æ®
        for i, doc in enumerate(documents):
            url = urls[i] if i < len(urls) else "unknown"
            
            doc.metadata.update({
                "source_type": "web",
                "url": url,
            })
            
            safe_print(f"âœ… å·²åŠ è½½: {url}")
        
        safe_print(f"\nğŸŒ æ€»å…±åŠ è½½äº† {len(documents)} ä¸ªç½‘é¡µ")
        logger.info(f"æˆåŠŸåŠ è½½ {len(documents)} ä¸ªç½‘é¡µ")
        
        # å¯é€‰çš„æ–‡æœ¬æ¸…ç†
        if clean:
            processor = DocumentProcessor()
            cleaned_documents = []
            for doc in documents:
                cleaned_text = processor.clean_text(doc.text)
                cleaned_doc = LlamaDocument(
                    text=cleaned_text,
                    metadata=doc.metadata,
                    id_=doc.id_
                )
                cleaned_documents.append(cleaned_doc)
            return cleaned_documents
        
        return documents
        
    except Exception as e:
        safe_print(f"âŒ åŠ è½½ç½‘é¡µå¤±è´¥: {e}")
        logger.error(f"åŠ è½½ç½‘é¡µå¤±è´¥: {e}")
        return []

