"""
RAGASè¯„ä¼°å™¨è§‚å¯Ÿå™¨

ä½¿ç”¨RAGASæ¡†æ¶è¿›è¡ŒRAGç³»ç»Ÿè¯„ä¼°
"""

from typing import Any, Dict, List, Optional
from datetime import datetime

from src.observers.base import BaseObserver, ObserverType
from src.config import config
from src.logger import setup_logger

logger = setup_logger('ragas_evaluator')


class RAGASEvaluator(BaseObserver):
    """RAGASè¯„ä¼°å™¨è§‚å¯Ÿå™¨
    
    ä½¿ç”¨RAGASæ¡†æ¶è¿›è¡ŒRAGç³»ç»Ÿè¯„ä¼°
    æ”¯æŒå¤šç»´åº¦è¯„ä¼°æŒ‡æ ‡ï¼š
    - faithfulnessï¼ˆå¿ å®åº¦ï¼‰
    - context_precisionï¼ˆä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ï¼‰
    - context_recallï¼ˆä¸Šä¸‹æ–‡å¬å›ç‡ï¼‰
    - answer_relevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰
    - context_relevancyï¼ˆä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼‰
    """
    
    def __init__(
        self,
        name: str = "ragas_evaluator",
        enabled: bool = True,
        metrics: Optional[List[str]] = None,
        batch_size: int = 10,
    ):
        """åˆå§‹åŒ–RAGASè¯„ä¼°å™¨
        
        Args:
            name: è§‚å¯Ÿå™¨åç§°
            enabled: æ˜¯å¦å¯ç”¨
            metrics: è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨ï¼ˆé»˜è®¤ä½¿ç”¨æ‰€æœ‰æŒ‡æ ‡ï¼‰
            batch_size: æ‰¹é‡è¯„ä¼°å¤§å°
        """
        super().__init__(name, enabled)
        self.metrics = metrics or [
            "faithfulness",
            "context_precision",
            "context_recall",
            "answer_relevancy",
            "context_relevancy",
        ]
        self.batch_size = batch_size
        
        # è¯„ä¼°æ•°æ®å­˜å‚¨
        self.evaluation_data: List[Dict[str, Any]] = []
        self.evaluation_results: List[Dict[str, Any]] = []
        
        # RAGASç›¸å…³å¯¹è±¡
        self.dataset = None
        
        if self.enabled:
            self.setup()
    
    def get_observer_type(self) -> ObserverType:
        return ObserverType.EVALUATION
    
    def setup(self) -> None:
        """è®¾ç½®RAGASè¯„ä¼°å™¨"""
        logger.info("ğŸ“Š åˆå§‹åŒ– RAGAS è¯„ä¼°å™¨")
        
        try:
            # å»¶è¿Ÿå¯¼å…¥RAGASï¼ˆå› ä¸ºå®ƒæ˜¯å¯é€‰ä¾èµ–ï¼‰
            import ragas
            from ragas import evaluate
            from ragas.datasets_schema import Dataset
            
            self.ragas = ragas
            self.evaluate = evaluate
            self.Dataset = Dataset
            
            logger.info(f"âœ… RAGAS è¯„ä¼°å™¨å·²åˆå§‹åŒ–")
            logger.info(f"   è¯„ä¼°æŒ‡æ ‡: {', '.join(self.metrics)}")
            
        except ImportError as e:
            logger.warning(f"âš ï¸  RAGAS æœªå®‰è£…: {e}")
            logger.info("   è¯·è¿è¡Œ: pip install ragas")
            logger.info("   è§‚å¯Ÿå™¨å°†è¢«ç¦ç”¨")
            self.enabled = False
        except Exception as e:
            logger.error(f"âŒ RAGAS åˆå§‹åŒ–å¤±è´¥: {e}")
            self.enabled = False
    
    def on_query_start(self, query: str, **kwargs) -> Optional[str]:
        """æŸ¥è¯¢å¼€å§‹æ—¶å›è°ƒ"""
        if not self.enabled:
            return None
        
        # è®°å½•æŸ¥è¯¢å¼€å§‹æ—¶é—´
        self.current_query_start = datetime.now()
        logger.debug(f"ğŸ” RAGAS è®°å½•æŸ¥è¯¢: {query[:50]}...")
        return None
    
    def on_query_end(
        self,
        query: str,
        answer: str,
        sources: List[Dict],
        trace_id: Optional[str] = None,
        **kwargs
    ) -> None:
        """æŸ¥è¯¢ç»“æŸæ—¶å›è°ƒ"""
        if not self.enabled:
            return
        
        try:
            # æå–ä¸Šä¸‹æ–‡
            contexts = []
            for source in sources:
                if isinstance(source, dict):
                    # ä»sourceä¸­æå–æ–‡æœ¬
                    context_text = source.get('text', '') or source.get('content', '')
                    if context_text:
                        contexts.append(context_text)
                elif hasattr(source, 'text'):
                    contexts.append(source.text)
            
            # è®°å½•è¯„ä¼°æ•°æ®
            evaluation_entry = {
                "question": query,
                "answer": answer,
                "contexts": contexts,
                "ground_truth": kwargs.get('ground_truth', None),  # å¯é€‰çš„çœŸå€¼
                "timestamp": datetime.now().isoformat(),
                "trace_id": trace_id,
            }
            
            self.evaluation_data.append(evaluation_entry)
            logger.debug(f"âœ… RAGAS è®°å½•æŸ¥è¯¢å®Œæˆ: {len(contexts)} ä¸ªä¸Šä¸‹æ–‡")
            
            # å¦‚æœè¾¾åˆ°æ‰¹é‡å¤§å°ï¼Œæ‰§è¡Œæ‰¹é‡è¯„ä¼°
            if len(self.evaluation_data) >= self.batch_size:
                self._run_batch_evaluation()
                
        except Exception as e:
            logger.warning(f"âš ï¸  RAGAS è®°å½•æŸ¥è¯¢å¤±è´¥: {e}")
    
    def on_retrieval(self, query: str, nodes: List[Any], **kwargs) -> None:
        """æ£€ç´¢å®Œæˆæ—¶å›è°ƒï¼ˆå¯é€‰ï¼‰"""
        if not self.enabled:
            return
        
        # å¯ä»¥åœ¨è¿™é‡Œè®°å½•æ£€ç´¢ç›¸å…³çš„ä¿¡æ¯
        logger.debug(f"ğŸ“š RAGAS è®°å½•æ£€ç´¢: {len(nodes)} ä¸ªèŠ‚ç‚¹")
    
    def _run_batch_evaluation(self) -> None:
        """æ‰§è¡Œæ‰¹é‡è¯„ä¼°"""
        if not self.enabled or not self.evaluation_data:
            return
        
        try:
            logger.info(f"ğŸ“Š å¼€å§‹æ‰¹é‡è¯„ä¼°: {len(self.evaluation_data)} æ¡æ•°æ®")
            
            # å‡†å¤‡æ•°æ®é›†
            dataset_dict = {
                "question": [entry["question"] for entry in self.evaluation_data],
                "answer": [entry["answer"] for entry in self.evaluation_data],
                "contexts": [entry["contexts"] for entry in self.evaluation_data],
            }
            
            # å¦‚æœæœ‰ground_truthï¼Œæ·»åŠ åˆ°æ•°æ®é›†
            if any(entry.get("ground_truth") for entry in self.evaluation_data):
                dataset_dict["ground_truth"] = [
                    entry.get("ground_truth", "") for entry in self.evaluation_data
                ]
            
            # åˆ›å»ºæ•°æ®é›†
            dataset = self.Dataset.from_dict(dataset_dict)
            
            # æ‰§è¡Œè¯„ä¼°
            result = self.evaluate(
                dataset=dataset,
                metrics=self.metrics,
            )
            
            # ä¿å­˜ç»“æœ
            evaluation_result = {
                "timestamp": datetime.now().isoformat(),
                "data_count": len(self.evaluation_data),
                "metrics": result.to_dict() if hasattr(result, 'to_dict') else str(result),
                "raw_result": result,
            }
            
            self.evaluation_results.append(evaluation_result)
            logger.info(f"âœ… æ‰¹é‡è¯„ä¼°å®Œæˆ: {len(self.evaluation_data)} æ¡æ•°æ®")
            
            # æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦
            if hasattr(result, '__dict__'):
                logger.info(f"ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦:")
                for metric_name in self.metrics:
                    if hasattr(result, metric_name):
                        metric_value = getattr(result, metric_name)
                        logger.info(f"   {metric_name}: {metric_value}")
            
            # æ¸…ç©ºå·²è¯„ä¼°çš„æ•°æ®
            self.evaluation_data.clear()
            
        except Exception as e:
            logger.error(f"âŒ RAGAS æ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
            logger.exception(e)
    
    def evaluate_all(self) -> Optional[Dict[str, Any]]:
        """è¯„ä¼°æ‰€æœ‰å·²æ”¶é›†çš„æ•°æ®
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        if not self.enabled:
            return None
        
        if not self.evaluation_data:
            logger.warning("âš ï¸  æ²¡æœ‰å¾…è¯„ä¼°çš„æ•°æ®")
            return None
        
        # æ‰§è¡Œæœ€åä¸€æ¬¡æ‰¹é‡è¯„ä¼°
        self._run_batch_evaluation()
        
        if not self.evaluation_results:
            return None
        
        # è¿”å›æœ€æ–°çš„è¯„ä¼°ç»“æœ
        return self.evaluation_results[-1] if self.evaluation_results else None
    
    def get_report(self) -> Dict[str, Any]:
        """è·å–è¯„ä¼°æŠ¥å‘Š"""
        report = {
            "observer_type": "ragas_evaluator",
            "enabled": self.enabled,
            "metrics": self.metrics,
            "pending_evaluations": len(self.evaluation_data),
            "completed_evaluations": len(self.evaluation_results),
            "latest_result": self.evaluation_results[-1] if self.evaluation_results else None,
        }
        
        return report
    
    def teardown(self) -> None:
        """æ¸…ç†èµ„æº"""
        if not self.enabled:
            return
        
        # å¦‚æœæœ‰æœªè¯„ä¼°çš„æ•°æ®ï¼Œæ‰§è¡Œæœ€åä¸€æ¬¡è¯„ä¼°
        if self.evaluation_data:
            logger.info(f"ğŸ“Š æ¸…ç†å‰æ‰§è¡Œæœ€åä¸€æ¬¡è¯„ä¼°: {len(self.evaluation_data)} æ¡æ•°æ®")
            self._run_batch_evaluation()
        
        logger.info("âœ… RAGAS è¯„ä¼°å™¨å·²æ¸…ç†")

