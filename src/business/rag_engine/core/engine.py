"""
RAGå¼•æ“æ ¸å¿ƒæ¨¡å—ï¼šModularQueryEngineç±»å®ç°

ä¸»è¦åŠŸèƒ½ï¼š
- ModularQueryEngineç±»ï¼šæ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“ï¼Œæ”¯æŒvectorã€bm25ã€hybridã€grepã€multiç­‰ç­–ç•¥
- query()ï¼šæ‰§è¡ŒæŸ¥è¯¢ï¼Œè¿”å›æ ¼å¼åŒ–çš„å›ç­”å’Œå¼•ç”¨æ¥æº
- stream_query()ï¼šæµå¼æŸ¥è¯¢ï¼Œå®æ—¶è¿”å›ç­”æ¡ˆtokenï¼ˆç”¨äºWebåº”ç”¨ï¼‰

æ‰§è¡Œæµç¨‹ï¼š
1. åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“ï¼ˆåˆ›å»ºæ£€ç´¢å™¨ã€åå¤„ç†å™¨ç­‰ï¼‰
2. æ‰§è¡ŒæŸ¥è¯¢ï¼ˆéæµå¼æˆ–æµå¼ï¼‰
3. å¤„ç†æ£€ç´¢ç»“æœ
4. åº”ç”¨åå¤„ç†ï¼ˆé‡æ’åºç­‰ï¼‰
5. ç”Ÿæˆå›ç­”å¹¶æ ¼å¼åŒ–
6. è¿”å›æŸ¥è¯¢ç»“æœ

ç‰¹æ€§ï¼š
- æ¨¡å—åŒ–è®¾è®¡
- æ”¯æŒå¤šç§æ£€ç´¢ç­–ç•¥
- å¯æ’æ‹”çš„åå¤„ç†å™¨
- å®Œæ•´çš„é”™è¯¯å¤„ç†å’Œå…œåº•æœºåˆ¶
- çœŸæ­£çš„æµå¼è¾“å‡ºæ”¯æŒï¼ˆä½¿ç”¨DeepSeekåŸç”Ÿæµå¼APIï¼‰
"""

from typing import List, Optional, Tuple, Dict, Any
from llama_index.core import Settings
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core import get_response_synthesizer
from src.infrastructure.config import config
from src.infrastructure.indexer import IndexManager
from src.infrastructure.logger import get_logger
from src.business.rag_engine.formatting import ResponseFormatter
from src.infrastructure.observers.manager import ObserverManager
from src.infrastructure.observers.factory import create_observer_from_config
from src.business.rag_engine.retrieval.factory import create_retriever
from src.business.rag_engine.processing.execution import create_postprocessors, execute_query
from src.business.rag_engine.processing.query_processor import QueryProcessor
from src.business.rag_engine.utils.utils import handle_fallback
from src.infrastructure.llms import create_deepseek_llm_for_query
from src.business.rag_engine.models import QueryContext, QueryResult, SourceModel

logger = get_logger('rag_engine')


class ModularQueryEngine:
    """æ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“ï¼ˆå·¥å‚æ¨¡å¼ï¼‰"""
    
    SUPPORTED_STRATEGIES = ["vector", "bm25", "hybrid", "grep", "multi"]
    
    def __init__(
        self,
        index_manager: IndexManager,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        retrieval_strategy: Optional[str] = None,
        similarity_top_k: Optional[int] = None,
        enable_rerank: Optional[bool] = None,
        rerank_top_n: Optional[int] = None,
        reranker_type: Optional[str] = None,
        similarity_cutoff: Optional[float] = None,
        enable_markdown_formatting: bool = True,
        observer_manager: Optional[ObserverManager] = None,
        enable_auto_routing: Optional[bool] = None,
        **kwargs
    ):
        """åˆå§‹åŒ–æ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“"""
        self.index_manager = index_manager
        self.index = index_manager.get_index()
        
        # é…ç½®å‚æ•°
        self._load_config(
            retrieval_strategy, similarity_top_k, enable_rerank,
            rerank_top_n, reranker_type, similarity_cutoff, enable_auto_routing
        )
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.formatter = ResponseFormatter(enable_formatting=enable_markdown_formatting)
        self.observer_manager = self._setup_observer_manager(observer_manager)
        self.llm = self._setup_llm(api_key, model)
        self.query_processor = QueryProcessor(llm=self.llm)
        logger.info("æŸ¥è¯¢å¤„ç†å™¨å·²åˆå§‹åŒ–", note="æ ‡å‡†åŒ–æµç¨‹ï¼šæ„å›¾ç†è§£+æ”¹å†™")
        
        # åˆå§‹åŒ–è·¯ç”±å’Œæ£€ç´¢ç»„ä»¶
        self.query_router = self._setup_query_router(index_manager) if self.enable_auto_routing else None
        self.retriever, self.query_engine = self._setup_retrieval_components()
        self.postprocessors = create_postprocessors(
            self.index_manager,
            self.similarity_cutoff,
            self.enable_rerank,
            self.rerank_top_n,
            reranker_type=self.reranker_type,
        )
        
        self._log_initialization_summary()
    
    def _load_config(
        self,
        retrieval_strategy: Optional[str],
        similarity_top_k: Optional[int],
        enable_rerank: Optional[bool],
        rerank_top_n: Optional[int],
        reranker_type: Optional[str],
        similarity_cutoff: Optional[float],
        enable_auto_routing: Optional[bool]
    ) -> None:
        """åŠ è½½å¹¶éªŒè¯é…ç½®å‚æ•°"""
        self.retrieval_strategy = retrieval_strategy or config.RETRIEVAL_STRATEGY
        self.similarity_top_k = similarity_top_k or config.SIMILARITY_TOP_K
        self.enable_rerank = enable_rerank if enable_rerank is not None else config.ENABLE_RERANK
        self.rerank_top_n = rerank_top_n or config.RERANK_TOP_N
        self.reranker_type = reranker_type or config.RERANKER_TYPE
        self.similarity_cutoff = similarity_cutoff or config.SIMILARITY_CUTOFF
        self.enable_auto_routing = enable_auto_routing if enable_auto_routing is not None else config.ENABLE_AUTO_ROUTING
        
        if self.retrieval_strategy not in self.SUPPORTED_STRATEGIES:
            raise ValueError(
                f"ä¸æ”¯æŒçš„æ£€ç´¢ç­–ç•¥: {self.retrieval_strategy}. "
                f"æ”¯æŒçš„ç­–ç•¥: {self.SUPPORTED_STRATEGIES}"
            )
    
    def _setup_observer_manager(self, observer_manager: Optional[ObserverManager]) -> ObserverManager:
        """è®¾ç½®è§‚å¯Ÿå™¨ç®¡ç†å™¨"""
        if observer_manager is not None:
            manager = observer_manager
            logger.info("ä½¿ç”¨æä¾›çš„è§‚å¯Ÿå™¨ç®¡ç†å™¨", observer_count=len(manager.observers))
        else:
            manager = create_observer_from_config()
            logger.info("ä»é…ç½®åˆ›å»ºè§‚å¯Ÿå™¨ç®¡ç†å™¨", observer_count=len(manager.observers))
        
        callback_handlers = manager.get_callback_handlers()
        if callback_handlers:
            from llama_index.core.callbacks import CallbackManager
            Settings.callback_manager = CallbackManager(callback_handlers)
            logger.info("è®¾ç½®å›è°ƒå¤„ç†å™¨åˆ°LlamaIndex", handler_count=len(callback_handlers))
        
        return manager
    
    def _setup_llm(self, api_key: Optional[str], model: Optional[str]):
        """è®¾ç½®LLM"""
        api_key = api_key or config.DEEPSEEK_API_KEY
        model = model or config.LLM_MODEL
        if not api_key:
            raise ValueError("æœªè®¾ç½®DEEPSEEK_API_KEY")
        
        return create_deepseek_llm_for_query(
            api_key=api_key,
            model=model,
            max_tokens=4096,
        )
    
    def _setup_query_router(self, index_manager: IndexManager):
        """è®¾ç½®æŸ¥è¯¢è·¯ç”±å™¨"""
        from src.business.rag_engine.routing.query_router import QueryRouter
        router = QueryRouter(
            index_manager=index_manager,
            llm=self.llm,
            enable_auto_routing=True,
        )
        logger.info("æŸ¥è¯¢è·¯ç”±å™¨å·²å¯ç”¨", mode="è‡ªåŠ¨è·¯ç”±æ¨¡å¼")
        return router
    
    def _setup_retrieval_components(self) -> Tuple[Optional[Any], Optional[RetrieverQueryEngine]]:
        """è®¾ç½®æ£€ç´¢ç»„ä»¶"""
        if not self.enable_auto_routing:
            retriever = create_retriever(
                self.index,
                self.retrieval_strategy,
                self.similarity_top_k
            )
            query_engine = self._create_query_engine_from_retriever(retriever)
            return retriever, query_engine
        else:
            return None, None
    
    def _log_initialization_summary(self) -> None:
        """è®°å½•åˆå§‹åŒ–æ‘˜è¦"""
        logger.info(
            "æ¨¡å—åŒ–æŸ¥è¯¢å¼•æ“åˆå§‹åŒ–å®Œæˆ",
            strategy=self.retrieval_strategy,
            top_k=self.similarity_top_k,
            rerank_enabled=self.enable_rerank,
            similarity_cutoff=self.similarity_cutoff,
            auto_routing=self.enable_auto_routing
        )
    
    def _create_query_engine_from_retriever(self, retriever, streaming: bool = False) -> RetrieverQueryEngine:
        """ä»æ£€ç´¢å™¨åˆ›å»ºæŸ¥è¯¢å¼•æ“
        
        Args:
            retriever: æ£€ç´¢å™¨å®ä¾‹
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            
        Returns:
            RetrieverQueryEngineå®ä¾‹
        """
        if streaming:
            # åˆ›å»ºæµå¼å“åº”åˆæˆå™¨
            response_synthesizer = get_response_synthesizer(
                streaming=True,
                llm=self.llm
            )
            return RetrieverQueryEngine(
                retriever=retriever,
                response_synthesizer=response_synthesizer,
                node_postprocessors=self.postprocessors,
            )
        else:
            return RetrieverQueryEngine.from_args(
                retriever=retriever,
                llm=self.llm,
                node_postprocessors=self.postprocessors,
            )
    
    def _get_or_create_query_engine(
        self,
        final_query: str,
        understanding: Optional[Dict[str, Any]] = None,
        streaming: bool = False
    ) -> Tuple[RetrieverQueryEngine, str]:
        """è·å–æˆ–åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼ˆæ”¯æŒè‡ªåŠ¨è·¯ç”±ï¼‰
        
        Args:
            final_query: å¤„ç†åçš„æŸ¥è¯¢
            understanding: æŸ¥è¯¢ç†è§£ç»“æœï¼ˆå¯é€‰ï¼‰
            streaming: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
            
        Returns:
            (æŸ¥è¯¢å¼•æ“å®ä¾‹, è·¯ç”±å†³ç­–/ç­–ç•¥åç§°)
        """
        if self.enable_auto_routing and self.query_router:
            # è‡ªåŠ¨è·¯ç”±æ¨¡å¼ï¼šæ ¹æ®æŸ¥è¯¢æ„å›¾åŠ¨æ€é€‰æ‹©æ£€ç´¢å™¨
            if understanding:
                retriever, routing_decision = self.query_router.route_with_understanding(
                    final_query,
                    understanding=understanding,
                    top_k=self.similarity_top_k
                )
            else:
                retriever, routing_decision = self.query_router.route(
                    final_query,
                    top_k=self.similarity_top_k
                )
            
            query_engine = self._create_query_engine_from_retriever(retriever, streaming=streaming)
            strategy_info = f"ç­–ç•¥={routing_decision}, åŸå› =è‡ªåŠ¨è·¯ç”±æ¨¡å¼ï¼Œæ ¹æ®æŸ¥è¯¢æ„å›¾åŠ¨æ€é€‰æ‹©"
            return query_engine, strategy_info
        else:
            # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨åˆå§‹åŒ–æ—¶åˆ›å»ºçš„æŸ¥è¯¢å¼•æ“
            # å¦‚æœæ˜¯æµå¼æ¨¡å¼ï¼Œéœ€è¦é‡æ–°åˆ›å»ºæµå¼æŸ¥è¯¢å¼•æ“
            if streaming:
                # è·å–å½“å‰æŸ¥è¯¢å¼•æ“çš„æ£€ç´¢å™¨
                current_retriever = self.retriever
                if current_retriever:
                    query_engine = self._create_query_engine_from_retriever(current_retriever, streaming=True)
                    strategy_info = f"ç­–ç•¥={self.retrieval_strategy}, åŸå› =å›ºå®šæ£€ç´¢æ¨¡å¼ï¼ˆæµå¼ï¼‰"
                    return query_engine, strategy_info
            strategy_info = f"ç­–ç•¥={self.retrieval_strategy}, åŸå› =å›ºå®šæ£€ç´¢æ¨¡å¼ï¼ˆåˆå§‹åŒ–æ—¶é…ç½®ï¼‰"
            return self.query_engine, strategy_info
    
    def _execute_with_query_engine(
        self,
        query_engine: RetrieverQueryEngine,
        final_query: str,
        collect_trace: bool
    ) -> Tuple[str, List[dict], Optional[str], Optional[Dict[str, Any]]]:
        """ä½¿ç”¨æŸ¥è¯¢å¼•æ“æ‰§è¡ŒæŸ¥è¯¢
        
        Args:
            query_engine: æŸ¥è¯¢å¼•æ“å®ä¾‹
            final_query: å¤„ç†åçš„æŸ¥è¯¢
            collect_trace: æ˜¯å¦æ”¶é›†è¿½è¸ªä¿¡æ¯
            
        Returns:
            (ç­”æ¡ˆ, å¼•ç”¨æ¥æº, æ¨ç†é“¾å†…å®¹, è¿½è¸ªä¿¡æ¯)
        """
        return execute_query(
            query_engine,
            self.formatter,
            self.observer_manager,
            final_query,
            collect_trace
        )
    
    def query(
        self, 
        question: str, 
        collect_trace: bool = False
    ) -> Tuple[str, List[dict], Optional[str], Optional[Dict[str, Any]]]:
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆå…¼å®¹ç°æœ‰APIï¼‰
        
        Returns:
            (ç­”æ¡ˆ, å¼•ç”¨æ¥æº, æ¨ç†é“¾å†…å®¹, è¿½è¸ªä¿¡æ¯)
        """
        
        # Step 1: æŸ¥è¯¢å¤„ç†ï¼ˆæ ‡å‡†åŒ–æµç¨‹ï¼šæ„å›¾ç†è§£+æ”¹å†™ï¼‰
        processed = self.query_processor.process(question)
        final_query = processed["final_query"]
        understanding = processed.get("understanding")
        
        logger.info(
            "æŸ¥è¯¢å¤„ç†å®Œæˆ",
            original_query=question[:50] if len(question) > 50 else question,
            processed_query=final_query[:50] if len(final_query) > 50 else final_query,
            processing_method=processed['processing_method']
        )
        
        # Step 2: è·å–æˆ–åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼ˆæ”¯æŒè‡ªåŠ¨è·¯ç”±ï¼‰
        query_engine, strategy_info = self._get_or_create_query_engine(
            final_query,
            understanding
        )
        
        logger.info("ä½¿ç”¨æ£€ç´¢ç­–ç•¥", strategy_info=strategy_info)
        
        # Step 3: æ‰§è¡ŒæŸ¥è¯¢
        answer, sources, reasoning_content, trace_info = self._execute_with_query_engine(
            query_engine,
            final_query,
            collect_trace
        )
        
        # è®°å½•è¿½è¸ªä¿¡æ¯
        if collect_trace and trace_info:
            trace_info["original_query"] = question
            trace_info["processed_query"] = final_query
            trace_info["query_processing"] = processed
        
        # å¤„ç†å…œåº•é€»è¾‘ï¼ˆæ— æ¥æºã€ä½ç›¸ä¼¼åº¦æˆ–ç©ºç­”æ¡ˆæ—¶è§¦å‘ï¼‰
        # æ³¨æ„ï¼šä½¿ç”¨åŸå§‹æŸ¥è¯¢è¿›è¡Œå…œåº•å¤„ç†ï¼Œç¡®ä¿ç”¨æˆ·çœ‹åˆ°çš„æ˜¯åŸå§‹é—®é¢˜çš„ç­”æ¡ˆ
        answer, fallback_reason = handle_fallback(
            answer, sources, question, self.llm, self.similarity_cutoff
        )
        
        # å¦‚æœæ”¶é›†è¿½è¸ªä¿¡æ¯ï¼Œè®°å½•å…œåº•çŠ¶æ€
        if collect_trace and trace_info:
            trace_info['fallback_used'] = bool(fallback_reason)
            trace_info['fallback_reason'] = fallback_reason
        
        return answer, sources, reasoning_content, trace_info
    
    def query_with_context(
        self,
        context: QueryContext
    ) -> QueryResult:
        """æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨ QueryContext å’Œ QueryResult æ¨¡å‹ï¼‰
        
        Args:
            context: æŸ¥è¯¢ä¸Šä¸‹æ–‡æ¨¡å‹
            
        Returns:
            QueryResult æ¨¡å‹
        """
        logger.info(
            "æ‰§è¡ŒæŸ¥è¯¢ï¼ˆä½¿ç”¨ä¸Šä¸‹æ–‡æ¨¡å‹ï¼‰",
            query=context.query[:50] if len(context.query) > 50 else context.query,
            user_id=context.user_id,
            session_id=context.session_id,
            strategy=context.strategy,
            top_k=context.top_k
        )
        
        # ä½¿ç”¨å¤„ç†åçš„æŸ¥è¯¢æˆ–åŸå§‹æŸ¥è¯¢
        query_text = context.processed_query or context.query
        
        # æ‰§è¡ŒæŸ¥è¯¢
        answer, sources, reasoning_content, trace_info = self.query(
            query_text,
            collect_trace=bool(context.metadata.get('collect_trace', False))
        )
        
        # è½¬æ¢ä¸º SourceModel åˆ—è¡¨
        source_models = []
        for source in sources:
            if isinstance(source, dict):
                source_models.append(SourceModel(**source))
            else:
                source_models.append(SourceModel(
                    text=source.get('text', ''),
                    score=source.get('score', 0.0),
                    metadata=source.get('metadata', {}),
                    file_name=source.get('file_name'),
                    page_number=source.get('page_number'),
                    node_id=source.get('node_id')
                ))
        
        # åˆ›å»º QueryResult
        result = QueryResult(
            answer=answer,
            sources=source_models,
            reasoning_content=reasoning_content,
            trace_info=trace_info,
            metadata={
                **context.metadata,
                'query': context.query,
                'processed_query': context.processed_query,
                'strategy': context.strategy,
                'top_k': context.top_k
            }
        )
        
        logger.info(
            "æŸ¥è¯¢å®Œæˆ",
            user_id=context.user_id,
            sources_count=len(result.sources),
            answer_len=len(answer)
        )
        
        return result
    
    async def stream_query(self, question: str):
        """å¼‚æ­¥æµå¼æŸ¥è¯¢ï¼ˆç”¨äºWebåº”ç”¨ï¼‰- ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥ä½¿ç”¨ DeepSeek æµå¼è¾“å‡º
        
        ç»•è¿‡ LlamaIndex çš„ StreamingResponse ç¼“å†²ï¼Œç›´æ¥ä½¿ç”¨ DeepSeek çš„ stream_chatï¼Œ
        å®ç°çœŸæ­£çš„å®æ—¶æµå¼è¾“å‡ºã€‚
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Yields:
            dict: æµå¼å“åº”å­—å…¸ï¼ŒåŒ…å«ä»¥ä¸‹ç±»å‹ï¼š
                - 'type': 'token', 'data': tokenæ–‡æœ¬
                - 'type': 'sources', 'data': å¼•ç”¨æ¥æºåˆ—è¡¨
                - 'type': 'reasoning', 'data': æ¨ç†é“¾å†…å®¹
                - 'type': 'done', 'data': å®Œæ•´ç­”æ¡ˆå’Œå…ƒæ•°æ®
        """
        # Step 1: æŸ¥è¯¢å¤„ç†ï¼ˆæ ‡å‡†åŒ–æµç¨‹ï¼šæ„å›¾ç†è§£+æ”¹å†™ï¼‰
        processed = self.query_processor.process(question)
        final_query = processed["final_query"]
        understanding = processed.get("understanding")
        
        logger.info(
            "æµå¼æŸ¥è¯¢å¤„ç†å®Œæˆï¼ˆç›´æ¥æµå¼æ¨¡å¼ï¼‰",
            original_query=question[:50] if len(question) > 50 else question,
            processed_query=final_query[:50] if len(final_query) > 50 else final_query,
            processing_method=processed['processing_method']
        )
        
        # Step 2: è·å–æ£€ç´¢å™¨å’Œæ£€ç´¢èŠ‚ç‚¹
        retriever = None
        strategy_info = ""
        
        if self.enable_auto_routing and self.query_router:
            # è‡ªåŠ¨è·¯ç”±æ¨¡å¼
            if understanding:
                retriever, routing_decision = self.query_router.route_with_understanding(
                    final_query,
                    understanding=understanding,
                    top_k=self.similarity_top_k
                )
            else:
                retriever, routing_decision = self.query_router.route(
                    final_query,
                    top_k=self.similarity_top_k
                )
            strategy_info = f"ç­–ç•¥={routing_decision}, åŸå› =è‡ªåŠ¨è·¯ç”±æ¨¡å¼"
        else:
            # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨åˆå§‹åŒ–æ—¶åˆ›å»ºçš„æ£€ç´¢å™¨
            retriever = self.retriever
            strategy_info = f"ç­–ç•¥={self.retrieval_strategy}, åŸå› =å›ºå®šæ£€ç´¢æ¨¡å¼"
        
        logger.info("ä½¿ç”¨æ£€ç´¢ç­–ç•¥ï¼ˆç›´æ¥æµå¼ï¼‰", strategy_info=strategy_info)
        
        # Step 3: æ£€ç´¢èŠ‚ç‚¹
        nodes_with_scores = []
        sources = []
        full_answer = ""
        reasoning_content = ""
        
        try:
            if retriever:
                # æ‰§è¡Œæ£€ç´¢
                nodes_with_scores = retriever.retrieve(final_query)
                
                # åº”ç”¨åå¤„ç†
                if self.postprocessors:
                    for postprocessor in self.postprocessors:
                        nodes_with_scores = postprocessor.postprocess_nodes(
                            nodes_with_scores,
                            query_str=final_query
                        )
                
                # è½¬æ¢ä¸ºå¼•ç”¨æ¥æºæ ¼å¼
                for i, node_with_score in enumerate(nodes_with_scores, 1):
                    node = node_with_score.node if hasattr(node_with_score, 'node') else node_with_score
                    score = node_with_score.score if hasattr(node_with_score, 'score') else None
                    
                    source = {
                        'index': i,
                        'text': node.text if hasattr(node, 'text') else str(node),
                        'score': score,
                        'metadata': node.metadata if hasattr(node, 'metadata') else {},
                    }
                    sources.append(source)
                
                logger.info(f"æ£€ç´¢åˆ° {len(nodes_with_scores)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            # Step 4: æ„å»º prompt
            from src.business.rag_engine.formatting.templates import CHAT_MARKDOWN_TEMPLATE
            
            # æ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
            context_str = ""
            if nodes_with_scores:
                context_parts = []
                for i, node_with_score in enumerate(nodes_with_scores, 1):
                    node = node_with_score.node if hasattr(node_with_score, 'node') else node_with_score
                    text = node.text if hasattr(node, 'text') else str(node)
                    context_parts.append(f"[{i}] {text}")
                context_str = "\n\n".join(context_parts)
            else:
                context_str = "ï¼ˆçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼‰"
            
            # æ„å»ºå®Œæ•´ prompt
            # CHAT_MARKDOWN_TEMPLATE åªåŒ…å« context_strï¼Œéœ€è¦æ‰‹åŠ¨æ·»åŠ æŸ¥è¯¢
            prompt = CHAT_MARKDOWN_TEMPLATE.format(context_str=context_str)
            prompt += f"\n\nç”¨æˆ·é—®é¢˜ï¼š{final_query}\n\nè¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"
            
            # Step 5: ç›´æ¥ä½¿ç”¨ DeepSeek æµå¼è¾“å‡º
            import time
            from src.infrastructure.llms.reasoning import extract_reasoning_from_stream_chunk
            from llama_index.core.llms import ChatMessage, MessageRole
            
            # åˆ›å»º ChatMessage å¯¹è±¡ï¼ˆLlamaIndex è¦æ±‚ï¼‰
            chat_message = ChatMessage(
                role=MessageRole.USER,
                content=prompt
            )
            messages = [chat_message]
            
            last_token_time = time.time()
            token_count = 0
            last_chunk = None
            
            logger.debug("ğŸš€ å¼€å§‹ç›´æ¥æµå¼è°ƒç”¨ DeepSeek API")
            
            # ç›´æ¥è°ƒç”¨ DeepSeek çš„ stream_chatï¼ˆç»•è¿‡ LlamaIndex ç¼“å†²ï¼‰
            for chunk in self.llm.stream_chat(messages):
                # æå–æ¨ç†é“¾å†…å®¹ï¼ˆæµå¼ï¼‰
                chunk_reasoning = extract_reasoning_from_stream_chunk(chunk)
                if chunk_reasoning:
                    reasoning_content += chunk_reasoning
                
                # æå– token å†…å®¹ï¼ˆå¢é‡ï¼‰
                # DeepSeek æµå¼è¿”å›åº”è¯¥æ˜¯å¢é‡çš„ï¼Œæ£€æŸ¥å®é™…è¿”å›æ ¼å¼
                chunk_text = ""
                
                # è°ƒè¯•ï¼šè®°å½• chunk çš„ç»“æ„
                if token_count == 0:
                    logger.debug(f"ğŸ” Chunk ç»“æ„æ£€æŸ¥: hasattr(chunk, 'delta')={hasattr(chunk, 'delta')}, hasattr(chunk, 'message')={hasattr(chunk, 'message')}")
                    if hasattr(chunk, 'delta'):
                        delta = chunk.delta
                        logger.debug(f"ğŸ” Delta ç»“æ„: {dir(delta)}")
                        if hasattr(delta, 'content'):
                            logger.debug(f"ğŸ” Delta.content ç±»å‹: {type(delta.content)}, å€¼: {repr(delta.content)}")
                    if hasattr(chunk, 'message'):
                        message = chunk.message
                        logger.debug(f"ğŸ” Message ç»“æ„: {dir(message)}")
                        if hasattr(message, 'content'):
                            logger.debug(f"ğŸ” Message.content ç±»å‹: {type(message.content)}, å€¼é•¿åº¦: {len(str(message.content)) if message.content else 0}")
                
                # æå–å¢é‡ tokenï¼ˆDeepSeek æµå¼è¿”å›åº”è¯¥æ˜¯å¢é‡çš„ï¼‰
                # å…³é”®ï¼šmessage.content æ˜¯ç´¯åŠ çš„ï¼Œdelta.content æ˜¯å¢é‡çš„
                
                # æ–¹æ³•1ï¼šä¼˜å…ˆä½¿ç”¨ delta.contentï¼ˆå¢é‡ï¼‰
                if hasattr(chunk, 'delta'):
                    delta = chunk.delta
                    if hasattr(delta, 'content') and delta.content:
                        chunk_text = str(delta.content)
                        # éªŒè¯ï¼šdelta.content åº”è¯¥æ˜¯å¢é‡ï¼ˆå¾ˆçŸ­ï¼‰
                        if len(chunk_text) > 50:
                            logger.warning(f"âš ï¸ Delta.content é•¿åº¦å¼‚å¸¸: {len(chunk_text)} å­—ç¬¦ï¼Œå¯èƒ½æ˜¯ç´¯åŠ çš„ï¼å†…å®¹: {chunk_text[:50]}...")
                
                # æ–¹æ³•2ï¼šå¦‚æœæ²¡æœ‰ deltaï¼Œä» message.content è®¡ç®—å¢é‡
                elif hasattr(chunk, 'message'):
                    message = chunk.message
                    if hasattr(message, 'content') and message.content:
                        current_content = str(message.content)
                        # message.content æ˜¯ç´¯åŠ çš„ï¼Œè®¡ç®—å¢é‡ï¼šå½“å‰ - ä¹‹å‰
                        if full_answer and current_content.startswith(full_answer):
                            # æ­£å¸¸æƒ…å†µï¼šå½“å‰å†…å®¹åŒ…å«ä¹‹å‰çš„å†…å®¹ï¼Œæå–å¢é‡
                            chunk_text = current_content[len(full_answer):]
                            if not chunk_text:
                                # å¢é‡ä¸ºç©ºï¼Œå¯èƒ½æ˜¯é‡å¤çš„ chunkï¼Œè·³è¿‡
                                continue
                        elif not full_answer:
                            # ç¬¬ä¸€æ¬¡ï¼šä½¿ç”¨æ•´ä¸ªå†…å®¹
                            chunk_text = current_content
                        else:
                            # å¼‚å¸¸æƒ…å†µï¼šå½“å‰å†…å®¹ä¸åŒ…å«ä¹‹å‰çš„å†…å®¹
                            logger.warning(f"âš ï¸ Message.content æ ¼å¼å¼‚å¸¸: å½“å‰é•¿åº¦={len(current_content)}, ä¹‹å‰é•¿åº¦={len(full_answer)}")
                            # å°è¯•è®¡ç®—å¢é‡ï¼ˆå–å·®å€¼éƒ¨åˆ†ï¼‰
                            if len(current_content) > len(full_answer):
                                chunk_text = current_content[len(full_answer):]
                            else:
                                # å¦‚æœå½“å‰å†…å®¹æ›´çŸ­ï¼Œå¯èƒ½æ˜¯æ–°çš„å¼€å§‹ï¼Œä½¿ç”¨æ•´ä¸ªå†…å®¹
                                chunk_text = current_content
                                full_answer = ""  # é‡ç½®
                
                # æ–¹æ³•3ï¼šæ£€æŸ¥ raw å“åº”ï¼ˆOpenAI æ ¼å¼ï¼‰
                if not chunk_text and hasattr(chunk, 'raw'):
                    raw = chunk.raw
                    if isinstance(raw, dict):
                        choices = raw.get('choices', [])
                        if choices and len(choices) > 0:
                            choice = choices[0]
                            delta = choice.get('delta', {})
                            if isinstance(delta, dict):
                                chunk_text = delta.get('content', '')
                                if chunk_text:
                                    chunk_text = str(chunk_text)
                
                if chunk_text:
                    token_count += 1
                    current_time = time.time()
                    time_since_last = current_time - last_token_time
                    last_token_time = current_time
                    
                    # è®°å½•æ¯ä¸ª token çš„åˆ°è¾¾æ—¶é—´ï¼ˆä»…åœ¨å‰å‡ ä¸ªå’Œé—´éš”è¾ƒé•¿æ—¶è®°å½•ï¼‰
                    if token_count <= 5 or time_since_last > 0.1:
                        logger.debug(f"ğŸ”¤ Token #{token_count} '{chunk_text[:20]}...' åˆ°è¾¾ï¼Œé—´éš”: {time_since_last*1000:.1f}ms")
                    
                    full_answer += chunk_text
                    # ç«‹å³ yield tokenï¼ˆæ— ç¼“å†²ï¼‰- æ¯ä¸ª token å•ç‹¬è¿”å›ï¼Œä¸ç´¯è®¡
                    # æ³¨æ„ï¼šè¿™é‡Œ yield çš„æ˜¯å•ä¸ª tokenï¼Œä¸æ˜¯ç´¯è®¡çš„ full_answer
                    yield {'type': 'token', 'data': chunk_text}
                
                last_chunk = chunk
            
            logger.debug(f"âœ… æµå¼ç”Ÿæˆå®Œæˆï¼Œå…± {token_count} ä¸ª token")
            
            # Step 6: æ ¼å¼åŒ–ç­”æ¡ˆ
            full_answer = self.formatter.format(full_answer, None)
            
            # Step 7: æå–æœ€ç»ˆæ¨ç†é“¾ï¼ˆä»æœ€åä¸€ä¸ª chunkï¼‰
            if last_chunk:
                from src.infrastructure.llms import extract_reasoning_content
                final_reasoning = extract_reasoning_content(last_chunk)
                if final_reasoning:
                    reasoning_content = final_reasoning
            
            # è¿”å›å¼•ç”¨æ¥æº
            if sources:
                yield {'type': 'sources', 'data': sources}
            
            # è¿”å›æ¨ç†é“¾ï¼ˆç­”æ¡ˆå®Œæˆåï¼Œéæµå¼ï¼‰
            if reasoning_content:
                yield {'type': 'reasoning', 'data': reasoning_content}
            
            # è¿”å›å®Œæˆäº‹ä»¶
            yield {
                'type': 'done',
                'data': {
                    'answer': full_answer,
                    'sources': sources,
                    'reasoning_content': reasoning_content if reasoning_content else None,
                }
            }
            
        except Exception as e:
            logger.error(f"æµå¼æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            # å‘é€é”™è¯¯äº‹ä»¶
            yield {
                'type': 'error',
                'data': {'message': str(e)}
            }
            raise


