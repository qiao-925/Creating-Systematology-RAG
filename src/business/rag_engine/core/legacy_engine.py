"""
RAGå¼•æ“é—ç•™æ¨¡å—ï¼šCitationQueryEngineå®ç°ï¼Œå¸¦å¼•ç”¨æº¯æº

ä¸»è¦åŠŸèƒ½ï¼š
- QueryEngineç±»ï¼šæŸ¥è¯¢å¼•æ“ï¼ŒåŸºäºCitationQueryEngineå®ç°
- query()ï¼šæ‰§è¡ŒæŸ¥è¯¢ï¼Œè¿”å›å¸¦å¼•ç”¨çš„å›ç­”
- æ”¯æŒæ¨ç†é“¾æå–å’Œæ ¼å¼åŒ–

æ‰§è¡Œæµç¨‹ï¼š
1. åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“ï¼ˆè¿æ¥ç´¢å¼•ã€LLMã€å›è°ƒç®¡ç†å™¨ï¼‰
2. æ‰§è¡ŒæŸ¥è¯¢å¹¶è·å–å“åº”
3. æå–æ¨ç†é“¾ï¼ˆå¦‚æœæ”¯æŒï¼‰
4. æ ¼å¼åŒ–å“åº”å’Œå¼•ç”¨æ¥æº
5. è¿”å›æŸ¥è¯¢ç»“æœ

ç‰¹æ€§ï¼š
- å¸¦å¼•ç”¨æº¯æºçš„æŸ¥è¯¢
- æ”¯æŒæ¨ç†é“¾æ˜¾ç¤º
- å®Œæ•´çš„å“åº”æ ¼å¼åŒ–
- æ”¯æŒè°ƒè¯•å’Œè¿½è¸ª
"""

import time
from typing import List, Optional, Tuple, Dict, Any

from llama_index.core import VectorStoreIndex, Settings, PromptTemplate
from llama_index.core.query_engine import CitationQueryEngine
from llama_index.core.base.response.schema import Response
from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler

from src.infrastructure.config import config, get_gpu_device
from src.infrastructure.indexer import IndexManager
from src.infrastructure.logger import get_logger
from src.business.rag_engine.formatting import ResponseFormatter
from src.business.rag_engine.formatting.templates import SIMPLE_MARKDOWN_TEMPLATE
from src.business.rag_engine.utils.utils import extract_sources_from_response
from src.infrastructure.llms import create_deepseek_llm_for_query, extract_reasoning_content

logger = get_logger('rag_engine')


class QueryEngine:
    """æŸ¥è¯¢å¼•æ“ï¼ˆé—ç•™å®ç°ï¼‰"""
    
    def __init__(
        self,
        index_manager: IndexManager,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        model: Optional[str] = None,
        similarity_top_k: Optional[int] = None,
        citation_chunk_size: int = 512,
        enable_debug: bool = False,
        similarity_threshold: Optional[float] = None,
        enable_markdown_formatting: bool = True,
    ):
        """åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“
        
        Args:
            index_manager: ç´¢å¼•ç®¡ç†å™¨
            api_key: DeepSeek APIå¯†é’¥
            api_base: APIç«¯ç‚¹
            model: æ¨¡å‹åç§°
            similarity_top_k: æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£æ•°é‡
            citation_chunk_size: å¼•ç”¨å—å¤§å°
            enable_debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            enable_markdown_formatting: æ˜¯å¦å¯ç”¨Markdownæ ¼å¼åŒ–
        """
        self.index_manager = index_manager
        self.similarity_top_k = similarity_top_k or config.SIMILARITY_TOP_K
        self.citation_chunk_size = citation_chunk_size
        self.enable_debug = enable_debug
        self.similarity_threshold = similarity_threshold or config.SIMILARITY_THRESHOLD
        
        # åˆå§‹åŒ–å“åº”æ ¼å¼åŒ–å™¨
        self.formatter = ResponseFormatter(enable_formatting=enable_markdown_formatting)
        logger.info(f"å“åº”æ ¼å¼åŒ–å™¨å·²{'å¯ç”¨' if enable_markdown_formatting else 'ç¦ç”¨'}")
        
        # é…ç½®DeepSeek LLM
        self.api_key = api_key or config.DEEPSEEK_API_KEY
        self.model = model or config.LLM_MODEL
        
        if not self.api_key:
            raise ValueError("æœªè®¾ç½®DEEPSEEK_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶")
        
        # é…ç½®è°ƒè¯•æ¨¡å¼
        if self.enable_debug:
            logger.info("ğŸ” å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆLlamaDebugHandlerï¼‰")
            self.llama_debug = LlamaDebugHandler(print_trace_on_end=True)
            Settings.callback_manager = CallbackManager([self.llama_debug])
        
        logger.info(f"ğŸ¤– åˆå§‹åŒ–DeepSeek LLM: {self.model}")
        self.llm = create_deepseek_llm_for_query(
            api_key=self.api_key,
            model=self.model,
            max_tokens=4096,
        )
        
        # è·å–ç´¢å¼•
        self.index = self.index_manager.get_index()
        
        # åˆ›å»º Markdown Prompt æ¨¡æ¿ï¼ˆå¦‚æœå¯ç”¨æ ¼å¼åŒ–ï¼‰
        markdown_template = None
        if enable_markdown_formatting:
            markdown_template = PromptTemplate(SIMPLE_MARKDOWN_TEMPLATE)
            logger.info("ğŸ“ å¯ç”¨ Markdown æ ¼å¼åŒ– Prompt")
        
        # åˆ›å»ºå¸¦å¼•ç”¨çš„æŸ¥è¯¢å¼•æ“
        logger.info("ğŸ“ åˆ›å»ºå¼•ç”¨æŸ¥è¯¢å¼•æ“")
        query_engine_kwargs = {
            'llm': self.llm,
            'similarity_top_k': self.similarity_top_k,
            'citation_chunk_size': self.citation_chunk_size,
        }
        
        if markdown_template is not None:
            query_engine_kwargs['text_qa_template'] = markdown_template
        
        self.query_engine = CitationQueryEngine.from_args(
            self.index,
            **query_engine_kwargs
        )
        
        logger.info("âœ… æŸ¥è¯¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def query(self, question: str, collect_trace: bool = False) -> Tuple[str, List[dict], Optional[str], Optional[Dict[str, Any]]]:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›å¸¦å¼•ç”¨çš„ç­”æ¡ˆ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            collect_trace: æ˜¯å¦æ”¶é›†è¯¦ç»†çš„è¿½è¸ªä¿¡æ¯
            
        Returns:
            (ç­”æ¡ˆæ–‡æœ¬, å¼•ç”¨æ¥æºåˆ—è¡¨, æ¨ç†é“¾å†…å®¹, è¿½è¸ªä¿¡æ¯å­—å…¸)
        """
        from src.business.rag_engine.utils.utils import collect_trace_info, handle_fallback
        
        trace_info = None
        
        try:
            device = get_gpu_device()
            device_mode = "GPUåŠ é€Ÿ" if device.startswith("cuda") else "CPUæ¨¡å¼"
            
            logger.info(f"ğŸ’¬ æŸ¥è¯¢: {question}")
            logger.debug(f"æŸ¥è¯¢è®¾å¤‡: {device} ({device_mode})")
            
            if collect_trace:
                trace_info = {
                    "query": question,
                    "start_time": time.time(),
                    "retrieval": {},
                    "llm_generation": {}
                }
            
            # æ‰§è¡Œæ£€ç´¢
            retrieval_start = time.time()
            
            # è·å– Collection ç»Ÿè®¡ä¿¡æ¯
            stats = self.index_manager.get_stats()
            collection_total_docs = stats.get('document_count', 0)
            collection_name = stats.get('collection_name', 'unknown')
            
            if 'error' in stats:
                error_info = stats.get('error', 'æœªçŸ¥é”™è¯¯')
                logger.warning(f"âš ï¸  è·å–Collectionç»Ÿè®¡ä¿¡æ¯æ—¶å‡ºç°é—®é¢˜: {error_info}")
            
            logger.info(f"ğŸ“Š Collection: {collection_name}, æ€»æ–‡æ¡£æ•°: {collection_total_docs}")
            
            if collection_total_docs == 0:
                logger.warning(f"âš ï¸  **é‡è¦æç¤º**: Collection '{collection_name}' çš„æ–‡æ¡£æ•°ä¸º0")
                logger.warning(f"   è¯·å‰å¾€ 'è®¾ç½®é¡µé¢ > æ•°æ®æºç®¡ç†' é‡æ–°å¯¼å…¥æ•°æ®")
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response: Response = self.query_engine.query(question)
            retrieval_time = time.time() - retrieval_start
            
            # æå–æ¨ç†é“¾å†…å®¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            reasoning_content = extract_reasoning_content(response)
            
            # æå–ç­”æ¡ˆ
            answer = str(response)
            answer = self.formatter.format(answer, None)
            
            # æå–å¼•ç”¨æ¥æº
            sources = extract_sources_from_response(response)
            
            # å¤„ç†å…œåº•é€»è¾‘ï¼ˆä½¿ç”¨å…±äº«å‡½æ•°ï¼Œä¸æ–°å¼•æ“é€»è¾‘ç»Ÿä¸€ï¼‰
            # ä½¿ç”¨ SIMILARITY_CUTOFF ç¡®ä¿ä¸æ–°å¼•æ“ä½¿ç”¨ç›¸åŒçš„é˜ˆå€¼é…ç½®
            similarity_cutoff = config.SIMILARITY_CUTOFF
            answer, fallback_reason = handle_fallback(
                answer, sources, question, self.llm, similarity_cutoff
            )
            
            # æ”¶é›†è¿½è¸ªä¿¡æ¯
            if collect_trace and trace_info:
                trace_info = collect_trace_info(
                    trace_info, retrieval_time, sources, self.similarity_top_k,
                    similarity_cutoff, self.model, answer, fallback_reason
                )
                if reasoning_content:
                    trace_info["has_reasoning"] = True
                    trace_info["reasoning_length"] = len(reasoning_content)
            
            logger.info(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(sources)} ä¸ªå¼•ç”¨æ¥æº")
            if reasoning_content:
                logger.debug(f"ğŸ§  æ¨ç†é“¾å†…å®¹å·²æå–ï¼ˆé•¿åº¦: {len(reasoning_content)} å­—ç¬¦ï¼‰")
            
            return answer, sources, reasoning_content, trace_info
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    async def stream_query(self, question: str):
        """å¼‚æ­¥æµå¼æŸ¥è¯¢ï¼ˆæš‚æœªå®ç°ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Yields:
            dict: åŒ…å«typeå’Œdataçš„å­—å…¸
        """
        raise NotImplementedError("æµå¼æŸ¥è¯¢æš‚æœªå®ç°")
    
    def get_retriever(self):
        """è·å–æ£€ç´¢å™¨ï¼ˆç”¨äºé«˜çº§ç”¨æ³•ï¼‰"""
        return self.index.as_retriever(similarity_top_k=self.similarity_top_k)
