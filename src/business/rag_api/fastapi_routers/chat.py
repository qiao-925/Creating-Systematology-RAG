"""
RAG API - FastAPIå¯¹è¯è·¯ç”±

æç®€è®¾è®¡ï¼šåªæä¾›ä¸¤ä¸ªæ ¸å¿ƒæ¥å£
- æµå¼å¯¹è¯ï¼ˆè‡ªåŠ¨åˆ›å»º/ä½¿ç”¨ä¼šè¯ï¼‰
- è·å–ä¼šè¯å†å²
"""

import asyncio
import json
import time
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse

from src.business.rag_api.fastapi_dependencies import get_rag_service
from src.business.rag_api.rag_service import RAGService
from src.business.rag_api.models import (
    ChatRequest,
    SessionHistoryResponse,
)
from src.infrastructure.logger import get_logger
from src.infrastructure.llms import create_deepseek_llm_for_query
from src.infrastructure.llms.reasoning import extract_reasoning_from_stream_chunk, extract_reasoning_content
from src.business.rag_engine.formatting.templates import CHAT_MARKDOWN_TEMPLATE
from src.business.rag_engine.retrieval.factory import create_retriever
from llama_index.core.llms import ChatMessage, MessageRole
from src.infrastructure.config import config

# å…¨å±€å˜é‡ï¼šæ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè®°å½•å¯¹è¯è·¯ç”±ç›¸å…³çš„æ—¥å¿—ä¿¡æ¯
logger = get_logger('rag_api_chat_router')

# å…¨å±€å˜é‡ï¼šFastAPI è·¯ç”±å™¨ï¼Œå®šä¹‰å¯¹è¯ç›¸å…³çš„ API è·¯ç”±
router = APIRouter(prefix="/chat", tags=["å¯¹è¯"])


def _retrieve_nodes_and_sources(
    query: str,  # ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
    index_manager,  # ç´¢å¼•ç®¡ç†å™¨ï¼Œç”¨äºè·å–å‘é‡ç´¢å¼•
    query_engine,  # æŸ¥è¯¢å¼•æ“ï¼Œå¯èƒ½åŒ…å«åå¤„ç†å™¨
) -> tuple[list, list]:
    """æ£€ç´¢èŠ‚ç‚¹å¹¶è½¬æ¢ä¸ºæ¥æºæ ¼å¼
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
        index_manager: ç´¢å¼•ç®¡ç†å™¨å®ä¾‹ï¼Œç”¨äºè·å–å‘é‡ç´¢å¼•
        query_engine: æŸ¥è¯¢å¼•æ“å®ä¾‹ï¼Œå¯èƒ½åŒ…å«åå¤„ç†å™¨ç”¨äºä¼˜åŒ–æ£€ç´¢ç»“æœ
    
    Returns:
        tuple: (nodes_with_scores, sources)
            - nodes_with_scores: æ£€ç´¢åˆ°çš„èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå¸¦ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
            - sources: è½¬æ¢åçš„æ¥æºä¿¡æ¯åˆ—è¡¨ï¼Œç”¨äºå‰ç«¯å±•ç¤º
    """
    nodes_with_scores = []  # æ£€ç´¢åˆ°çš„èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå¸¦ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
    sources = []  # è½¬æ¢åçš„æ¥æºä¿¡æ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸ºå­—å…¸åˆ—è¡¨
    
    if not index_manager:
        return nodes_with_scores, sources  # æ— ç´¢å¼•ç®¡ç†å™¨ï¼Œä½¿ç”¨çº¯ LLM æ¨¡å¼
    
    try:
        index = index_manager.get_index()  # ä»ç´¢å¼•ç®¡ç†å™¨è·å–å‘é‡ç´¢å¼•
        retriever = create_retriever(  # åˆ›å»ºæ£€ç´¢å™¨å®ä¾‹
            index=index,
            retrieval_strategy=config.RETRIEVAL_STRATEGY,
            similarity_top_k=config.SIMILARITY_TOP_K
        )
        
        nodes_with_scores = retriever.retrieve(query)  # æ‰§è¡Œæ£€ç´¢
        
        # åº”ç”¨åå¤„ç†ï¼ˆä¼˜åŒ–ã€å»é‡ç­‰ï¼‰
        if hasattr(query_engine, 'postprocessors') and query_engine.postprocessors:
            for postprocessor in query_engine.postprocessors:
                nodes_with_scores = postprocessor.postprocess_nodes(
                    nodes_with_scores,
                    query_str=query
                )
        
        # è½¬æ¢ä¸ºå¼•ç”¨æ¥æºæ ¼å¼
        for i, node_with_score in enumerate(nodes_with_scores, 1):
            node = node_with_score.node if hasattr(node_with_score, 'node') else node_with_score
            score = node_with_score.score if hasattr(node_with_score, 'score') else None
            
            source = {
                'index': i,  # æ¥æºåºå·
                'text': node.text if hasattr(node, 'text') else str(node),  # èŠ‚ç‚¹æ–‡æœ¬
                'score': score,  # ç›¸ä¼¼åº¦åˆ†æ•°
                'metadata': node.metadata if hasattr(node, 'metadata') else {},  # å…ƒæ•°æ®
            }
            sources.append(source)
        
        logger.info(f"æ£€ç´¢åˆ° {len(nodes_with_scores)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
    except Exception as e:
        logger.warning(f"æ£€ç´¢å¤±è´¥ï¼Œä½¿ç”¨çº¯ LLM æ¨¡å¼: {e}")
    
    return nodes_with_scores, sources


def _build_prompt(query: str, nodes_with_scores: list) -> str:
    """æ„å»º prompt
    
    Args:
        query: ç”¨æˆ·æŸ¥è¯¢æ–‡æœ¬
        nodes_with_scores: æ£€ç´¢åˆ°çš„èŠ‚ç‚¹åˆ—è¡¨ï¼ˆå¸¦ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
    
    Returns:
        str: æ„å»ºå®Œæˆçš„ prompt æ–‡æœ¬ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡å’Œç”¨æˆ·é—®é¢˜
    """
    if nodes_with_scores:
        context_parts = []  # æ ¼å¼åŒ–åçš„ä¸Šä¸‹æ–‡ç‰‡æ®µåˆ—è¡¨
        for i, node_with_score in enumerate(nodes_with_scores, 1):
            node = node_with_score.node if hasattr(node_with_score, 'node') else node_with_score
            text = node.text if hasattr(node, 'text') else str(node)
            context_parts.append(f"[{i}] {text}")
        context_str = "\n\n".join(context_parts)  # ç”¨åŒæ¢è¡Œç¬¦è¿æ¥
    else:
        context_str = "ï¼ˆçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼‰"
    
    prompt = CHAT_MARKDOWN_TEMPLATE.format(context_str=context_str)
    prompt += f"\n\nç”¨æˆ·é—®é¢˜ï¼š{query}\n\nè¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚"
    return prompt


def _extract_token_from_chunk(chunk, full_answer: str) -> str:
    """ä» chunk æå–å¢é‡ token
    
    Args:
        chunk: LLM è¿”å›çš„æµå¼å“åº” chunk å¯¹è±¡
        full_answer: å½“å‰å·²ç´¯ç§¯çš„å®Œæ•´ç­”æ¡ˆæ–‡æœ¬ï¼Œç”¨äºè®¡ç®—å¢é‡
    
    Returns:
        str: æå–åˆ°çš„å¢é‡ token æ–‡æœ¬ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å› None
    """
    # ä¼˜å…ˆä½¿ç”¨ delta.contentï¼ˆå¢é‡ï¼Œç›´æ¥å¯ç”¨ï¼‰
    if hasattr(chunk, 'delta') and hasattr(chunk.delta, 'content') and chunk.delta.content:
        return str(chunk.delta.content)
    
    # ä» message.content è®¡ç®—å¢é‡ï¼ˆmessage.content æ˜¯ç´¯åŠ çš„ï¼‰
    if hasattr(chunk, 'message') and hasattr(chunk.message, 'content') and chunk.message.content:
        current_content = str(chunk.message.content)  # å½“å‰ç´¯ç§¯çš„å®Œæ•´å†…å®¹
        if full_answer and current_content.startswith(full_answer):
            chunk_text = current_content[len(full_answer):]  # è®¡ç®—å¢é‡éƒ¨åˆ†
            return chunk_text if chunk_text else None
        else:
            return current_content  # ç¬¬ä¸€æ¬¡æˆ–å¼‚å¸¸æƒ…å†µ
    
    return None


def _format_answer(full_answer: str, sources: list, query_engine) -> str:
    """æ ¼å¼åŒ–æœ€ç»ˆç­”æ¡ˆ
    
    Args:
        full_answer: å®Œæ•´çš„ç­”æ¡ˆæ–‡æœ¬ï¼ˆæœªæ ¼å¼åŒ–ï¼‰
        sources: å¼•ç”¨æ¥æºåˆ—è¡¨
        query_engine: æŸ¥è¯¢å¼•æ“å®ä¾‹ï¼Œå¯èƒ½åŒ…å«æ ¼å¼åŒ–å™¨
    
    Returns:
        str: æ ¼å¼åŒ–åçš„ç­”æ¡ˆæ–‡æœ¬
    """
    formatted_answer = full_answer  # æ ¼å¼åŒ–åçš„ç­”æ¡ˆï¼Œé»˜è®¤ä¸ºåŸå§‹ç­”æ¡ˆ
    if query_engine and hasattr(query_engine, 'formatter'):
        try:
            formatted_answer = query_engine.formatter.format(full_answer, sources)
            logger.debug("ç­”æ¡ˆæ ¼å¼åŒ–å®Œæˆ")
        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç­”æ¡ˆ: {e}")
            formatted_answer = full_answer
    return formatted_answer


async def _generate_stream(
    request: ChatRequest,  # ç”¨æˆ·è¯·æ±‚å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å’Œä¼šè¯ID
    rag_service: RAGService,  # RAG æœåŠ¡å®ä¾‹ï¼Œæä¾›ç´¢å¼•ç®¡ç†å’ŒæŸ¥è¯¢å¼•æ“
):
    """ç”Ÿæˆ SSE æµçš„ä¸»æ–¹æ³•
    
    è¿™æ˜¯æµå¼å¯¹è¯çš„æ ¸å¿ƒæ–¹æ³•ï¼Œè´Ÿè´£ï¼š
    1. æ£€ç´¢ç›¸å…³æ–‡æ¡£èŠ‚ç‚¹
    2. æ„å»º prompt
    3. è°ƒç”¨ LLM è¿›è¡Œæµå¼ç”Ÿæˆ
    4. æå–å¹¶ yield token
    5. æ ¼å¼åŒ–ç­”æ¡ˆå¹¶è¿”å›æœ€ç»ˆç»“æœ
    
    Args:
        request: ç”¨æˆ·è¯·æ±‚å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹å’Œä¼šè¯ID
        rag_service: RAG æœåŠ¡å®ä¾‹ï¼Œæä¾›ç´¢å¼•ç®¡ç†å’ŒæŸ¥è¯¢å¼•æ“
    
    Yields:
        str: SSE æ ¼å¼çš„æ•°æ®æµï¼ŒåŒ…å« tokenã€sourcesã€reasoningã€done ç­‰äº‹ä»¶
    """
    try:
        # Step 1: è·å–å¿…è¦çš„ç»„ä»¶
        index_manager = rag_service.index_manager  # ç´¢å¼•ç®¡ç†å™¨
        query_engine = rag_service.modular_query_engine  # æŸ¥è¯¢å¼•æ“
        
        # Step 2: æ£€ç´¢èŠ‚ç‚¹å’Œæ¥æº
        nodes_with_scores, sources = _retrieve_nodes_and_sources(
            request.message,
            index_manager,
            query_engine
        )
        
        # Step 3: æ„å»º prompt
        prompt = _build_prompt(request.message, nodes_with_scores)
        
        # Step 4: æµå¼å¤„ç† LLM å“åº”
        llm = create_deepseek_llm_for_query()  # åˆ›å»º DeepSeek LLM å®ä¾‹
        chat_message = ChatMessage(role=MessageRole.USER, content=prompt)
        messages = [chat_message]  # æ¶ˆæ¯åˆ—è¡¨
        
        full_answer = ""  # ç´¯ç§¯çš„å®Œæ•´ç­”æ¡ˆæ–‡æœ¬
        reasoning_content = ""  # ç´¯ç§¯çš„æ¨ç†é“¾å†…å®¹
        token_count = 0  # å·²å¤„ç†çš„ token æ•°é‡
        last_chunk = None  # æœ€åä¸€ä¸ª chunkï¼Œç”¨äºæå–æœ€ç»ˆæ¨ç†é“¾
        
        logger.debug("ğŸš€ å¼€å§‹ç›´æ¥æµå¼è°ƒç”¨ DeepSeek APIï¼ˆç»•è¿‡ä¸­é—´å±‚ï¼‰")
        
        for chunk in llm.stream_chat(messages):
            # æå–æ¨ç†é“¾å†…å®¹ï¼ˆæµå¼ï¼‰
            chunk_reasoning = extract_reasoning_from_stream_chunk(chunk)
            if chunk_reasoning:
                reasoning_content += chunk_reasoning
            
            # æå–å¢é‡ token
            chunk_text = _extract_token_from_chunk(chunk, full_answer)
            
            if chunk_text:
                token_count += 1
                full_answer += chunk_text
                yield f"data: {json.dumps({'type': 'token', 'data': chunk_text}, ensure_ascii=False)}\n\n"
            
            last_chunk = chunk
        
        # æå–æœ€ç»ˆæ¨ç†é“¾ï¼ˆä»æœ€åä¸€ä¸ª chunkï¼‰
        if last_chunk:
            final_reasoning = extract_reasoning_content(last_chunk)
            if final_reasoning:
                reasoning_content = final_reasoning
        
        logger.debug(f"âœ… ç›´æ¥æµå¼ç”Ÿæˆå®Œæˆï¼Œå…± {token_count} ä¸ª token")
        
        # Step 5: æ ¼å¼åŒ–ç­”æ¡ˆ
        formatted_answer = _format_answer(full_answer, sources, query_engine)
        
        # Step 6-8: è¿”å›å¼•ç”¨æ¥æºã€æ¨ç†é“¾å’Œå®Œæˆäº‹ä»¶
        if sources:
            yield f"data: {json.dumps({'type': 'sources', 'data': sources}, ensure_ascii=False)}\n\n"
        if reasoning_content:
            yield f"data: {json.dumps({'type': 'reasoning', 'data': reasoning_content}, ensure_ascii=False)}\n\n"
        yield f"data: {json.dumps({'type': 'done', 'data': {'answer': formatted_answer, 'sources': sources, 'reasoning_content': reasoning_content if reasoning_content else None}}, ensure_ascii=False)}\n\n"
    
    except Exception as e:
        logger.error("ç›´æ¥æµå¼å¯¹è¯å¤±è´¥", error=str(e), exc_info=True)
        error_chunk = {"type": "error", "data": {"message": str(e)}}
        data = json.dumps(error_chunk, ensure_ascii=False)
        yield f"data: {data}\n\n"


@router.post("/stream")
async def stream_chat(
    request: ChatRequest,  # ç”¨æˆ·è¯·æ±‚å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹å’Œä¼šè¯ID
    rag_service: RAGService = Depends(get_rag_service),  # RAG æœåŠ¡ä¾èµ–æ³¨å…¥
):
    """æµå¼å¯¹è¯æ¥å£ - ç›´æ¥æµå¼ç®¡é“ç‰ˆæœ¬
    
    ç»•è¿‡ä¸­é—´å±‚ï¼Œç›´æ¥åœ¨ FastAPI å±‚å»ºç«‹ä¸ DeepSeek çš„æµå¼ç®¡é“ï¼š
    1. æ£€ç´¢èŠ‚ç‚¹ï¼ˆå¦‚æœéœ€è¦ RAGï¼‰
    2. æ„å»º prompt
    3. ç›´æ¥è°ƒç”¨ DeepSeek stream_chat
    4. ä» raw å“åº”ä¸­æå–å¢é‡ token
    5. ç«‹å³ yield ç»™å‰ç«¯
    
    - å¦‚æœæä¾›äº† session_idï¼Œä½¿ç”¨è¯¥ä¼šè¯
    - å¦‚æœæ²¡æœ‰æä¾› session_idï¼Œè‡ªåŠ¨åˆ›å»ºæ–°ä¼šè¯
    - æµå¼è¿”å›ç­”æ¡ˆï¼ŒåŒ…å« tokenã€sources å’Œ done äº‹ä»¶
    
    Args:
        request: ç”¨æˆ·è¯·æ±‚å¯¹è±¡ï¼ŒåŒ…å«æ¶ˆæ¯å†…å®¹å’Œå¯é€‰çš„ä¼šè¯ID
        rag_service: RAG æœåŠ¡å®ä¾‹ï¼Œé€šè¿‡ä¾èµ–æ³¨å…¥è·å–
    
    Returns:
        StreamingResponse: SSE æ ¼å¼çš„æµå¼å“åº”
    """
    logger.info(
        "æ”¶åˆ°æµå¼å¯¹è¯è¯·æ±‚ï¼ˆç›´æ¥æµå¼ç®¡é“ï¼‰",
        message=request.message[:50] if len(request.message) > 50 else request.message,
        session_id=request.session_id
    )
    
    # è¿”å› SSE æ ¼å¼çš„æµå¼å“åº”
    return StreamingResponse(
        _generate_stream(request, rag_service),  # ç”Ÿæˆ SSE æµçš„å¼‚æ­¥ç”Ÿæˆå™¨
        media_type="text/event-stream",  # SSE åª’ä½“ç±»å‹
        headers={
            "Cache-Control": "no-cache",  # ç¦ç”¨ç¼“å­˜
            "Connection": "keep-alive",  # ä¿æŒè¿æ¥
            "X-Accel-Buffering": "no",  # ç¦ç”¨ Nginx ç¼“å†²
        }
    )


@router.get("/sessions/{session_id}/history", response_model=SessionHistoryResponse)
async def get_session_history(
    session_id: str,  # ä¼šè¯IDï¼Œä» URL è·¯å¾„å‚æ•°è·å–
    rag_service: RAGService = Depends(get_rag_service),  # RAG æœåŠ¡ä¾èµ–æ³¨å…¥
):
    """è·å–æŒ‡å®šä¼šè¯çš„å†å²è®°å½•
    
    Args:
        session_id: ä¼šè¯IDï¼Œä» URL è·¯å¾„å‚æ•°è·å–
        rag_service: RAG æœåŠ¡å®ä¾‹ï¼Œé€šè¿‡ä¾èµ–æ³¨å…¥è·å–
    
    Returns:
        SessionHistoryResponse: ä¼šè¯å†å²è®°å½•å“åº”å¯¹è±¡
    
    Raises:
        HTTPException: 
            - 404: ä¼šè¯ä¸å­˜åœ¨
            - 500: è·å–ä¼šè¯å†å²å¤±è´¥
    """
    logger.info("è·å–ä¼šè¯å†å²", session_id=session_id)
    try:
        return await asyncio.to_thread(rag_service.get_session_history, session_id)
    except FileNotFoundError as e:
        logger.warning("ä¼šè¯ä¸å­˜åœ¨", session_id=session_id, error=str(e))
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail={"code": "SESSION_NOT_FOUND", "message": str(e)}
        )
    except Exception as e:
        logger.error("è·å–ä¼šè¯å†å²å¤±è´¥", session_id=session_id, error=str(e), exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail={"code": "HISTORY_FETCH_FAILED", "message": f"è·å–ä¼šè¯å†å²å¤±è´¥: {str(e)}"}
        )
