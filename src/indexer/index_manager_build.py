"""
ç´¢å¼•ç®¡ç†å™¨æ„å»ºæ–¹æ³•æ¨¡å—
åŒ…å«build_indexæ–¹æ³•çš„å®ç°
"""

import time
from typing import List, Optional, Tuple, Dict

from llama_index.core.schema import Document as LlamaDocument

from src.config import config, get_gpu_device
from src.logger import setup_logger
from src.indexer.index_builder import build_index_batch_mode, build_index_normal_mode
from src.indexer.index_vector_ids import get_vector_ids_batch
from src.indexer.index_utils import (
    filter_vectorized_documents,
    compute_documents_hash
)

logger = setup_logger('indexer')


def build_index_method(
    index_manager,
    documents: List[LlamaDocument],
    show_progress: bool = True,
    cache_manager=None,
    task_id: Optional[str] = None
) -> Tuple:
    """æ„å»ºæˆ–æ›´æ–°ç´¢å¼•ï¼ˆIndexManagerçš„build_indexæ–¹æ³•å®ç°ï¼‰"""
    start_time = time.time()
    
    if not documents:
        print("âš ï¸  æ²¡æœ‰æ–‡æ¡£å¯ç´¢å¼•")
        return index_manager.get_index(), {}
    
    # æ–‡æ¡£çº§æ–­ç‚¹ç»­ä¼ 
    documents_to_process, already_vectorized = filter_vectorized_documents(index_manager, documents)
    
    if already_vectorized > 0:
        logger.info(f"âœ… æ£€æµ‹åˆ° {already_vectorized} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å¤„ç†")
        print(f"ğŸ“Š æ–­ç‚¹ç»­ä¼ : {already_vectorized}/{len(documents)} ä¸ªæ–‡æ¡£å·²å‘é‡åŒ–ï¼Œå‰©ä½™ {len(documents_to_process)} ä¸ªå¾…å¤„ç†")
    
    if not documents_to_process:
        logger.info(f"âœ… æ‰€æœ‰æ–‡æ¡£å·²å‘é‡åŒ–ï¼Œè·³è¿‡å‘é‡åŒ–æ­¥éª¤")
        index = index_manager.get_index()
        vector_ids_map = get_vector_ids_batch(
            index_manager,
            [doc.metadata.get("file_path", "") for doc in documents 
             if doc.metadata.get("file_path")]
        )
        
        if cache_manager and task_id and config.ENABLE_CACHE:
            docs_hash = compute_documents_hash(documents)
            cache_manager.mark_step_completed(
                task_id=task_id,
                step_name=cache_manager.STEP_VECTORIZE,
                input_hash=docs_hash,
                vector_count=index_manager.chroma_collection.count() if hasattr(index_manager, 'chroma_collection') else 0,
                collection_name=index_manager.collection_name
            )
        
        return index, vector_ids_map
    
    documents = documents_to_process
    device = get_gpu_device()
    
    print(f"\nğŸ”¨ å¼€å§‹æ„å»ºç´¢å¼•ï¼Œå…± {len(documents)} ä¸ªæ–‡æ¡£")
    print(f"   åˆ†å—å‚æ•°: size={index_manager.chunk_size}, overlap={index_manager.chunk_overlap}")
    
    if device.startswith("cuda"):
        import torch
        device_name = torch.cuda.get_device_name()
        print(f"ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} âš¡ GPUåŠ é€Ÿæ¨¡å¼")
        print(f"   GPU: {device_name}")
        logger.info(f"ğŸ“Š ç´¢å¼•æ„å»ºä½¿ç”¨GPU: {device_name} ({device})")
    else:
        print(f"ğŸ“Š ç´¢å¼•æ„å»ºè®¾å¤‡: {device} ğŸŒ CPUæ¨¡å¼")
        logger.warning(f"ğŸ“Š ç´¢å¼•æ„å»ºä½¿ç”¨CPUï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
    
    try:
        if config.INDEX_BATCH_MODE:
            index, _ = build_index_batch_mode(index_manager, documents, show_progress)
        else:
            index, _ = build_index_normal_mode(index_manager, documents, show_progress)
        
        # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯
        stats = index_manager.get_stats()
        total_elapsed = time.time() - start_time
        
        print(f"ğŸ“Š ç´¢å¼•ç»Ÿè®¡: {stats}")
        logger.info(
            f"ç´¢å¼•æ„å»ºå®Œæˆ: "
            f"æ–‡æ¡£æ•°={len(documents)}, "
            f"å‘é‡æ•°={stats.get('document_count', 0)}, "
            f"æ€»è€—æ—¶={total_elapsed:.2f}s"
        )
        
        # æ„å»ºå‘é‡IDæ˜ å°„
        vector_ids_map = get_vector_ids_batch(
            index_manager,
            [doc.metadata.get("file_path", "") for doc in documents 
             if doc.metadata.get("file_path")]
        )
        
        # å¦‚æœæä¾›äº†ç¼“å­˜ç®¡ç†å™¨ï¼Œæ›´æ–°ç¼“å­˜çŠ¶æ€
        if cache_manager and task_id and config.ENABLE_CACHE:
            try:
                docs_hash = compute_documents_hash(documents)
                vector_count = stats.get('document_count', 0)
                cache_manager.mark_step_completed(
                    task_id=task_id,
                    step_name=cache_manager.STEP_VECTORIZE,
                    input_hash=docs_hash,
                    vector_count=vector_count,
                    collection_name=index_manager.collection_name
                )
            except Exception as e:
                logger.warning(f"æ›´æ–°å‘é‡åŒ–ç¼“å­˜çŠ¶æ€å¤±è´¥: {e}")
        
        return index_manager._index, vector_ids_map
        
    except Exception as e:
        print(f"âŒ ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
        if cache_manager and task_id:
            try:
                cache_manager.mark_step_failed(
                    task_id=task_id,
                    step_name=cache_manager.STEP_VECTORIZE,
                    error_message=str(e)
                )
            except Exception:
                pass
        raise

