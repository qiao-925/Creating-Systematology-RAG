"""
Embeddingæ¨¡å‹åŠ è½½å’Œç®¡ç†å·¥å…·
"""

import os
from pathlib import Path
from typing import Optional

from llama_index.embeddings.huggingface import HuggingFaceEmbedding

from src.config import config, get_gpu_device, is_gpu_available
from src.logger import setup_logger

logger = setup_logger('indexer')

# å…¨å±€ embedding æ¨¡å‹ç¼“å­˜
_global_embed_model: Optional[HuggingFaceEmbedding] = None


def _setup_huggingface_env():
    """é…ç½® HuggingFace ç¯å¢ƒå˜é‡ï¼ˆé•œåƒå’Œç¦»çº¿æ¨¡å¼ï¼‰
    
    æ³¨æ„ï¼šç¯å¢ƒå˜é‡å·²åœ¨ src/__init__.py ä¸­é¢„è®¾ï¼Œè¿™é‡Œä»…ç”¨äºæ—¥å¿—è®°å½•å’Œç¡®è®¤
    """
    # è®¾ç½®é•œåƒåœ°å€
    if config.HF_ENDPOINT:
        os.environ['HF_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HUGGINGFACE_HUB_ENDPOINT'] = config.HF_ENDPOINT
        os.environ['HF_HUB_ENDPOINT'] = config.HF_ENDPOINT  # æ–°ç‰ˆæœ¬ä½¿ç”¨è¿™ä¸ª
        logger.info(f"ğŸŒ ä½¿ç”¨ HuggingFace é•œåƒ: {config.HF_ENDPOINT}")
    
    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    if config.HF_OFFLINE_MODE:
        os.environ['HF_HUB_OFFLINE'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '1'
        logger.info(f"ğŸ“´ å¯ç”¨ç¦»çº¿æ¨¡å¼ï¼ˆä»…ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")
    else:
        # ç¡®ä¿ç¦»çº¿æ¨¡å¼å…³é—­
        os.environ.pop('HF_HUB_OFFLINE', None)
        os.environ.pop('TRANSFORMERS_OFFLINE', None)


def load_embedding_model(model_name: Optional[str] = None, force_reload: bool = False) -> HuggingFaceEmbedding:
    """åŠ è½½ Embedding æ¨¡å‹ï¼ˆæ”¯æŒå…¨å±€å•ä¾‹æ¨¡å¼ï¼‰
    
    Args:
        model_name: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹
        force_reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼ˆå³ä½¿å·²ç¼“å­˜ï¼‰
        
    Returns:
        HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    
    model_name = model_name or config.EMBEDDING_MODEL
    
    # å¦‚æœå·²ç»åŠ è½½è¿‡ä¸”æ¨¡å‹åç§°ç›¸åŒï¼Œç›´æ¥è¿”å›ï¼ˆé™¤éå¼ºåˆ¶é‡æ–°åŠ è½½ï¼‰
    if _global_embed_model is not None and not force_reload:
        # æ£€æŸ¥ç¼“å­˜çš„æ¨¡å‹åç§°æ˜¯å¦ä¸æ–°é…ç½®ä¸€è‡´
        cached_model_name = getattr(_global_embed_model, 'model_name', None)
        if cached_model_name == model_name:
            logger.info(f"âœ… ä½¿ç”¨ç¼“å­˜çš„ Embedding æ¨¡å‹ï¼ˆå…¨å±€å˜é‡ï¼‰: {model_name}")
            logger.info(f"   æ¨¡å‹å¯¹è±¡ID: {id(_global_embed_model)}")
            return _global_embed_model
        else:
            # æ¨¡å‹åç§°ä¸ä¸€è‡´ï¼Œæ¸…é™¤ç¼“å­˜å¹¶é‡æ–°åŠ è½½
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹é…ç½®å˜æ›´: {cached_model_name} -> {model_name}")
            logger.info(f"   æ¸…é™¤æ—§æ¨¡å‹ç¼“å­˜ï¼Œé‡æ–°åŠ è½½æ–°æ¨¡å‹")
            _global_embed_model = None
    
    # å¦‚æœéœ€è¦å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œæ¸…é™¤ç¼“å­˜
    if force_reload:
        logger.info(f"ğŸ”„ å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å‹")
        _global_embed_model = None
    
    # é…ç½® HuggingFace ç¯å¢ƒå˜é‡
    _setup_huggingface_env()
    
    # åŠ è½½æ¨¡å‹
    logger.info(f"ğŸ“¦ å¼€å§‹åŠ è½½ Embedding æ¨¡å‹ï¼ˆå…¨æ–°åŠ è½½ï¼‰: {model_name}")
    
    try:
        # æ˜¾å¼æŒ‡å®šç¼“å­˜ç›®å½•ä»¥ç¡®ä¿ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        cache_folder = str(Path.home() / ".cache" / "huggingface")
        
        # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
        device = get_gpu_device()
        import torch
        
        # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
        if device.startswith("cuda") and is_gpu_available():
            device_name = torch.cuda.get_device_name()
            cuda_version = torch.version.cuda
            logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ:")
            logger.info(f"   è®¾å¤‡: {device}")
            logger.info(f"   GPUåç§°: {device_name}")
            logger.info(f"   CUDAç‰ˆæœ¬: {cuda_version}")
        else:
            logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
            logger.info("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
            logger.info("ğŸ’¡ å»ºè®®: å¦‚æœ‰GPUï¼Œè¯·å®‰è£…CUDAç‰ˆæœ¬çš„PyTorchä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        
        # æ„å»ºæ¨¡å‹å‚æ•°
        model_kwargs = {
            "trust_remote_code": True,
            "cache_folder": cache_folder,
        }
        
        _global_embed_model = HuggingFaceEmbedding(
            model_name=model_name,
            embed_batch_size=config.EMBED_BATCH_SIZE,  # å¯ç”¨æ‰¹å¤„ç†ï¼Œæå‡æ€§èƒ½
            max_length=config.EMBED_MAX_LENGTH,  # è®¾ç½®æœ€å¤§é•¿åº¦
            **model_kwargs
        )
        
        # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPUï¼ˆå¦‚æœä¸æ”¯æŒé€šè¿‡å‚æ•°æŒ‡å®šï¼‰
        try:
            if device.startswith("cuda") and is_gpu_available():
                # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                if hasattr(_global_embed_model, '_model') and hasattr(_global_embed_model._model, 'to'):
                    _global_embed_model._model = _global_embed_model._model.to(device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                elif hasattr(_global_embed_model, 'model') and hasattr(_global_embed_model.model, 'to'):
                    _global_embed_model.model = _global_embed_model.model.to(device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
            else:
                logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
            logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
        
        logger.info(f"âœ… Embedding æ¨¡å‹åŠ è½½å®Œæˆ: {model_name}")
        logger.info(f"ğŸ“ ç¼“å­˜ç›®å½•: {cache_folder}")
        if device.startswith("cuda"):
            logger.info(f"âš¡ GPUåŠ é€Ÿæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (æ¨è10-50)")
        else:
            logger.info(f"ğŸŒ CPUæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (å»ºè®®è°ƒæ•´ä¸º5-10)")
        logger.info(f"ğŸ“ æœ€å¤§é•¿åº¦: {config.EMBED_MAX_LENGTH}")
    except Exception as e:
        # å¦‚æœæ˜¯ç¦»çº¿æ¨¡å¼ä¸”ç¼ºå°‘ç¼“å­˜ï¼Œå°è¯•åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼
        if config.HF_OFFLINE_MODE and "offline" in str(e).lower():
            logger.warning(f"âš ï¸  ç¦»çº¿æ¨¡å¼ä¸‹æœ¬åœ°æ— ç¼“å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°åœ¨çº¿æ¨¡å¼å°è¯•ä¸‹è½½")
            os.environ.pop('HF_HUB_OFFLINE', None)
            
            try:
                cache_folder = str(Path.home() / ".cache" / "huggingface")
                
                # è·å–GPUè®¾å¤‡ï¼ˆä½¿ç”¨å¯åŠ¨æ—¶æ£€æµ‹çš„ç»“æœï¼‰
                device = get_gpu_device()
                import torch
                
                # è¾“å‡ºè¯¦ç»†çš„è®¾å¤‡ä¿¡æ¯
                if device.startswith("cuda") and is_gpu_available():
                    device_name = torch.cuda.get_device_name()
                    logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ: {device_name} ({device})")
                else:
                    logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
                    logger.info("ğŸ’¡ æ€§èƒ½æç¤º: CPUæ¨¡å¼è¾ƒæ…¢ï¼Œç´¢å¼•æ„å»ºå¯èƒ½éœ€è¦30åˆ†é’Ÿ+ï¼ˆGPUæ¨¡å¼ä¸‹çº¦5åˆ†é’Ÿï¼‰")
                
                # æ„å»ºæ¨¡å‹å‚æ•°
                model_kwargs = {
                    "trust_remote_code": True,
                    "cache_folder": cache_folder,
                }
                
                _global_embed_model = HuggingFaceEmbedding(
                    model_name=model_name,
                    embed_batch_size=config.EMBED_BATCH_SIZE,
                    max_length=config.EMBED_MAX_LENGTH,
                    **model_kwargs
                )
                
                # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPU
                try:
                    if device.startswith("cuda") and is_gpu_available():
                        # HuggingFaceEmbedding ä½¿ç”¨ _model å±æ€§
                        if hasattr(_global_embed_model, '_model') and hasattr(_global_embed_model._model, 'to'):
                            _global_embed_model._model = _global_embed_model._model.to(device)
                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                        elif hasattr(_global_embed_model, 'model') and hasattr(_global_embed_model.model, 'to'):
                            _global_embed_model.model = _global_embed_model.model.to(device)
                            logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ° GPU: {device}")
                    else:
                        logger.info(f"ğŸ“Œ æ¨¡å‹ä¿æŒåœ¨ CPU ä¸Š")
                except Exception as e:
                    logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
                    logger.info(f"ğŸ“Œ æ¨¡å‹å°†ä½¿ç”¨ CPU")
                
                logger.info(f"âœ… Embedding æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ: {model_name}")
                if device.startswith("cuda"):
                    logger.info(f"âš¡ GPUåŠ é€Ÿæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (æ¨è10-50)")
                else:
                    logger.info(f"ğŸŒ CPUæ¨¡å¼ - æ‰¹å¤„ç†å¤§å°: {config.EMBED_BATCH_SIZE} (å»ºè®®è°ƒæ•´ä¸º5-10)")
                logger.info(f"ğŸ“ æœ€å¤§é•¿åº¦: {config.EMBED_MAX_LENGTH}")
            except Exception as retry_error:
                logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {retry_error}")
                raise
        else:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    return _global_embed_model


def set_global_embed_model(model: HuggingFaceEmbedding):
    """è®¾ç½®å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Args:
        model: HuggingFaceEmbedding å®ä¾‹
    """
    global _global_embed_model
    _global_embed_model = model
    logger.debug("ğŸ”§ è®¾ç½®å…¨å±€ Embedding æ¨¡å‹")


def get_global_embed_model() -> Optional[HuggingFaceEmbedding]:
    """è·å–å…¨å±€ Embedding æ¨¡å‹å®ä¾‹
    
    Returns:
        å·²åŠ è½½çš„æ¨¡å‹å®ä¾‹ï¼Œå¦‚æœæœªåŠ è½½åˆ™è¿”å› None
    """
    return _global_embed_model


def clear_embedding_model_cache():
    """æ¸…é™¤å…¨å±€ Embedding æ¨¡å‹ç¼“å­˜
    
    ç”¨äºæ¨¡å‹åˆ‡æ¢æˆ–å¼ºåˆ¶é‡æ–°åŠ è½½åœºæ™¯
    """
    global _global_embed_model
    if _global_embed_model is not None:
        logger.info(f"ğŸ§¹ æ¸…é™¤ Embedding æ¨¡å‹ç¼“å­˜")
        _global_embed_model = None


def get_embedding_model_status() -> dict:
    """è·å– Embedding æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    
    Returns:
        åŒ…å«æ¨¡å‹çŠ¶æ€çš„å­—å…¸ï¼š
        {
            "loaded": bool,              # æ˜¯å¦å·²åŠ è½½
            "model_name": str,           # æ¨¡å‹åç§°
            "cache_dir": str,            # ç¼“å­˜ç›®å½•
            "cache_exists": bool,        # æœ¬åœ°ç¼“å­˜æ˜¯å¦å­˜åœ¨
            "offline_mode": bool,        # æ˜¯å¦ç¦»çº¿æ¨¡å¼
            "mirror": str,               # é•œåƒåœ°å€
        }
    """
    import os
    from pathlib import Path
    
    model_name = config.EMBEDDING_MODEL
    
    # æ£€æŸ¥ç¼“å­˜ç›®å½•
    cache_root = Path.home() / ".cache" / "huggingface" / "hub"
    # HuggingFace ç¼“å­˜æ ¼å¼: models--{org}--{model}
    model_cache_name = model_name.replace("/", "--")
    cache_dir = cache_root / f"models--{model_cache_name}"
    cache_exists = cache_dir.exists()
    
    return {
        "loaded": _global_embed_model is not None,
        "model_name": model_name,
        "cache_dir": str(cache_dir),
        "cache_exists": cache_exists,
        "offline_mode": config.HF_OFFLINE_MODE,
        "mirror": config.HF_ENDPOINT if config.HF_ENDPOINT else "huggingface.co (å®˜æ–¹)",
    }

