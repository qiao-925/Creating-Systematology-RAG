"""
ç´¢å¼•ç®¡ç†å™¨ä¸»ç±» - åˆå§‹åŒ–æ¨¡å—
IndexManageråˆå§‹åŒ–ç›¸å…³æ–¹æ³•
"""

import os
from pathlib import Path
from typing import Optional

import chromadb
from llama_index.core import Settings
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

from src.config import config, get_gpu_device, is_gpu_available
from src.logger import setup_logger
from src.embeddings.base import BaseEmbedding
from src.indexer.embedding_utils import (
    load_embedding_model,
    clear_embedding_model_cache,
    _setup_huggingface_env,
    get_global_embed_model
)
from src.indexer.index_core import print_database_info
from src.indexer.index_dimension import ensure_collection_dimension_match

logger = setup_logger('indexer')


def init_index_manager(
    collection_name: Optional[str],
    persist_dir: Optional[Path],  # ä¿ç•™å‚æ•°ç”¨äºå‘åå…¼å®¹ï¼ŒChroma Cloudæ¨¡å¼ä¸å†ä½¿ç”¨
    embedding_model_name: str,
    chunk_size: int,
    chunk_overlap: int,
    embed_model_instance: Optional[HuggingFaceEmbedding],
    embedding_instance: Optional[BaseEmbedding]
):
    """åˆå§‹åŒ–ç´¢å¼•ç®¡ç†å™¨çš„æ ¸å¿ƒç»„ä»¶
    
    Returns:
        tuple: (embed_model, chroma_client, chroma_collection)
    """
    # åˆå§‹åŒ–embeddingæ¨¡å‹
    # ä¼˜å…ˆä½¿ç”¨æ–°æ¶æ„çš„BaseEmbeddingå®ä¾‹ï¼Œæ”¯æŒå¯æ’æ‹”è®¾è®¡
    # æ–°æ¶æ„æä¾›äº†ç»Ÿä¸€çš„æ¥å£å’Œæ›´å¥½çš„æ‰©å±•æ€§ï¼ŒåŒæ—¶å…¼å®¹æ—§æ¥å£
    if embedding_instance is not None:
        logger.info(f"âœ… ä½¿ç”¨æä¾›çš„Embeddingå®ä¾‹: {embedding_instance}")
        # é€‚é…å™¨æ¨¡å¼ï¼šå¦‚æœå®ä¾‹æœ‰é€‚é…æ–¹æ³•ï¼Œè½¬æ¢ä¸ºllama_indexå…¼å®¹æ ¼å¼
        # å¦åˆ™ç›´æ¥ä½¿ç”¨ï¼ˆå¯èƒ½æ˜¯å·²å…¼å®¹çš„å®ä¾‹ï¼‰
        if hasattr(embedding_instance, 'get_llama_index_embedding'):
            embed_model = embedding_instance.get_llama_index_embedding()
        else:
            embed_model = embedding_instance
    elif embed_model_instance is not None:
        logger.info(f"âœ… ä½¿ç”¨é¢„åŠ è½½çš„Embeddingæ¨¡å‹ï¼ˆæ—§æ¥å£ï¼‰: {embedding_model_name}")
        embed_model = embed_model_instance
    else:
        # æ£€æŸ¥å…¨å±€ç¼“å­˜
        # Embeddingæ¨¡å‹åŠ è½½æˆæœ¬é«˜ï¼ˆæ•°GBå¤§å°ã€GPUå†…å­˜å ç”¨ï¼‰ï¼Œå…¨å±€ç¼“å­˜é¿å…é‡å¤åŠ è½½
        # å¤šä¸ªIndexManagerå®ä¾‹å…±äº«åŒä¸€ä¸ªæ¨¡å‹å®ä¾‹ï¼ŒèŠ‚çœå†…å­˜å’ŒåŠ è½½æ—¶é—´
        global_embed_model = get_global_embed_model()
        cached_model_name = None
        if global_embed_model is not None:
            cached_model_name = getattr(global_embed_model, 'model_name', None)
        
        # å¦‚æœé…ç½®çš„æ¨¡å‹åç§°ä¸ç¼“å­˜ä¸ä¸€è‡´ï¼Œå¿…é¡»æ¸…ç©ºç¼“å­˜
        # ä¸åŒæ¨¡å‹çš„å‘é‡ç»´åº¦ä¸åŒï¼Œæ··ç”¨ä¼šå¯¼è‡´ç´¢å¼•ç»´åº¦ä¸åŒ¹é…é”™è¯¯
        if cached_model_name and cached_model_name != embedding_model_name:
            logger.info(f"ğŸ”„ æ£€æµ‹åˆ°æ¨¡å‹é…ç½®å˜æ›´: {cached_model_name} -> {embedding_model_name}")
            clear_embedding_model_cache()
        
        # éªŒè¯ç¼“å­˜æ¨¡å‹æ˜¯å¦å¯ç”¨
        # æ¨¡å‹å¯èƒ½å·²è¢«é‡Šæ”¾æˆ–æŸåï¼Œéœ€è¦å®é™…è°ƒç”¨ä¸€æ¬¡ç¡®è®¤å¯ç”¨æ€§
        if global_embed_model is not None:
            try:
                test_embedding = global_embed_model.get_query_embedding("test")
                cached_dim = len(test_embedding)
                logger.info(f"âœ… ä½¿ç”¨å…¨å±€ç¼“å­˜çš„Embeddingæ¨¡å‹: {embedding_model_name} (ç»´åº¦: {cached_dim})")
                embed_model = global_embed_model
            except Exception as e:
                # ç¼“å­˜æ¨¡å‹ä¸å¯ç”¨æ—¶æ¸…ç©ºï¼Œé¿å…åç»­ç»§ç»­ä½¿ç”¨æŸåçš„æ¨¡å‹
                logger.warning(f"âš ï¸  éªŒè¯ç¼“å­˜æ¨¡å‹å¤±è´¥ï¼Œé‡æ–°åŠ è½½: {e}")
                clear_embedding_model_cache()
                embed_model = None
        else:
            embed_model = None
        
        # å¦‚æœç¼“å­˜ä¸å¯ç”¨ï¼ŒåŠ è½½æ–°æ¨¡å‹
        if embed_model is None:
            _setup_huggingface_env()
            logger.info(f"ğŸ“¦ æ­£åœ¨åŠ è½½Embeddingæ¨¡å‹: {embedding_model_name}")
            try:
                embed_model = load_embedding_model(
                    model_name=embedding_model_name,
                    force_reload=False
                )
            except Exception as e:
                logger.warning(f"âš ï¸  load_embedding_modelå¤±è´¥: {e}")
                embed_model = _load_embedding_model_fallback(embedding_model_name)
    
    # é…ç½®å…¨å±€Settings
    # llama_indexä½¿ç”¨å…¨å±€Settingså­˜å‚¨é»˜è®¤é…ç½®ï¼Œæ‰€æœ‰ç»„ä»¶éƒ½ä¼šä»ä¸­è¯»å–
    # å¿…é¡»è®¾ç½®è¿™äº›å€¼ï¼Œå¦åˆ™æ–‡æ¡£åˆ†å—å’Œå‘é‡åŒ–ä¼šä½¿ç”¨é»˜è®¤å€¼ï¼ˆå¯èƒ½ä¸ç¬¦åˆé¢„æœŸï¼‰
    Settings.embed_model = embed_model
    Settings.chunk_size = chunk_size
    Settings.chunk_overlap = chunk_overlap
    
    # åˆå§‹åŒ–Chroma Cloudå®¢æˆ·ç«¯
    logger.info("ğŸ—„ï¸  åˆå§‹åŒ–Chroma Cloudå‘é‡æ•°æ®åº“")
    from src.config import config
    
    if not config.CHROMA_CLOUD_API_KEY or not config.CHROMA_CLOUD_TENANT or not config.CHROMA_CLOUD_DATABASE:
        raise ValueError(
            "Chroma Cloudé…ç½®ä¸å®Œæ•´ï¼Œè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š\n"
            "- CHROMA_CLOUD_API_KEY\n"
            "- CHROMA_CLOUD_TENANT\n"
            "- CHROMA_CLOUD_DATABASE"
        )
    
    chroma_client = chromadb.CloudClient(
        api_key=config.CHROMA_CLOUD_API_KEY,
        tenant=config.CHROMA_CLOUD_TENANT,
        database=config.CHROMA_CLOUD_DATABASE
    )
    
    # åˆ›å»ºæˆ–è·å–é›†åˆ
    chroma_collection = chroma_client.get_or_create_collection(
        name=collection_name,
        metadata={"hnsw:space": "cosine"}
    )
    
    return embed_model, chroma_client, chroma_collection


def _load_embedding_model_fallback(embedding_model_name: str):
    """å›é€€çš„embeddingæ¨¡å‹åŠ è½½æ–¹æ³•"""
    cache_folder = str(Path.home() / ".cache" / "huggingface")
    device = get_gpu_device()
    import torch
    
    if device.startswith("cuda") and is_gpu_available():
        device_name = torch.cuda.get_device_name()
        logger.info(f"âœ… Embeddingæ¨¡å‹ä½¿ç”¨GPUåŠ é€Ÿ:")
        logger.info(f"   è®¾å¤‡: {device}")
        logger.info(f"   GPUåç§°: {device_name}")
    else:
        logger.warning("âš ï¸  Embeddingæ¨¡å‹ä½¿ç”¨CPUæ¨¡å¼")
    
    model_kwargs = {
        "trust_remote_code": True,
        "cache_folder": cache_folder,
    }
    
    embed_model = HuggingFaceEmbedding(
        model_name=embedding_model_name,
        embed_batch_size=config.EMBED_BATCH_SIZE,
        max_length=config.EMBED_MAX_LENGTH,
        **model_kwargs
    )
    
    # æ‰‹åŠ¨å°†æ¨¡å‹ç§»åˆ° GPU
    try:
        if device.startswith("cuda") and is_gpu_available():
            if hasattr(embed_model, '_model') and hasattr(embed_model._model, 'to'):
                embed_model._model = embed_model._model.to(device)
            elif hasattr(embed_model, 'model') and hasattr(embed_model.model, 'to'):
                embed_model.model = embed_model.model.to(device)
    except Exception as e:
        logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU: {e}")
    
    if device.startswith("cuda"):
        logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (GPUåŠ é€Ÿ, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE})")
    else:
        logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ (CPUæ¨¡å¼, æ‰¹å¤„ç†: {config.EMBED_BATCH_SIZE}, å»ºè®®è°ƒæ•´ä¸º5-10)")
    
    return embed_model

