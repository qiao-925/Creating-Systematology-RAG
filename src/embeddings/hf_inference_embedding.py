"""
Hugging Face Inference API Embedding é€‚é…å™¨
æ”¯æŒé€šè¿‡ HF Inference Providers è°ƒç”¨ embedding æ¨¡å‹
ä½¿ç”¨å®˜æ–¹ huggingface_hub SDK
"""

import os
from typing import List, Optional
import time

from huggingface_hub import InferenceClient

from src.embeddings.base import BaseEmbedding
from src.config import config
from src.logger import setup_logger

logger = setup_logger('hf_inference_embedding')


class HFInferenceEmbedding(BaseEmbedding):
    """Hugging Face Inference API Embedding é€‚é…å™¨
    
    ä½¿ç”¨ Hugging Face Inference Providers æœåŠ¡è°ƒç”¨ embedding æ¨¡å‹
    æ”¯æŒæŒ‰é‡ä»˜è´¹ï¼ŒPRO ç”¨æˆ·æ¯æœˆæœ‰ $2.00 å…è´¹é¢åº¦
    """
    
    def __init__(
        self,
        model_name: str = "Qwen/Qwen3-Embedding-0.6B",
        api_key: Optional[str] = None,
        provider: str = "hf-inference",
        dimension: Optional[int] = None,
        timeout: int = 30,
        max_retries: int = 3,
        retry_delay: float = 1.0,
    ):
        """åˆå§‹åŒ– HF Inference API Embedding
        
        Args:
            model_name: Hugging Face æ¨¡å‹åç§°ï¼ˆé»˜è®¤ Qwen/Qwen3-Embedding-0.6Bï¼‰
            api_key: Hugging Face API Tokenï¼ˆä»ç¯å¢ƒå˜é‡ HF_TOKEN æˆ–é…ç½®è¯»å–ï¼‰
            provider: Inference Providerï¼ˆé»˜è®¤ hf-inferenceï¼‰
            dimension: å‘é‡ç»´åº¦ï¼ˆè‡ªåŠ¨æ£€æµ‹ï¼Œå¦‚æœæä¾›åˆ™ç”¨äºéªŒè¯ï¼‰
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        self.model_name = model_name
        self.api_key = api_key or os.getenv("HF_TOKEN") or getattr(config, 'HF_TOKEN', None)
        self.provider = provider
        self.timeout = timeout
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self._dimension = dimension
        
        if not self.api_key:
            raise ValueError(
                "HF_TOKEN æœªè®¾ç½®ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ HF_TOKEN æˆ–é…ç½®ä¸­çš„ HF_TOKENã€‚"
                "è·å– Token: https://huggingface.co/settings/tokens"
            )
        
        # åˆå§‹åŒ–å®˜æ–¹ SDK
        self.client = InferenceClient(
            provider=self.provider,
            api_key=self.api_key,
        )
        
        logger.info(f"ğŸ“¡ åˆå§‹åŒ– Hugging Face Inference API Embedding")
        logger.info(f"   æ¨¡å‹: {self.model_name}")
        logger.info(f"   Provider: {self.provider}")
        
        # éªŒè¯ API å¹¶è·å–ç»´åº¦
        self._validate_and_get_dimension()
    
    def _validate_and_get_dimension(self):
        """éªŒè¯ API å¯ç”¨æ€§å¹¶è·å–å‘é‡ç»´åº¦"""
        try:
            # ä½¿ç”¨æµ‹è¯•æ–‡æœ¬è·å–ç»´åº¦
            test_embedding = self._make_request(["test"])
            if test_embedding:
                self._dimension = len(test_embedding[0])
                logger.info(f"âœ… API è¿æ¥æ­£å¸¸ï¼Œå‘é‡ç»´åº¦: {self._dimension}")
            else:
                raise ValueError("API è¿”å›ç©ºç»“æœ")
        except Exception as e:
            logger.warning(f"âš ï¸  API éªŒè¯å¤±è´¥: {e}")
            if self._dimension is None:
                # Qwen3-Embedding-0.6B çš„é»˜è®¤ç»´åº¦æ˜¯ 1024
                # å¦‚æœæ— æ³•æ£€æµ‹ï¼Œä½¿ç”¨æ¨¡å‹ç‰¹å®šçš„é»˜è®¤å€¼
                if "qwen" in self.model_name.lower() and "0.6b" in self.model_name.lower():
                    default_dim = 1024
                elif "qwen" in self.model_name.lower() and "8b" in self.model_name.lower():
                    default_dim = 1024
                else:
                    default_dim = 384  # å…¶ä»–æ¨¡å‹çš„é€šç”¨é»˜è®¤å€¼
                logger.warning(f"âš ï¸  æ— æ³•è‡ªåŠ¨æ£€æµ‹ç»´åº¦ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ {default_dim}ï¼ˆåŸºäºæ¨¡å‹åç§°æ¨æ–­ï¼‰")
                self._dimension = default_dim
    
    def _make_request(self, texts: List[str], retry_count: int = 0) -> List[List[float]]:
        """å‘èµ· API è¯·æ±‚ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            retry_count: å½“å‰é‡è¯•æ¬¡æ•°
            
        Returns:
            å‘é‡åˆ—è¡¨
            
        Raises:
            RuntimeError: API è°ƒç”¨å¤±è´¥
        """
        try:
            # ä½¿ç”¨å®˜æ–¹ SDK çš„ feature_extraction æ–¹æ³•
            # SDK çš„ feature_extraction ä¸€æ¬¡åªèƒ½å¤„ç†ä¸€ä¸ªæ–‡æœ¬ï¼Œéœ€è¦é€ä¸ªå¤„ç†
            results = []
            for text in texts:
                embedding = self.client.feature_extraction(
                    text,
                    model=self.model_name,
                )
                # ç¡®ä¿è¿”å›çš„æ˜¯åˆ—è¡¨æ ¼å¼
                if isinstance(embedding, list):
                    results.append(embedding)
                elif hasattr(embedding, '__iter__') and not isinstance(embedding, str):
                    # å¦‚æœæ˜¯å¯è¿­ä»£å¯¹è±¡ï¼ˆå¦‚ numpy arrayï¼‰ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
                    results.append(list(embedding))
                else:
                    # å•ä¸ªå€¼çš„æƒ…å†µï¼ˆä¸å¤ªå¯èƒ½ï¼Œä½†å¤„ç†ä¸€ä¸‹ï¼‰
                    results.append([float(embedding)])
            
            return results
                    
        except Exception as e:
            # å¤„ç†æ¨¡å‹åŠ è½½ä¸­çš„æƒ…å†µï¼ˆ503 é”™è¯¯ï¼‰
            error_str = str(e).lower()
            if "503" in error_str or "loading" in error_str or "model" in error_str:
                if retry_count < self.max_retries:
                    wait_seconds = self.retry_delay * (retry_count + 1)
                    logger.info(f"â³ æ¨¡å‹æ­£åœ¨åŠ è½½ï¼Œç­‰å¾… {wait_seconds} ç§’åé‡è¯•...")
                    time.sleep(wait_seconds)
                    return self._make_request(texts, retry_count + 1)
                else:
                    raise RuntimeError("æ¨¡å‹åŠ è½½è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
            
            # å…¶ä»–é”™è¯¯ï¼Œä½¿ç”¨é‡è¯•æœºåˆ¶
            if retry_count < self.max_retries:
                wait_time = self.retry_delay * (retry_count + 1)  # æŒ‡æ•°é€€é¿
                logger.warning(
                    f"âš ï¸  API è°ƒç”¨å¤±è´¥ï¼Œ{wait_time}ç§’åé‡è¯• "
                    f"({retry_count + 1}/{self.max_retries}): {e}"
                )
                time.sleep(wait_time)
                return self._make_request(texts, retry_count + 1)
            else:
                logger.error(f"âŒ API è°ƒç”¨å¤±è´¥ï¼ˆå·²é‡è¯•{self.max_retries}æ¬¡ï¼‰: {e}")
                raise RuntimeError(f"Hugging Face Inference API è°ƒç”¨å¤±è´¥: {e}")
    
    def get_query_embedding(self, query: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢å‘é‡"""
        embeddings = self.get_text_embeddings([query])
        return embeddings[0]
    
    def get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡
        
        æ”¯æŒæ‰¹é‡å¤„ç†ï¼Œè‡ªåŠ¨åˆ†æ‰¹ä»¥é¿å…å•æ¬¡è¯·æ±‚è¿‡å¤§
        """
        if not texts:
            return []
        
        # HF API é€šå¸¸æ”¯æŒæ‰¹é‡å¤„ç†ï¼Œä½†å»ºè®®æ¯æ‰¹ä¸è¶…è¿‡ 100 ä¸ªæ–‡æœ¬
        batch_size = 100
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            logger.debug(f"å¤„ç† embedding æ‰¹æ¬¡: {i // batch_size + 1}/{(len(texts) + batch_size - 1) // batch_size}")
            
            batch_embeddings = self._make_request(batch)
            all_embeddings.extend(batch_embeddings)
        
        return all_embeddings
    
    def get_embedding_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        if self._dimension is None:
            # å¦‚æœæœªè®¾ç½®ï¼Œå°è¯•è·å–
            test_embedding = self.get_query_embedding("test")
            self._dimension = len(test_embedding)
        return self._dimension
    
    def get_model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        return self.model_name

