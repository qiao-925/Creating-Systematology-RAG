"""
æœ¬åœ°Embeddingæ¨¡å‹é€‚é…å™¨
å°è£…HuggingFaceæœ¬åœ°æ¨¡å‹ï¼Œæä¾›ç»Ÿä¸€æ¥å£
"""

import os
from pathlib import Path
from typing import List, Optional
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

from src.embeddings.base import BaseEmbedding
from src.config import config, get_gpu_device, is_gpu_available
from src.logger import setup_logger

logger = setup_logger('local_embedding')


class LocalEmbedding(BaseEmbedding):
    """æœ¬åœ°HuggingFaceæ¨¡å‹é€‚é…å™¨
    
    å°è£…ç°æœ‰çš„HuggingFaceEmbeddingé€»è¾‘ï¼Œå®ç°BaseEmbeddingæ¥å£
    """
    
    def __init__(
        self,
        model_name: Optional[str] = None,
        device: Optional[str] = None,
        embed_batch_size: Optional[int] = None,
        max_length: Optional[int] = None,
        cache_folder: Optional[str] = None,
    ):
        """åˆå§‹åŒ–æœ¬åœ°Embeddingæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
            device: è®¾å¤‡ï¼ˆcuda/cpuï¼Œé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼‰
            embed_batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
            max_length: æœ€å¤§é•¿åº¦ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ï¼‰
            cache_folder: ç¼“å­˜ç›®å½•ï¼ˆé»˜è®¤~/.cache/huggingfaceï¼‰
        """
        self.model_name = model_name or config.EMBEDDING_MODEL
        self.device = device or get_gpu_device()
        self.embed_batch_size = embed_batch_size or config.EMBED_BATCH_SIZE
        self.max_length = max_length or config.EMBED_MAX_LENGTH
        self.cache_folder = cache_folder or str(Path.home() / ".cache" / "huggingface")
        
        # é…ç½®HuggingFaceç¯å¢ƒ
        self._setup_huggingface_env()
        
        # åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _setup_huggingface_env(self):
        """é…ç½®HuggingFaceç¯å¢ƒå˜é‡"""
        # è®¾ç½®é•œåƒåœ°å€
        if config.HF_ENDPOINT:
            os.environ['HF_ENDPOINT'] = config.HF_ENDPOINT
            os.environ['HUGGINGFACE_HUB_ENDPOINT'] = config.HF_ENDPOINT
            os.environ['HF_HUB_ENDPOINT'] = config.HF_ENDPOINT
            logger.info(f"ğŸŒ ä½¿ç”¨ HuggingFace é•œåƒ: {config.HF_ENDPOINT}")
        
        # è®¾ç½®ç¦»çº¿æ¨¡å¼
        if config.HF_OFFLINE_MODE:
            os.environ['HF_HUB_OFFLINE'] = '1'
            os.environ['TRANSFORMERS_OFFLINE'] = '1'
            logger.info(f"ğŸ“´ å¯ç”¨ç¦»çº¿æ¨¡å¼")
        else:
            os.environ.pop('HF_HUB_OFFLINE', None)
            os.environ.pop('TRANSFORMERS_OFFLINE', None)
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        logger.info(f"ğŸ“¦ åŠ è½½æœ¬åœ° Embedding æ¨¡å‹: {self.model_name}")
        
        # è¾“å‡ºè®¾å¤‡ä¿¡æ¯
        if self.device.startswith("cuda") and is_gpu_available():
            import torch
            device_name = torch.cuda.get_device_name()
            logger.info(f"âœ… ä½¿ç”¨GPUåŠ é€Ÿ: {self.device} ({device_name})")
        else:
            logger.warning("âš ï¸  ä½¿ç”¨CPUæ¨¡å¼ï¼ˆæ€§èƒ½è¾ƒæ…¢ï¼‰")
        
        # æ„å»ºæ¨¡å‹å‚æ•°
        model_kwargs = {
            "trust_remote_code": True,
            "cache_folder": self.cache_folder,
        }
        
        # åˆ›å»ºHuggingFaceEmbeddingå®ä¾‹
        self._model = HuggingFaceEmbedding(
            model_name=self.model_name,
            embed_batch_size=self.embed_batch_size,
            max_length=self.max_length,
            **model_kwargs
        )
        
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        try:
            if self.device.startswith("cuda") and is_gpu_available():
                if hasattr(self._model, '_model') and hasattr(self._model._model, 'to'):
                    self._model._model = self._model._model.to(self.device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU: {self.device}")
                elif hasattr(self._model, 'model') and hasattr(self._model.model, 'to'):
                    self._model.model = self._model.model.to(self.device)
                    logger.info(f"âœ… æ¨¡å‹å·²ç§»åŠ¨åˆ°GPU: {self.device}")
        except Exception as e:
            logger.warning(f"âš ï¸  æ— æ³•å°†æ¨¡å‹ç§»åŠ¨åˆ°GPU: {e}")
        
        logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        logger.info(f"   æ‰¹å¤„ç†å¤§å°: {self.embed_batch_size}")
        logger.info(f"   æœ€å¤§é•¿åº¦: {self.max_length}")
    
    def get_query_embedding(self, query: str) -> List[float]:
        """ç”ŸæˆæŸ¥è¯¢å‘é‡"""
        return self._model.get_query_embedding(query)
    
    def get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆæ–‡æœ¬å‘é‡"""
        return self._model.get_text_embedding_batch(texts)
    
    def get_embedding_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        # ç”Ÿæˆä¸€ä¸ªæµ‹è¯•å‘é‡æ¥è·å–ç»´åº¦
        test_embedding = self.get_query_embedding("test")
        return len(test_embedding)
    
    def get_model_name(self) -> str:
        """è·å–æ¨¡å‹åç§°"""
        return self.model_name
    
    def get_llama_index_embedding(self) -> HuggingFaceEmbedding:
        """è·å–åº•å±‚çš„LlamaIndex Embeddingå®ä¾‹
        
        ç”¨äºå‘åå…¼å®¹ï¼Œç›´æ¥ä¼ é€’ç»™LlamaIndexç»„ä»¶
        
        Returns:
            HuggingFaceEmbeddingå®ä¾‹
        """
        return self._model

