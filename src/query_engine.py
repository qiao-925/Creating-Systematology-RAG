"""
æŸ¥è¯¢å¼•æ“æ¨¡å—
é›†æˆDeepSeek APIï¼Œå®ç°å¸¦å¼•ç”¨æº¯æºçš„æŸ¥è¯¢åŠŸèƒ½
æ”¯æŒPhoenixå¯è§‚æµ‹æ€§å’ŒLlamaDebugHandlerè°ƒè¯•
"""

import time
from typing import List, Optional, Tuple, Dict, Any
from llama_index.core import VectorStoreIndex, Settings
from llama_index.core.query_engine import CitationQueryEngine
from llama_index.core.base.response.schema import Response
from llama_index.core.schema import Document as LlamaDocument
from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler
from llama_index.llms.deepseek import DeepSeek

from src.config import config
from src.indexer import IndexManager
from src.logger import setup_logger

logger = setup_logger('query_engine')


class QueryEngine:
    """æŸ¥è¯¢å¼•æ“"""
    
    def __init__(
        self,
        index_manager: IndexManager,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        model: Optional[str] = None,
        similarity_top_k: Optional[int] = None,
        citation_chunk_size: int = 512,
        enable_debug: bool = False,
        similarity_threshold: Optional[float] = None,
    ):
        """åˆå§‹åŒ–æŸ¥è¯¢å¼•æ“
        
        Args:
            index_manager: ç´¢å¼•ç®¡ç†å™¨
            api_key: DeepSeek APIå¯†é’¥
            api_base: APIç«¯ç‚¹
            model: æ¨¡å‹åç§°
            similarity_top_k: æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£æ•°é‡
            citation_chunk_size: å¼•ç”¨å—å¤§å°
            enable_debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆLlamaDebugHandlerï¼‰
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼å¯ç”¨æ¨ç†æ¨¡å¼
        """
        self.index_manager = index_manager
        self.similarity_top_k = similarity_top_k or config.SIMILARITY_TOP_K
        self.citation_chunk_size = citation_chunk_size
        self.enable_debug = enable_debug
        self.similarity_threshold = similarity_threshold or config.SIMILARITY_THRESHOLD
        
        # é…ç½®DeepSeek LLM
        self.api_key = api_key or config.DEEPSEEK_API_KEY
        self.model = model or config.LLM_MODEL
        
        if not self.api_key:
            raise ValueError("æœªè®¾ç½®DEEPSEEK_API_KEYï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶")
        
        # é…ç½®è°ƒè¯•æ¨¡å¼
        if self.enable_debug:
            print("ğŸ” å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆLlamaDebugHandlerï¼‰")
            self.llama_debug = LlamaDebugHandler(print_trace_on_end=True)
            Settings.callback_manager = CallbackManager([self.llama_debug])
            logger.info("è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
        
        print(f"ğŸ¤– åˆå§‹åŒ–DeepSeek LLM: {self.model}")
        # ä½¿ç”¨å®˜æ–¹ DeepSeek é›†æˆ
        self.llm = DeepSeek(
            api_key=self.api_key,
            model=self.model,
            temperature=0.5,  # æé«˜æ¸©åº¦ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›
            max_tokens=4096,
        )
        
        # è·å–ç´¢å¼•
        self.index = self.index_manager.get_index()
        
        # åˆ›å»ºå¸¦å¼•ç”¨çš„æŸ¥è¯¢å¼•æ“
        print("ğŸ“ åˆ›å»ºå¼•ç”¨æŸ¥è¯¢å¼•æ“")
        self.query_engine = CitationQueryEngine.from_args(
            self.index,
            llm=self.llm,
            similarity_top_k=self.similarity_top_k,
            citation_chunk_size=self.citation_chunk_size,
        )
        
        print("âœ… æŸ¥è¯¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def query(self, question: str, collect_trace: bool = False) -> Tuple[str, List[dict], Optional[Dict[str, Any]]]:
        """æ‰§è¡ŒæŸ¥è¯¢å¹¶è¿”å›å¸¦å¼•ç”¨çš„ç­”æ¡ˆ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            collect_trace: æ˜¯å¦æ”¶é›†è¯¦ç»†çš„è¿½è¸ªä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•æ¨¡å¼ï¼‰
            
        Returns:
            (ç­”æ¡ˆæ–‡æœ¬, å¼•ç”¨æ¥æºåˆ—è¡¨, è¿½è¸ªä¿¡æ¯å­—å…¸)
            
        Note:
            è¿½è¸ªä¿¡æ¯åŒ…å«ï¼šæ£€ç´¢æ—¶é—´ã€æ£€ç´¢åˆ°çš„chunkã€ç›¸ä¼¼åº¦åˆ†æ•°ã€LLMè°ƒç”¨æ—¶é—´ç­‰
        """
        trace_info = None
        
        try:
            print(f"\nğŸ’¬ æŸ¥è¯¢: {question}")
            
            if collect_trace:
                trace_info = {
                    "query": question,
                    "start_time": time.time(),
                    "retrieval": {},
                    "llm_generation": {}
                }
            
            # ===== 1. æ‰§è¡Œæ£€ç´¢ =====
            retrieval_start = time.time()
            
            # æ‰§è¡ŒæŸ¥è¯¢
            response: Response = self.query_engine.query(question)
            
            retrieval_time = time.time() - retrieval_start
            
            # æå–ç­”æ¡ˆ
            answer = str(response)
            
            # æå–å¼•ç”¨æ¥æº
            sources = []
            if hasattr(response, 'source_nodes') and response.source_nodes:
                for i, node in enumerate(response.source_nodes, 1):
                    source = {
                        'index': i,
                        'text': node.node.text,
                        'score': node.score if hasattr(node, 'score') else None,
                        'metadata': node.node.metadata,
                    }
                    sources.append(source)
            
            # ===== è¿‡æ»¤ä½è´¨é‡ç»“æœå¹¶è¯„ä¼°æ£€ç´¢è´¨é‡ =====
            high_quality_sources = [s for s in sources if s.get('score', 0) >= self.similarity_threshold]
            max_score = max([s.get('score', 0) for s in sources]) if sources else 0
            
            if len(high_quality_sources) < len(sources):
                logger.info(f"è¿‡æ»¤äº† {len(sources) - len(high_quality_sources)} ä¸ªä½è´¨é‡ç»“æœï¼ˆé˜ˆå€¼: {self.similarity_threshold}ï¼‰")
            
            if max_score < self.similarity_threshold:
                print(f"âš ï¸  æ£€ç´¢è´¨é‡è¾ƒä½ï¼ˆæœ€é«˜ç›¸ä¼¼åº¦: {max_score:.2f}ï¼‰ï¼Œç­”æ¡ˆå¯èƒ½æ›´å¤šä¾èµ–æ¨¡å‹æ¨ç†")
                logger.warning(f"æ£€ç´¢è´¨é‡è¾ƒä½ï¼Œæœ€é«˜ç›¸ä¼¼åº¦: {max_score:.2f}ï¼Œé˜ˆå€¼: {self.similarity_threshold}")
            elif len(high_quality_sources) >= 2:
                print(f"âœ… æ£€ç´¢è´¨é‡è‰¯å¥½ï¼ˆé«˜è´¨é‡ç»“æœ: {len(high_quality_sources)}ä¸ªï¼Œæœ€é«˜ç›¸ä¼¼åº¦: {max_score:.2f}ï¼‰")
            
            # ===== 2. æ”¶é›†è¿½è¸ªä¿¡æ¯ =====
            if collect_trace and trace_info:
                trace_info["retrieval"] = {
                    "time_cost": round(retrieval_time, 2),
                    "top_k": self.similarity_top_k,
                    "chunks_retrieved": len(sources),
                    "chunks": sources,
                    "avg_score": round(sum(s['score'] for s in sources if s['score']) / len(sources), 3) if sources else 0,
                }
                
                trace_info["llm_generation"] = {
                    "model": self.model,
                    "response_length": len(answer),
                }
                
                trace_info["total_time"] = round(time.time() - trace_info["start_time"], 2)
                
                # è®°å½•è¯¦ç»†æ—¥å¿—
                logger.debug(f"æŸ¥è¯¢è¿½è¸ª: {trace_info}")
            
            print(f"âœ… æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(sources)} ä¸ªå¼•ç”¨æ¥æº")
            
            return answer, sources, trace_info
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    async def stream_query(self, question: str):
        """å¼‚æ­¥æµå¼æŸ¥è¯¢ï¼ˆç”¨äºWebåº”ç”¨ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Yields:
            dict: åŒ…å«typeå’Œdataçš„å­—å…¸
                - type='token': dataä¸ºæ–‡æœ¬token
                - type='sources': dataä¸ºå¼•ç”¨æ¥æºåˆ—è¡¨
                - type='done': dataä¸ºå®Œæ•´ç­”æ¡ˆ
        """
        import asyncio
        
        try:
            print(f"\nğŸ’¬ æµå¼æŸ¥è¯¢: {question}")
            
            # æ‰§è¡Œæµå¼æŸ¥è¯¢
            response_stream = self.query_engine.query(question)
            
            # å¯¹äºCitationQueryEngineï¼Œæˆ‘ä»¬éœ€è¦å…ˆè·å–å®Œæ•´å“åº”ï¼Œç„¶åæ¨¡æ‹Ÿæµå¼è¾“å‡º
            # å› ä¸ºå¼•ç”¨éœ€è¦åœ¨å®Œæ•´ç­”æ¡ˆç”Ÿæˆåæ‰èƒ½æå–
            answer = str(response_stream)
            
            # æå–å¼•ç”¨æ¥æº
            sources = []
            if hasattr(response_stream, 'source_nodes') and response_stream.source_nodes:
                for i, node in enumerate(response_stream.source_nodes, 1):
                    source = {
                        'index': i,
                        'text': node.node.text,
                        'score': node.score if hasattr(node, 'score') else None,
                        'metadata': node.node.metadata,
                    }
                    sources.append(source)
            
            # æ¨¡æ‹Ÿæµå¼è¾“å‡ºï¼ˆé€å­—ç¬¦ï¼‰
            for char in answer:
                yield {'type': 'token', 'data': char}
                await asyncio.sleep(0.01)  # æ‰“å­—æœºæ•ˆæœ
            
            print(f"âœ… æµå¼æŸ¥è¯¢å®Œæˆï¼Œæ‰¾åˆ° {len(sources)} ä¸ªå¼•ç”¨æ¥æº")
            
            # è¿”å›å¼•ç”¨æ¥æºå’Œå®Œæ•´ç­”æ¡ˆ
            yield {'type': 'sources', 'data': sources}
            yield {'type': 'done', 'data': answer}
            
        except Exception as e:
            print(f"âŒ æµå¼æŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    def get_retriever(self):
        """è·å–æ£€ç´¢å™¨ï¼ˆç”¨äºé«˜çº§ç”¨æ³•ï¼‰"""
        return self.index.as_retriever(similarity_top_k=self.similarity_top_k)


class SimpleQueryEngine:
    """ç®€å•æŸ¥è¯¢å¼•æ“ï¼ˆä¸å¸¦å¼•ç”¨æº¯æºï¼Œç”¨äºå¿«é€ŸæŸ¥è¯¢ï¼‰"""
    
    def __init__(
        self,
        index_manager: IndexManager,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        model: Optional[str] = None,
        similarity_top_k: Optional[int] = None,
    ):
        """åˆå§‹åŒ–ç®€å•æŸ¥è¯¢å¼•æ“
        
        Args:
            index_manager: ç´¢å¼•ç®¡ç†å™¨
            api_key: DeepSeek APIå¯†é’¥
            api_base: APIç«¯ç‚¹
            model: æ¨¡å‹åç§°
            similarity_top_k: æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£æ•°é‡
        """
        self.index_manager = index_manager
        self.similarity_top_k = similarity_top_k or config.SIMILARITY_TOP_K
        
        # é…ç½®DeepSeek LLM
        self.api_key = api_key or config.DEEPSEEK_API_KEY
        self.model = model or config.LLM_MODEL
        
        if not self.api_key:
            raise ValueError("æœªè®¾ç½®DEEPSEEK_API_KEY")
        
        # ä½¿ç”¨å®˜æ–¹ DeepSeek é›†æˆ
        self.llm = DeepSeek(
            api_key=self.api_key,
            model=self.model,
            temperature=0.5,  # æé«˜æ¸©åº¦ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›
            max_tokens=4096,
        )
        
        # è·å–ç´¢å¼•
        self.index = self.index_manager.get_index()
        
        # åˆ›å»ºæ ‡å‡†æŸ¥è¯¢å¼•æ“
        self.query_engine = self.index.as_query_engine(
            llm=self.llm,
            similarity_top_k=self.similarity_top_k,
        )
    
    def query(self, question: str) -> str:
        """æ‰§è¡Œç®€å•æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            ç­”æ¡ˆæ–‡æœ¬
        """
        try:
            response = self.query_engine.query(question)
            return str(response)
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")
            raise


def format_sources(sources: List[dict]) -> str:
    """æ ¼å¼åŒ–å¼•ç”¨æ¥æºä¸ºå¯è¯»æ–‡æœ¬
    
    Args:
        sources: å¼•ç”¨æ¥æºåˆ—è¡¨
        
    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    if not sources:
        return "ï¼ˆæ— å¼•ç”¨æ¥æºï¼‰"
    
    formatted = "\n\nğŸ“š å¼•ç”¨æ¥æºï¼š\n"
    for source in sources:
        formatted += f"\n[{source['index']}] "
        
        # æ·»åŠ æ–‡æ¡£ä¿¡æ¯
        metadata = source['metadata']
        if 'title' in metadata:
            formatted += f"{metadata['title']}"
        elif 'file_name' in metadata:
            formatted += f"{metadata['file_name']}"
        elif 'url' in metadata:
            formatted += f"{metadata['url']}"
        
        # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°
        if source['score'] is not None:
            formatted += f" (ç›¸ä¼¼åº¦: {source['score']:.2f})"
        
        formatted += f"\n   {source['text'][:200]}..."
        
        if len(source['text']) > 200:
            formatted += f"\n   ï¼ˆå…±{len(source['text'])}å­—ï¼‰"
    
    return formatted


def create_query_engine(
    index_manager: IndexManager,
    with_citation: bool = True
) -> QueryEngine | SimpleQueryEngine:
    """åˆ›å»ºæŸ¥è¯¢å¼•æ“ï¼ˆä¾¿æ·å‡½æ•°ï¼‰
    
    Args:
        index_manager: ç´¢å¼•ç®¡ç†å™¨
        with_citation: æ˜¯å¦ä½¿ç”¨å¼•ç”¨æº¯æº
        
    Returns:
        QueryEngineæˆ–SimpleQueryEngineå¯¹è±¡
    """
    if with_citation:
        return QueryEngine(index_manager)
    else:
        return SimpleQueryEngine(index_manager)


class HybridQueryEngine:
    """æ··åˆæŸ¥è¯¢å¼•æ“ï¼šæœ¬åœ°çŸ¥è¯†åº“ + ç»´åŸºç™¾ç§‘å®æ—¶æŸ¥è¯¢"""
    
    def __init__(
        self,
        index_manager: IndexManager,
        api_key: Optional[str] = None,
        api_base: Optional[str] = None,
        model: Optional[str] = None,
        similarity_top_k: Optional[int] = None,
        enable_wikipedia: bool = True,
        wikipedia_threshold: float = 0.6,
        wikipedia_max_results: int = 2,
    ):
        """åˆå§‹åŒ–æ··åˆæŸ¥è¯¢å¼•æ“
        
        Args:
            index_manager: ç´¢å¼•ç®¡ç†å™¨
            api_key: DeepSeek APIå¯†é’¥
            api_base: APIç«¯ç‚¹
            model: æ¨¡å‹åç§°
            similarity_top_k: æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£æ•°é‡
            enable_wikipedia: æ˜¯å¦å¯ç”¨ç»´åŸºç™¾ç§‘æŸ¥è¯¢
            wikipedia_threshold: è§¦å‘ç»´åŸºç™¾ç§‘çš„ç›¸å…³åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰
            wikipedia_max_results: ç»´åŸºç™¾ç§‘æœ€å¤šè¿”å›ç»“æœæ•°
        """
        self.index_manager = index_manager
        self.enable_wikipedia = enable_wikipedia
        self.wikipedia_threshold = wikipedia_threshold
        self.wikipedia_max_results = wikipedia_max_results
        
        # é…ç½®DeepSeek LLM
        self.api_key = api_key or config.DEEPSEEK_API_KEY
        self.model = model or config.LLM_MODEL
        self.similarity_top_k = similarity_top_k or config.SIMILARITY_TOP_K
        
        if not self.api_key:
            raise ValueError("æœªè®¾ç½®DEEPSEEK_API_KEY")
        
        print(f"ğŸ¤– åˆå§‹åŒ–æ··åˆæŸ¥è¯¢å¼•æ“ï¼ˆç»´åŸºç™¾ç§‘: {'å¯ç”¨' if enable_wikipedia else 'ç¦ç”¨'}ï¼‰")
        
        # ä½¿ç”¨å®˜æ–¹ DeepSeek é›†æˆ
        self.llm = DeepSeek(
            api_key=self.api_key,
            model=self.model,
            temperature=0.5,  # æé«˜æ¸©åº¦ä»¥å¢å¼ºæ¨ç†èƒ½åŠ›
            max_tokens=4096,
        )
        
        # æœ¬åœ°æŸ¥è¯¢å¼•æ“
        self.local_engine = QueryEngine(
            index_manager,
            api_key=api_key,
            api_base=api_base,
            model=model,
            similarity_top_k=similarity_top_k,
        )
        
        # ç»´åŸºç™¾ç§‘readerï¼ˆå»¶è¿Ÿå¯¼å…¥ï¼‰
        self.wikipedia_reader = None
        if self.enable_wikipedia:
            try:
                from llama_index.readers.wikipedia import WikipediaReader
                self.wikipedia_reader = WikipediaReader()
                print("ğŸ“– ç»´åŸºç™¾ç§‘ Reader å·²åŠ è½½")
            except ImportError:
                print("âš ï¸  ç»´åŸºç™¾ç§‘ Reader æœªå®‰è£…ï¼Œå°†ä»…ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“")
                self.enable_wikipedia = False
        
        print("âœ… æ··åˆæŸ¥è¯¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def query(self, question: str) -> Tuple[str, List[dict], List[dict]]:
        """æ‰§è¡Œæ··åˆæŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            (ç­”æ¡ˆæ–‡æœ¬, æœ¬åœ°æ¥æºåˆ—è¡¨, ç»´åŸºç™¾ç§‘æ¥æºåˆ—è¡¨)
        """
        try:
            print(f"\nğŸ’¬ æ··åˆæŸ¥è¯¢: {question}")
            
            # Step 1: æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢
            print("ğŸ” æ­£åœ¨æŸ¥è¯¢æœ¬åœ°çŸ¥è¯†åº“...")
            local_answer, local_sources, _ = self.local_engine.query(question)
            
            # Step 2: åˆ¤æ–­æ˜¯å¦éœ€è¦ç»´åŸºç™¾ç§‘è¡¥å……
            wikipedia_sources = []
            if self._should_query_wikipedia(local_sources, question):
                print("ğŸŒ è§¦å‘ç»´åŸºç™¾ç§‘è¡¥å……æŸ¥è¯¢...")
                
                # æå–å…³é”®è¯
                keywords = self._extract_keywords(question)
                print(f"   å…³é”®è¯: {keywords}")
                
                # æ£€æµ‹æŸ¥è¯¢è¯­è¨€
                lang = self._detect_language(question)
                print(f"   è¯­è¨€: {'ä¸­æ–‡' if lang == 'zh' else 'è‹±æ–‡'}")
                
                # æŸ¥è¯¢ç»´åŸºç™¾ç§‘
                wiki_docs = self._query_wikipedia(keywords, lang)
                
                if wiki_docs:
                    # ä»ç»´åŸºç™¾ç§‘æ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³å†…å®¹
                    wikipedia_sources = self._retrieve_from_wiki_docs(wiki_docs, question)
                    print(f"âœ… æ‰¾åˆ° {len(wikipedia_sources)} ä¸ªç»´åŸºç™¾ç§‘æ¥æº")
            else:
                print("â„¹ï¸  æœ¬åœ°ç»“æœå……åˆ†ï¼Œè·³è¿‡ç»´åŸºç™¾ç§‘æŸ¥è¯¢")
            
            # Step 3: åˆå¹¶ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            if wikipedia_sources:
                final_answer = self._merge_answers(
                    question,
                    local_answer,
                    local_sources,
                    wikipedia_sources
                )
            else:
                final_answer = local_answer
            
            print(f"âœ… æ··åˆæŸ¥è¯¢å®Œæˆ")
            print(f"   æœ¬åœ°æ¥æº: {len(local_sources)} ä¸ª")
            print(f"   ç»´åŸºç™¾ç§‘æ¥æº: {len(wikipedia_sources)} ä¸ª")
            
            return final_answer, local_sources, wikipedia_sources
            
        except Exception as e:
            print(f"âŒ æ··åˆæŸ¥è¯¢å¤±è´¥: {e}")
            raise
    
    async def stream_query(self, question: str):
        """å¼‚æ­¥æµå¼æ··åˆæŸ¥è¯¢ï¼ˆç”¨äºWebåº”ç”¨ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Yields:
            dict: åŒ…å«typeå’Œdataçš„å­—å…¸
                - type='status': dataä¸ºçŠ¶æ€æ—¥å¿—æ¶ˆæ¯
                - type='token': dataä¸ºæ–‡æœ¬token
                - type='sources': dataä¸º{'local': [...], 'wikipedia': [...]}
                - type='done': dataä¸ºå®Œæ•´ç­”æ¡ˆ
        """
        import asyncio
        
        try:
            print(f"\nğŸ’¬ æµå¼æ··åˆæŸ¥è¯¢: {question}")
            
            # Step 1: æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢
            yield {'type': 'status', 'data': 'ğŸ” æ­£åœ¨æŸ¥è¯¢æœ¬åœ°çŸ¥è¯†åº“...'}
            print("ğŸ” æ­£åœ¨æŸ¥è¯¢æœ¬åœ°çŸ¥è¯†åº“...")
            local_answer, local_sources = self.local_engine.query(question)
            yield {'type': 'status', 'data': f'âœ… æœ¬åœ°æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(local_sources)} ä¸ªæ¥æº'}
            
            # Step 2: åˆ¤æ–­æ˜¯å¦éœ€è¦ç»´åŸºç™¾ç§‘è¡¥å……
            wikipedia_sources = []
            if self._should_query_wikipedia(local_sources, question):
                yield {'type': 'status', 'data': 'ğŸŒ æ­£åœ¨æŸ¥è¯¢ç»´åŸºç™¾ç§‘è¡¥å……...'}
                print("ğŸŒ è§¦å‘ç»´åŸºç™¾ç§‘è¡¥å……æŸ¥è¯¢...")
                
                # æå–å…³é”®è¯
                keywords = self._extract_keywords(question)
                print(f"   å…³é”®è¯: {keywords}")
                yield {'type': 'status', 'data': f'ğŸ”‘ å…³é”®è¯: {", ".join(keywords)}'}
                
                # æ£€æµ‹æŸ¥è¯¢è¯­è¨€
                lang = self._detect_language(question)
                lang_text = 'ä¸­æ–‡' if lang == 'zh' else 'è‹±æ–‡'
                print(f"   è¯­è¨€: {lang_text}")
                
                # æŸ¥è¯¢ç»´åŸºç™¾ç§‘
                wiki_docs = self._query_wikipedia(keywords, lang)
                
                if wiki_docs:
                    # ä»ç»´åŸºç™¾ç§‘æ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³å†…å®¹
                    wikipedia_sources = self._retrieve_from_wiki_docs(wiki_docs, question)
                    print(f"âœ… æ‰¾åˆ° {len(wikipedia_sources)} ä¸ªç»´åŸºç™¾ç§‘æ¥æº")
                    yield {'type': 'status', 'data': f'âœ… ç»´åŸºç™¾ç§‘æ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(wikipedia_sources)} ä¸ªæ¥æº'}
                else:
                    yield {'type': 'status', 'data': 'âš ï¸ æœªæ‰¾åˆ°ç»´åŸºç™¾ç§‘è¡¥å……å†…å®¹'}
            else:
                print("â„¹ï¸  æœ¬åœ°ç»“æœå……åˆ†ï¼Œè·³è¿‡ç»´åŸºç™¾ç§‘æŸ¥è¯¢")
                yield {'type': 'status', 'data': 'â„¹ï¸  æœ¬åœ°ç»“æœå……åˆ†ï¼Œè·³è¿‡ç»´åŸºç™¾ç§‘æŸ¥è¯¢'}
            
            # Step 3: åˆå¹¶ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
            yield {'type': 'status', 'data': 'ğŸ¤– æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...'}
            
            if wikipedia_sources:
                final_answer = self._merge_answers(
                    question,
                    local_answer,
                    local_sources,
                    wikipedia_sources
                )
            else:
                final_answer = local_answer
            
            # Step 4: æµå¼è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼ˆå¸¦æ‰“å­—æœºæ•ˆæœï¼‰
            for char in final_answer:
                yield {'type': 'token', 'data': char}
                await asyncio.sleep(0.01)  # æ‰“å­—æœºæ•ˆæœ
            
            print(f"âœ… æµå¼æ··åˆæŸ¥è¯¢å®Œæˆ")
            print(f"   æœ¬åœ°æ¥æº: {len(local_sources)} ä¸ª")
            print(f"   ç»´åŸºç™¾ç§‘æ¥æº: {len(wikipedia_sources)} ä¸ª")
            
            # è¿”å›å¼•ç”¨æ¥æºå’Œå®Œæ•´ç­”æ¡ˆ
            yield {
                'type': 'sources', 
                'data': {
                    'local': local_sources,
                    'wikipedia': wikipedia_sources
                }
            }
            yield {'type': 'done', 'data': final_answer}
            
        except Exception as e:
            print(f"âŒ æµå¼æ··åˆæŸ¥è¯¢å¤±è´¥: {e}")
            yield {'type': 'status', 'data': f'âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}'}
            raise
    
    def _should_query_wikipedia(self, local_sources: List[dict], question: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦è§¦å‘ç»´åŸºç™¾ç§‘æŸ¥è¯¢
        
        Args:
            local_sources: æœ¬åœ°æ£€ç´¢ç»“æœ
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æ˜¯å¦éœ€è¦æŸ¥è¯¢ç»´åŸºç™¾ç§‘
        """
        if not self.enable_wikipedia or not self.wikipedia_reader:
            return False
        
        # ç­–ç•¥1: æœ¬åœ°ç»“æœä¸ºç©º
        if not local_sources:
            print("   è§¦å‘åŸå› : æœ¬åœ°ç»“æœä¸ºç©º")
            return True
        
        # ç­–ç•¥2: æœ¬åœ°ç»“æœç›¸å…³åº¦ä½
        if local_sources:
            max_score = max(s.get('score', 0) for s in local_sources)
            if max_score < self.wikipedia_threshold:
                print(f"   è§¦å‘åŸå› : æœ¬åœ°ç›¸å…³åº¦ä½ ({max_score:.2f} < {self.wikipedia_threshold})")
                return True
        
        # ç­–ç•¥3: ç”¨æˆ·æ˜¾å¼è¯·æ±‚ç»´åŸºç™¾ç§‘
        keywords = ["ç»´åŸºç™¾ç§‘", "wikipedia", "ç™¾ç§‘", "wiki"]
        if any(keyword in question.lower() for keyword in keywords):
            print("   è§¦å‘åŸå› : ç”¨æˆ·æ˜¾å¼è¯·æ±‚")
            return True
        
        return False
    
    def _detect_language(self, text: str) -> str:
        """æ£€æµ‹æŸ¥è¯¢è¯­è¨€ï¼ˆç®€å•è§„åˆ™ï¼‰
        
        Args:
            text: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            è¯­è¨€ä»£ç ï¼ˆzh/enï¼‰
        """
        import re
        # æ£€æµ‹ä¸­æ–‡å­—ç¬¦
        if re.search(r'[\u4e00-\u9fff]', text):
            return "zh"
        return "en"
    
    def _extract_keywords(self, question: str) -> List[str]:
        """æå–æŸ¥è¯¢å…³é”®è¯ï¼ˆä½¿ç”¨LLMï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        try:
            prompt = f"""ä»ä»¥ä¸‹é—®é¢˜ä¸­æå–1-3ä¸ªæœ€å…³é”®çš„ä¸»é¢˜è¯æˆ–å®ä½“åç§°ï¼Œç”¨äºç»´åŸºç™¾ç§‘æœç´¢ã€‚
è¦æ±‚ï¼š
1. åªæå–åè¯æˆ–ä¸“æœ‰åè¯
2. å¤šä¸ªå…³é”®è¯ç”¨é€—å·åˆ†éš”
3. ä¸è¦æœ‰å¤šä½™è¯´æ˜ï¼Œç›´æ¥è¾“å‡ºå…³é”®è¯

é—®é¢˜ï¼š{question}
å…³é”®è¯ï¼š"""
            
            response = self.llm.complete(prompt)
            keywords_str = response.text.strip()
            
            # æ¸…ç†å¹¶åˆ†å‰²å…³é”®è¯
            keywords = [k.strip() for k in keywords_str.split(',')]
            keywords = [k for k in keywords if k]  # ç§»é™¤ç©ºå­—ç¬¦ä¸²
            
            # æœ€å¤šè¿”å›3ä¸ªå…³é”®è¯
            return keywords[:3]
            
        except Exception as e:
            print(f"âš ï¸  å…³é”®è¯æå–å¤±è´¥: {e}")
            # å›é€€æ–¹æ¡ˆï¼šç®€å•åˆ†è¯ï¼ˆå–æœ€åå‡ ä¸ªè¯ï¼‰
            words = question.split()
            return words[-2:] if len(words) >= 2 else words
    
    def _query_wikipedia(self, keywords: List[str], lang: str) -> List[LlamaDocument]:
        """æŸ¥è¯¢ç»´åŸºç™¾ç§‘
        
        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            lang: è¯­è¨€ä»£ç 
            
        Returns:
            ç»´åŸºç™¾ç§‘æ–‡æ¡£åˆ—è¡¨
        """
        if not keywords:
            return []
        
        try:
            from src.data_loader import load_documents_from_wikipedia
            
            # åŠ è½½ç»´åŸºç™¾ç§‘é¡µé¢ï¼ˆé™åˆ¶æ•°é‡ï¼‰
            docs = load_documents_from_wikipedia(
                pages=keywords[:self.wikipedia_max_results],
                lang=lang,
                auto_suggest=True,
                clean=True,
                show_progress=False
            )
            
            return docs
            
        except Exception as e:
            print(f"âš ï¸  ç»´åŸºç™¾ç§‘æŸ¥è¯¢å¤±è´¥: {e}")
            return []
    
    def _retrieve_from_wiki_docs(
        self,
        wiki_docs: List[LlamaDocument],
        question: str
    ) -> List[dict]:
        """ä»ç»´åŸºç™¾ç§‘æ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³å†…å®¹
        
        Args:
            wiki_docs: ç»´åŸºç™¾ç§‘æ–‡æ¡£åˆ—è¡¨
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            ç›¸å…³å†…å®¹åˆ—è¡¨ï¼ˆä¸ QueryEngine è¿”å›æ ¼å¼ä¸€è‡´ï¼‰
        """
        if not wiki_docs:
            return []
        
        try:
            # åˆ›å»ºä¸´æ—¶ç´¢å¼•ï¼ˆä¸æŒä¹…åŒ–ï¼‰
            from llama_index.core import VectorStoreIndex
            
            # ä½¿ç”¨å·²åŠ è½½çš„embeddingæ¨¡å‹
            temp_index = VectorStoreIndex.from_documents(
                wiki_docs,
                show_progress=False
            )
            
            # æ£€ç´¢ç›¸å…³å†…å®¹
            retriever = temp_index.as_retriever(
                similarity_top_k=min(self.wikipedia_max_results, len(wiki_docs))
            )
            nodes = retriever.retrieve(question)
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            sources = []
            for node in nodes:
                source = {
                    'text': node.node.text,
                    'score': node.score if hasattr(node, 'score') else None,
                    'metadata': node.node.metadata,
                }
                sources.append(source)
            
            return sources
            
        except Exception as e:
            print(f"âš ï¸  ç»´åŸºç™¾ç§‘å†…å®¹æ£€ç´¢å¤±è´¥: {e}")
            # å¦‚æœç´¢å¼•å¤±è´¥ï¼Œç›´æ¥è¿”å›æ–‡æ¡£å†…å®¹
            sources = []
            for doc in wiki_docs[:self.wikipedia_max_results]:
                source = {
                    'text': doc.text[:1000],  # æˆªå–å‰1000å­—ç¬¦
                    'score': 0.5,  # é»˜è®¤åˆ†æ•°
                    'metadata': doc.metadata,
                }
                sources.append(source)
            return sources
    
    def _merge_answers(
        self,
        question: str,
        local_answer: str,
        local_sources: List[dict],
        wikipedia_sources: List[dict]
    ) -> str:
        """åˆå¹¶æœ¬åœ°å’Œç»´åŸºç™¾ç§‘çš„ç­”æ¡ˆ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            local_answer: æœ¬åœ°ç­”æ¡ˆ
            local_sources: æœ¬åœ°æ¥æº
            wikipedia_sources: ç»´åŸºç™¾ç§‘æ¥æº
            
        Returns:
            åˆå¹¶åçš„ç­”æ¡ˆ
        """
        try:
            # å¦‚æœæ²¡æœ‰ç»´åŸºç™¾ç§‘è¡¥å……ï¼Œç›´æ¥è¿”å›æœ¬åœ°ç­”æ¡ˆ
            if not wikipedia_sources:
                return local_answer
            
            # æ„å»ºåŒ…å«ç»´åŸºç™¾ç§‘ä¿¡æ¯çš„ä¸Šä¸‹æ–‡
            wiki_context = "\n\n".join([
                f"ã€ç»´åŸºç™¾ç§‘-{s['metadata'].get('title', 'æœªçŸ¥')}ã€‘\n{s['text'][:500]}"
                for s in wikipedia_sources[:2]  # æœ€å¤šä½¿ç”¨2ä¸ªç»´åŸºç™¾ç§‘æ¥æº
            ])
            
            # ä½¿ç”¨LLMé‡æ–°ç”Ÿæˆç»¼åˆç­”æ¡ˆ
            prompt = f"""åŸºäºæœ¬åœ°çŸ¥è¯†åº“å’Œç»´åŸºç™¾ç§‘çš„ä¿¡æ¯ï¼Œç»¼åˆå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

æœ¬åœ°çŸ¥è¯†åº“çš„å›ç­”ï¼š
{local_answer}

ç»´åŸºç™¾ç§‘è¡¥å……ä¿¡æ¯ï¼š
{wiki_context}

è¯·ç»¼åˆä»¥ä¸Šä¿¡æ¯ï¼Œç»™å‡ºå®Œæ•´ã€å‡†ç¡®çš„å›ç­”ã€‚è¦æ±‚ï¼š
1. ä¼˜å…ˆä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“çš„ä¸“ä¸šå†…å®¹
2. ç”¨ç»´åŸºç™¾ç§‘è¡¥å……èƒŒæ™¯çŸ¥è¯†æˆ–æ‰©å±•ä¿¡æ¯
3. ä¿æŒå›ç­”çš„è¿è´¯æ€§å’Œå®Œæ•´æ€§
4. ä¸è¦æåŠ"æœ¬åœ°çŸ¥è¯†åº“"æˆ–"ç»´åŸºç™¾ç§‘"è¿™äº›æœ¯è¯­

ç»¼åˆå›ç­”ï¼š"""
            
            response = self.llm.complete(prompt)
            merged_answer = response.text.strip()
            
            return merged_answer
            
        except Exception as e:
            print(f"âš ï¸  ç­”æ¡ˆåˆå¹¶å¤±è´¥ï¼Œè¿”å›æœ¬åœ°ç­”æ¡ˆ: {e}")
            return local_answer


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    from llama_index.core import Document as LlamaDocument
    
    print("=== æµ‹è¯•æŸ¥è¯¢å¼•æ“ ===\n")
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_docs = [
        LlamaDocument(
            text="""ç³»ç»Ÿç§‘å­¦æ˜¯20ä¸–çºªä¸­æœŸå…´èµ·çš„ä¸€é—¨æ–°å…´å­¦ç§‘ï¼Œå®ƒç ”ç©¶ç³»ç»Ÿçš„ä¸€èˆ¬è§„å¾‹å’Œæ–¹æ³•ã€‚
            ç³»ç»Ÿç§‘å­¦åŒ…æ‹¬ç³»ç»Ÿè®ºã€æ§åˆ¶è®ºã€ä¿¡æ¯è®ºç­‰å¤šä¸ªåˆ†æ”¯ã€‚""",
            metadata={"title": "ç³»ç»Ÿç§‘å­¦æ¦‚è¿°", "source": "test"}
        ),
        LlamaDocument(
            text="""é’±å­¦æ£®ï¼ˆ1911-2009ï¼‰æ˜¯ä¸­å›½è‘—åç§‘å­¦å®¶ï¼Œè¢«èª‰ä¸º"ä¸­å›½èˆªå¤©ä¹‹çˆ¶"ã€‚
            ä»–åœ¨ç³»ç»Ÿå·¥ç¨‹å’Œç³»ç»Ÿç§‘å­¦é¢†åŸŸåšå‡ºäº†æ°å‡ºè´¡çŒ®ï¼Œæå‡ºäº†å¼€æ”¾çš„å¤æ‚å·¨ç³»ç»Ÿç†è®ºã€‚""",
            metadata={"title": "é’±å­¦æ£®ç”Ÿå¹³", "source": "test"}
        ),
        LlamaDocument(
            text="""ç³»ç»Ÿå·¥ç¨‹æ˜¯ä¸€ç§ç»„ç»‡ç®¡ç†æŠ€æœ¯ï¼Œç”¨äºè§£å†³å¤§è§„æ¨¡å¤æ‚ç³»ç»Ÿçš„è®¾è®¡å’Œå®æ–½é—®é¢˜ã€‚
            é’±å­¦æ£®å°†ç³»ç»Ÿå·¥ç¨‹å¼•å…¥ä¸­å›½ï¼Œå¹¶ç»“åˆä¸­å›½å®é™…è¿›è¡Œäº†åˆ›æ–°æ€§å‘å±•ã€‚""",
            metadata={"title": "ç³»ç»Ÿå·¥ç¨‹ç®€ä»‹", "source": "test"}
        ),
    ]
    
    # åˆ›å»ºç´¢å¼•
    index_manager = IndexManager(collection_name="test_query")
    index_manager.build_index(test_docs)
    
    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = QueryEngine(index_manager)
    
    # æµ‹è¯•æŸ¥è¯¢
    question = "é’±å­¦æ£®åœ¨ç³»ç»Ÿç§‘å­¦é¢†åŸŸæœ‰ä»€ä¹ˆè´¡çŒ®ï¼Ÿ"
    answer, sources = query_engine.query(question)
    
    print(f"\né—®é¢˜: {question}")
    print(f"\nç­”æ¡ˆ:\n{answer}")
    print(format_sources(sources))
    
    # æ¸…ç†
    index_manager.clear_index()
    print("\nâœ… æµ‹è¯•å®Œæˆ")

