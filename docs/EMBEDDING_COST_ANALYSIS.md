# Embedding 模型费用损耗评估

**最后更新**: 2025-01-XX  
**评估范围**: Hugging Face Inference API vs 本地模型

---

## 1. 定价信息

### 1.1 Hugging Face Inference Providers

| 账户类型 | 每月免费额度 | 超出后计费方式 |
|---------|------------|--------------|
| 免费用户 | $0.10 | 不支持（超出后无法使用） |
| PRO 用户 | $2.00 | 按需付费（按提供商原价，无加价） |
| 企业用户 | $2.00/席位 | 按需付费 |

### 1.2 HF Inference API 定价（示例）

根据文档，HF Inference API 按**计算时间 × 硬件价格**计费：

- **CPU 推理**（适合 embedding）：约 $0.00012/秒
- **GPU 推理**（大模型）：约 $0.0023/秒（Llama-3-8B 示例）

**注意**：实际价格取决于：
- 使用的模型
- 硬件类型（CPU/GPU）
- 推理时间

### 1.3 本地模型成本

- **一次性成本**：$0（使用开源模型）
- **运行成本**：
  - CPU 模式：电费（可忽略）
  - GPU 模式：电费（可忽略，已有硬件）
  - 存储成本：模型文件 ~500MB-2GB（可忽略）

---

## 2. 使用场景分析

### 2.1 项目配置

根据 `src/config/settings.py`：

```python
CHUNK_SIZE = 512          # 每个chunk的token数
CHUNK_OVERLAP = 50        # chunk重叠数
EMBED_BATCH_SIZE = 10     # 批处理大小（CPU）或 20（GPU）
```

### 2.2 典型使用场景

#### 场景 A：小规模项目（原型/测试）

- **文档数**: 10-50 个
- **平均文档长度**: 2000 tokens
- **chunk 数估算**: 
  - 每个文档 ≈ 2000 / (512 - 50) ≈ 4.3 chunks
  - 总计 ≈ 10-50 × 4.3 ≈ 43-215 chunks

#### 场景 B：中等规模项目

- **文档数**: 100-500 个
- **平均文档长度**: 5000 tokens
- **chunk 数估算**:
  - 每个文档 ≈ 5000 / (512 - 50) ≈ 10.8 chunks
  - 总计 ≈ 100-500 × 10.8 ≈ 1,080-5,400 chunks

#### 场景 C：大规模项目

- **文档数**: 1000+ 个
- **平均文档长度**: 10000 tokens
- **chunk 数估算**:
  - 每个文档 ≈ 10000 / (512 - 50) ≈ 21.6 chunks
  - 总计 ≈ 1000 × 21.6 ≈ 21,600 chunks

#### 场景 D：查询阶段（持续使用）

- **每日查询数**: 10-100 次
- **每月查询数**: 300-3,000 次
- **每次查询**: 1 次 embedding 调用

---

## 3. 成本计算

### 3.1 HF Inference API 成本估算

#### 假设条件

- **每次 embedding 调用耗时**: 0.1-0.5 秒（CPU 推理）
- **单价**: $0.00012/秒（CPU 推理，保守估计）
- **单次调用成本**: $0.000012 - $0.00006

#### 场景 A：小规模（43-215 chunks）

```
索引构建成本：
- 调用次数: 43-215 次
- 单次成本: $0.00003（取中值）
- 总成本: $0.0013 - $0.0065

结论: ✅ 免费额度内（$0.10）
```

#### 场景 B：中等规模（1,080-5,400 chunks）

```
索引构建成本：
- 调用次数: 1,080-5,400 次
- 单次成本: $0.00003
- 总成本: $0.032 - $0.162

结论: 
- 免费用户: ⚠️ 超出免费额度（$0.10）
- PRO 用户: ✅ 免费额度内（$2.00）
```

#### 场景 C：大规模（21,600 chunks）

```
索引构建成本：
- 调用次数: 21,600 次
- 单次成本: $0.00003
- 总成本: $0.648

结论:
- 免费用户: ❌ 超出免费额度
- PRO 用户: ✅ 免费额度内（$2.00）
```

#### 场景 D：查询阶段（每月 300-3,000 次）

```
查询成本：
- 调用次数: 300-3,000 次/月
- 单次成本: $0.00003
- 月成本: $0.009 - $0.09

结论: ✅ 所有用户都在免费额度内
```

### 3.2 本地模型成本

```
一次性成本: $0
运行成本: $0（已有硬件）
存储成本: $0（可忽略）

总成本: $0（无论规模大小）
```

---

## 4. 成本对比总结

| 场景 | 规模 | HF API（免费） | HF API（PRO） | 本地模型 |
|------|------|---------------|--------------|---------|
| **A: 小规模** | 43-215 chunks | ✅ $0.001-$0.006 | ✅ $0.001-$0.006 | ✅ $0 |
| **B: 中等规模** | 1,080-5,400 chunks | ⚠️ $0.032-$0.162 | ✅ $0.032-$0.162 | ✅ $0 |
| **C: 大规模** | 21,600 chunks | ❌ $0.648 | ✅ $0.648 | ✅ $0 |
| **D: 查询** | 300-3,000/月 | ✅ $0.009-$0.09 | ✅ $0.009-$0.09 | ✅ $0 |

---

## 5. 推荐方案

### 5.1 按场景推荐

#### ✅ **推荐使用 HF API** 的场景：

1. **小规模项目**（< 500 chunks）
   - 理由：免费额度足够，无需管理本地资源
   - 适用：原型验证、个人项目、测试环境

2. **资源受限环境**
   - 理由：无 GPU、内存不足、无法下载模型
   - 适用：云服务器、容器环境、边缘设备

3. **快速切换模型**
   - 理由：需要测试不同 embedding 模型
   - 适用：模型对比、A/B 测试

#### ✅ **推荐使用本地模型** 的场景：

1. **大规模项目**（> 5,000 chunks）
   - 理由：成本更低，性能更好（批处理优化）
   - 适用：生产环境、企业应用

2. **频繁索引构建**
   - 理由：避免重复 API 调用成本
   - 适用：持续更新、增量索引

3. **数据隐私要求高**
   - 理由：数据不离开本地
   - 适用：敏感数据、合规要求

4. **已有 GPU 资源**
   - 理由：本地模型性能更好（5 分钟 vs 30 分钟+）
   - 适用：有 GPU 的开发环境

### 5.2 混合策略

```
索引构建阶段 → 本地模型（成本低、性能好）
查询阶段 → 本地模型（延迟低、无成本）

特殊情况：
- 资源受限 → 查询阶段可用 API
- 快速验证 → 索引阶段可用 API
```

---

## 6. 成本优化建议

### 6.1 使用 HF API 时的优化

1. **批量处理**
   - 使用 `get_text_embeddings()` 批量调用
   - 减少 API 调用次数（当前实现已支持）

2. **缓存策略**
   - 相同文本不重复调用
   - 使用向量数据库缓存

3. **监控使用量**
   - 定期检查 HF 账单页面
   - 设置使用量告警

4. **选择合适的模型**
   - 使用 CPU 推理模型（成本更低）
   - 避免使用 GPU 推理模型（成本高）

### 6.2 使用本地模型时的优化

1. **启用批处理**
   - 配置 `EMBED_BATCH_SIZE=20`（GPU）或 `10`（CPU）
   - 提升 3-10 倍性能

2. **使用 GPU 加速**
   - 安装 CUDA 版本的 PyTorch
   - 索引构建时间从 30 分钟降至 5 分钟

3. **增量更新**
   - 仅对新文档生成 embedding
   - 避免重复计算

---

## 7. 实际案例估算

### 案例 1：个人知识库（100 个文档）

```
文档数: 100
平均长度: 3000 tokens
chunk 数: 100 × (3000 / 462) ≈ 650 chunks

成本估算:
- HF API: $0.0195（免费用户可用）
- 本地模型: $0

推荐: HF API（简单、免费额度足够）
```

### 案例 2：企业文档库（1000 个文档）

```
文档数: 1000
平均长度: 8000 tokens
chunk 数: 1000 × (8000 / 462) ≈ 17,316 chunks

成本估算:
- HF API: $0.52（PRO 用户免费额度内）
- 本地模型: $0

推荐: 
- 有 GPU → 本地模型（性能更好）
- 无 GPU → HF API（PRO 用户）
```

### 案例 3：持续查询服务（每天 50 次）

```
查询频率: 50 次/天
月查询数: 1,500 次
月成本: $0.045

结论: ✅ 所有用户都在免费额度内
推荐: 两种方案成本都很低，优先考虑性能
```

---

## 8. 决策树

```
开始
  │
  ├─ 项目规模 < 500 chunks？
  │   ├─ 是 → 使用 HF API（免费额度足够）
  │   └─ 否 → 继续判断
  │
  ├─ 有 GPU 资源？
  │   ├─ 是 → 使用本地模型（性能更好）
  │   └─ 否 → 继续判断
  │
  ├─ 是 PRO 用户？
  │   ├─ 是 → 使用 HF API（$2 免费额度）
  │   └─ 否 → 使用本地模型（避免超出免费额度）
  │
  └─ 数据隐私要求高？
      ├─ 是 → 使用本地模型
      └─ 否 → 根据上述条件选择
```

---

## 9. 总结

### 9.1 关键发现

1. **小规模项目**（< 500 chunks）：HF API 免费额度足够，推荐使用
2. **中等规模**（500-5,000 chunks）：PRO 用户可用 HF API，免费用户建议本地模型
3. **大规模项目**（> 5,000 chunks）：推荐本地模型（成本更低、性能更好）
4. **查询阶段**：两种方案成本都很低，优先考虑性能

### 9.2 最终建议

**对于你的项目**（系统科学知识库 RAG）：

- **索引构建阶段**：推荐**本地模型**
  - 理由：项目可能涉及大量文档，本地模型成本更低
  - 已有批处理优化，性能已足够好

- **查询阶段**：推荐**本地模型**
  - 理由：延迟更低，无额外成本
  - 单次查询仅需 1 次 embedding，本地模型足够快

- **特殊情况**：可考虑 HF API
  - 资源受限环境（无 GPU、内存不足）
  - 快速原型验证
  - 需要测试不同模型

---

## 10. 附录：实际定价查询

要获取最新的定价信息，请访问：

- [Hugging Face Inference Providers 定价页面](https://huggingface.co/pricing)
- [HF Inference API 文档](https://huggingface.co/docs/api-inference)
- [账单页面](https://huggingface.co/settings/billing)

---

## 11. HF Inference API 使用说明

### 11.1 如何获取 HF Token

1. 访问 [Hugging Face 设置页面](https://huggingface.co/settings/tokens)
2. 创建新的 Access Token（选择 "Read" 权限即可）
3. 复制 Token（格式：`hf_xxxxxxxxxxxxx`）

### 11.2 配置方式

#### 方式 1：通过环境变量配置（推荐）

在 `.env` 文件中添加：

```bash
# Embedding 类型
EMBEDDING_TYPE=hf-inference

# 模型名称（可选，默认使用 Qwen/Qwen3-Embedding-0.6B）
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B

# Hugging Face Token
HF_TOKEN=hf_xxxxxxxxxxxxx
```

#### 方式 2：通过代码配置

```python
from src.embeddings.factory import create_embedding

embedding = create_embedding(
    embedding_type="hf-inference",
    model_name="Qwen/Qwen3-Embedding-0.6B",
    api_key="hf_xxxxxxxxxxxxx"
)
```

### 11.3 使用示例

```python
from src.embeddings.factory import create_embedding

# 创建 HF Inference API Embedding 实例
embedding = create_embedding(
    embedding_type="hf-inference",
    model_name="Qwen/Qwen3-Embedding-0.6B"
)

# 生成单个查询向量
query_vector = embedding.get_query_embedding("什么是系统科学？")

# 批量生成文本向量
texts = ["文本1", "文本2", "文本3"]
vectors = embedding.get_text_embeddings(texts)

# 获取向量维度
dimension = embedding.get_embedding_dimension()
print(f"向量维度: {dimension}")
```

### 11.4 常见问题

#### Q1: API 调用失败，提示 "模型正在加载"

**原因**: 首次调用某个模型时，HF 需要加载模型到内存，可能需要等待 10-30 秒。

**解决方案**: 
- 代码已自动处理：会自动等待并重试
- 首次调用后，模型会保持加载状态，后续调用会更快

#### Q2: 提示 "HF_TOKEN 未设置"

**原因**: 未设置 Hugging Face Token。

**解决方案**:
1. 获取 Token：访问 https://huggingface.co/settings/tokens
2. 在 `.env` 文件中设置：`HF_TOKEN=your_token_here`
3. 或在代码中传递：`api_key="your_token_here"`

#### Q3: API 调用超时

**原因**: 网络问题或模型加载时间过长。

**解决方案**:
- 检查网络连接
- 增加 `timeout` 参数（默认 30 秒）
- 首次调用后，模型会保持加载状态，后续调用会更快

#### Q4: 如何切换回本地模型？

**解决方案**: 修改 `.env` 文件：

```bash
# 切换回本地模型
EMBEDDING_TYPE=local
EMBEDDING_MODEL=Qwen/Qwen3-Embedding-0.6B
```

#### Q5: 如何监控 API 使用量和成本？

**解决方案**:
1. 访问 [HF 账单页面](https://huggingface.co/settings/billing)
2. 查看 "Inference Providers" 使用情况
3. 设置使用量告警（PRO 用户和企业用户）

### 11.5 性能优化建议

1. **批量处理**: 使用 `get_text_embeddings()` 批量调用，减少 API 请求次数
2. **缓存策略**: 相同文本不重复调用，使用向量数据库缓存
3. **监控使用量**: 定期检查 HF 账单页面，设置使用量告警
4. **选择合适的模型**: 根据需求选择合适的模型（0.6B vs 8B）

### 11.6 支持的模型

理论上支持所有 Hugging Face 上的 embedding 模型，包括：

- `Qwen/Qwen3-Embedding-0.6B`（推荐，默认）
- `Qwen/Qwen3-Embedding-8B`（更高精度，但成本更高）
- `sentence-transformers/all-MiniLM-L6-v2`（轻量级）
- `BAAI/bge-base-zh-v1.5`（中文优化）
- 其他 Hugging Face 上的 embedding 模型

---

**版本**: v1.1  
**最后更新**: 2025-01-XX



