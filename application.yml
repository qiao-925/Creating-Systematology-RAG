# 应用配置文件
# 注意：此文件仅包含静态非敏感配置
# 敏感信息（API keys、tokens、密钥等）请使用 .env 文件

# ============================================================================
# 1. 应用基础配置
# ============================================================================
app:
  title: Creating Systematology
  port: 8501
  dev_mode: true

logging:
  level: DEBUG     # 控制台日志级别（改为 DEBUG 显示更多信息）
  file_level: DEBUG  # 文件日志级别

cache:
  enable: true  # 已废弃：缓存管理器功能已移除，此配置不再使用

# ============================================================================
# 2. API 与外部服务配置
# ============================================================================
api:
  deepseek:
    base: https://api.deepseek.com/v1

huggingface:
  endpoint: https://hf-mirror.com
  offline_mode: false

github:
  default_branch: main

# ============================================================================
# 3. 模型配置
# ============================================================================
model:
  llm: deepseek-chat  # 向后兼容：默认 LLM 模型
  embedding: BAAI/bge-base-zh-v1.5
  
  # 多模型配置（LiteLLM 统一接口）
  llms:
    default: deepseek-chat  # 默认使用的模型 ID
    initialization_timeout: 30.0  # 初始化超时时间（秒）
    max_retries: 3  # 最大重试次数
    retry_delay: 2.0  # 重试延迟（秒，指数退避）
    available:
      - id: deepseek-chat
        name: DeepSeek Chat
        litellm_model: deepseek/deepseek-chat
        api_key_env: DEEPSEEK_API_KEY
        temperature: 0.7
        max_tokens: 4096
        supports_reasoning: false
        request_timeout: 30.0  # API 请求超时（秒）
        
      - id: deepseek-reasoner
        name: DeepSeek Reasoner (推理)
        litellm_model: deepseek/deepseek-reasoner
        api_key_env: DEEPSEEK_API_KEY
        temperature: null  # 推理模型不支持 temperature
        max_tokens: 32768
        supports_reasoning: true
        request_timeout: 60.0  # API 请求超时（秒，推理模型需要更长时间）
        
      # 以下模型需要配置对应的 API Key 后启用
      # - id: qwen-plus
      #   name: 通义千问 Plus
      #   litellm_model: qwen/qwen-plus
      #   api_key_env: DASHSCOPE_API_KEY
      #   temperature: 0.7
      #   max_tokens: 8192
      #   supports_reasoning: false
      #
      # - id: gpt-4o
      #   name: GPT-4o
      #   litellm_model: gpt-4o
      #   api_key_env: OPENAI_API_KEY
      #   temperature: 0.7
      #   max_tokens: 4096
      #   supports_reasoning: false

embedding:
  type: hf-inference  # 使用 Hugging Face Inference API
  api_url: http://localhost:8000  # 仅在使用 api 类型时有效
  batch_size: 10
  max_length: 512

deepseek:
  enable_reasoning_display: true  # 是否在 UI 中显示推理链（始终显示）
  store_reasoning: true  # 是否存储推理链到会话历史（始终存储）
  json_output_enabled: false  # 是否全局启用 JSON Output（默认 false，仅结构化场景启用）

# ============================================================================
# 4. 向量存储与索引配置
# ============================================================================
vector_store:
  collection_name: default

paths:
  raw_data: ./data/raw
  processed_data: ./data/processed
  vector_store: ./data/vector_store  # 向量存储路径（向后兼容，Chroma Cloud 模式不再使用）
  activity_log: ./data/logs/activity
  github_repos: ./data/github_repos
  github_sync_state: ./data/github_sync_state.json
  cache_state: ./data/cache_state.json  # 已废弃：缓存管理器功能已移除，此配置不再使用
  sessions: ./data/sessions  # 会话持久化目录

index:
  chunk_size: 512
  chunk_overlap: 50
  similarity_top_k: 3
  similarity_threshold: 0.4  # 最大化召回，宽松过滤

# ============================================================================
# 5. RAG 核心配置
# ============================================================================
rag:
  retrieval_strategy: vector
  enable_rerank: false
  reranker:
    type: sentence-transformer
    model: null
    top_n: 3
  similarity_cutoff: 0.4  # 最大化召回，宽松过滤（后处理器过滤 + 兜底检查）
  hybrid_alpha: 0.5
  enable_auto_routing: true
  
  # 多策略检索配置
  multi_strategy:
    enabled_strategies:
      - vector
    merge_strategy: reciprocal_rank_fusion
    retriever_weights:
      vector: 1.0
      bm25: 0.8
      grep: 0.6
    enable_deduplication: true

module_registry:
  config_path: null
  auto_register_modules: true

batch_processing:
  index_batch_mode: false
  index_group_by: directory
  group_depth: 1
  docs_per_batch: 20
  nodes_per_batch: 0
  tokens_per_batch: 0
  index_strategy: nodes
  index_max_batches: 0

# ============================================================================
# 6. 可观测性与评估配置
# ============================================================================
observability:
  llama_debug:
    enable: true  # 默认启用
    print_trace: true

ragas:
  enable: true  # 默认启用
  metrics:
    - faithfulness
    - context_precision
    - context_recall
    - answer_relevancy
    - context_relevancy
  batch_size: 10

# ============================================================================
# 7. 测试配置
# ============================================================================
test:
  github:
    owner: qiao-925
    repo: Creating-Systematology-Test
    branch: main
